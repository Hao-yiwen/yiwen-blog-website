"use strict";(globalThis.webpackChunkyiwen_blog_website=globalThis.webpackChunkyiwen_blog_website||[]).push([[73266],{28453:(n,e,s)=>{s.d(e,{R:()=>o,x:()=>l});var t=s(96540);const i={},r=t.createContext(i);function o(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),t.createElement(r.Provider,{value:e},n.children)}},69895:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"ai/mainstream_transformer","title":"\u4e3b\u6d41 Transformer \u67b6\u6784\u8be6\u89e3\uff1aGPT\u3001BERT\u3001T5","description":"\u4e00\u3001\u67b6\u6784\u6982\u8ff0","source":"@site/docs/ai/mainstream_transformer.md","sourceDirName":"ai","slug":"/ai/mainstream_transformer","permalink":"/yiwen-blog-website/en/docs/ai/mainstream_transformer","draft":false,"unlisted":false,"editUrl":"https://github.com/Hao-yiwen/yiwen-blog-website/tree/master/docs/ai/mainstream_transformer.md","tags":[],"version":"current","frontMatter":{},"sidebar":"aiSidebar","previous":{"title":"\u4eba\u5de5\u667a\u80fd","permalink":"/yiwen-blog-website/en/docs/ai/intro"},"next":{"title":"\u591a\u5c42\u611f\u77e5\u673a","permalink":"/yiwen-blog-website/en/docs/ai/mlp"}}');var i=s(74848),r=s(28453);const o={},l="\u4e3b\u6d41 Transformer \u67b6\u6784\u8be6\u89e3\uff1aGPT\u3001BERT\u3001T5",a={},d=[{value:"\u4e00\u3001\u67b6\u6784\u6982\u8ff0",id:"\u4e00\u67b6\u6784\u6982\u8ff0",level:2},{value:"1.1 \u4e09\u79cd\u67b6\u6784\u7684\u672c\u8d28\u533a\u522b",id:"11-\u4e09\u79cd\u67b6\u6784\u7684\u672c\u8d28\u533a\u522b",level:3},{value:"1.2 \u6ce8\u610f\u529b\u673a\u5236\u7684\u5173\u952e\u5dee\u5f02",id:"12-\u6ce8\u610f\u529b\u673a\u5236\u7684\u5173\u952e\u5dee\u5f02",level:3},{value:"\u4e8c\u3001\u6838\u5fc3\u67b6\u6784\u5b9e\u73b0",id:"\u4e8c\u6838\u5fc3\u67b6\u6784\u5b9e\u73b0",level:2},{value:"2.1 GPT \u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",id:"21-gpt-\u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",level:3},{value:"2.2 BERT \u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",id:"22-bert-\u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",level:3},{value:"2.3 T5 \u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",id:"23-t5-\u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",level:3},{value:"\u4e09\u3001\u67b6\u6784\u7279\u6027\u5bf9\u6bd4",id:"\u4e09\u67b6\u6784\u7279\u6027\u5bf9\u6bd4",level:2},{value:"3.1 \u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790",id:"31-\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790",level:3},{value:"3.2 \u67b6\u6784\u8bbe\u8ba1\u6743\u8861",id:"32-\u67b6\u6784\u8bbe\u8ba1\u6743\u8861",level:3},{value:"\u56db\u3001\u73b0\u4ee3\u53d1\u5c55\u8d8b\u52bf",id:"\u56db\u73b0\u4ee3\u53d1\u5c55\u8d8b\u52bf",level:2},{value:"4.1 \u4e3a\u4ec0\u4e48 GPT \u6210\u4e3a\u4e3b\u6d41",id:"41-\u4e3a\u4ec0\u4e48-gpt-\u6210\u4e3a\u4e3b\u6d41",level:3},{value:"4.2 \u67b6\u6784\u521b\u65b0\u65b9\u5411",id:"42-\u67b6\u6784\u521b\u65b0\u65b9\u5411",level:3},{value:"\u53c2\u8003\u8d44\u6599",id:"\u53c2\u8003\u8d44\u6599",level:2}];function _(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"\u4e3b\u6d41-transformer-\u67b6\u6784\u8be6\u89e3gptbertt5",children:"\u4e3b\u6d41 Transformer \u67b6\u6784\u8be6\u89e3\uff1aGPT\u3001BERT\u3001T5"})}),"\n",(0,i.jsx)(e.h2,{id:"\u4e00\u67b6\u6784\u6982\u8ff0",children:"\u4e00\u3001\u67b6\u6784\u6982\u8ff0"}),"\n",(0,i.jsx)(e.h3,{id:"11-\u4e09\u79cd\u67b6\u6784\u7684\u672c\u8d28\u533a\u522b",children:"1.1 \u4e09\u79cd\u67b6\u6784\u7684\u672c\u8d28\u533a\u522b"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"GPT\uff08Generative Pre-trained Transformer\uff09"})," \u91c7\u7528 ",(0,i.jsx)(e.strong,{children:"Decoder-only"}),' \u67b6\u6784\uff0c\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u3002\u5b83\u53ea\u4f7f\u7528 Transformer \u7684\u89e3\u7801\u5668\u90e8\u5206\uff0c\u901a\u8fc7\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\uff08Causal Attention\uff09\u786e\u4fdd\u6bcf\u4e2a\u4f4d\u7f6e\u53ea\u80fd\u770b\u5230\u4e4b\u524d\u7684\u4fe1\u606f\uff0c\u8fd9\u79cd\u5355\u5411\u4fe1\u606f\u6d41\u4f7f\u5176\u5929\u7136\u9002\u5408\u6587\u672c\u751f\u6210\u4efb\u52a1\u3002GPT \u7684\u6838\u5fc3\u601d\u60f3\u662f"\u6839\u636e\u524d\u6587\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd"\uff0c\u8fd9\u79cd\u8bad\u7ec3\u65b9\u5f0f\u8ba9\u6a21\u578b\u5b66\u4f1a\u4e86\u8bed\u8a00\u7684\u6982\u7387\u5206\u5e03\u3002']}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"BERT\uff08Bidirectional Encoder Representations from Transformers\uff09"})," \u91c7\u7528 ",(0,i.jsx)(e.strong,{children:"Encoder-only"})," \u67b6\u6784\uff0c\u662f\u4e00\u4e2a\u53cc\u5411\u8bed\u8a00\u7406\u89e3\u6a21\u578b\u3002\u5b83\u4f7f\u7528\u5b8c\u6574\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5141\u8bb8\u6bcf\u4e2a\u4f4d\u7f6e\u540c\u65f6\u770b\u5230\u524d\u540e\u6587\u4fe1\u606f\u3002\u8fd9\u79cd\u53cc\u5411\u4fe1\u606f\u6d41\u8ba9 BERT \u80fd\u591f\u5145\u5206\u7406\u89e3\u4e0a\u4e0b\u6587\uff0c\u4f46\u4e5f\u4f7f\u5176\u5931\u53bb\u4e86\u81ea\u7136\u7684\u751f\u6210\u80fd\u529b\u3002BERT \u901a\u8fc7\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\uff08MLM\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u968f\u673a\u906e\u76d6\u90e8\u5206\u8bcd\u6c47\u5e76\u9884\u6d4b\u5b83\u4eec\uff0c\u4ece\u800c\u5b66\u4e60\u6df1\u5c42\u7684\u8bed\u8a00\u8868\u793a\u3002"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"T5\uff08Text-to-Text Transfer Transformer\uff09"})," \u91c7\u7528\u5b8c\u6574\u7684 ",(0,i.jsx)(e.strong,{children:"Encoder-Decoder"}),' \u67b6\u6784\uff0c\u5c06\u6240\u6709 NLP \u4efb\u52a1\u7edf\u4e00\u4e3a"\u6587\u672c\u5230\u6587\u672c"\u7684\u8f6c\u6362\u95ee\u9898\u3002\u7f16\u7801\u5668\u8d1f\u8d23\u7406\u89e3\u8f93\u5165\uff08\u4f7f\u7528\u53cc\u5411\u6ce8\u610f\u529b\uff09\uff0c\u89e3\u7801\u5668\u8d1f\u8d23\u751f\u6210\u8f93\u51fa\uff08\u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\uff09\uff0c\u4e24\u8005\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8fde\u63a5\u3002\u8fd9\u79cd\u67b6\u6784\u65e2\u4fdd\u7559\u4e86 BERT \u7684\u53cc\u5411\u7406\u89e3\u80fd\u529b\uff0c\u53c8\u5177\u5907 GPT \u7684\u751f\u6210\u80fd\u529b\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u5148\u7406\u89e3\u540e\u751f\u6210\u7684\u4efb\u52a1\u3002']}),"\n",(0,i.jsx)(e.h3,{id:"12-\u6ce8\u610f\u529b\u673a\u5236\u7684\u5173\u952e\u5dee\u5f02",children:"1.2 \u6ce8\u610f\u529b\u673a\u5236\u7684\u5173\u952e\u5dee\u5f02"}),"\n",(0,i.jsx)(e.p,{children:"\u4e09\u79cd\u67b6\u6784\u6700\u6838\u5fc3\u7684\u533a\u522b\u5728\u4e8e\u6ce8\u610f\u529b\u63a9\u7801\uff08Attention Mask\uff09\u7684\u5b9e\u73b0\u65b9\u5f0f\uff1a"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"GPT \u7684\u56e0\u679c\u63a9\u7801"}),"\uff1a\u4f7f\u7528\u4e0b\u4e09\u89d2\u77e9\u9635\uff0c\u786e\u4fdd\u4f4d\u7f6e i \u53ea\u80fd\u5173\u6ce8\u4f4d\u7f6e 0 \u5230 i \u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u4e25\u683c\u7684\u4ece\u5de6\u5230\u53f3\u7684\u4fe1\u606f\u6d41\u3002\u8fd9\u79cd\u63a9\u7801\u5728\u8bad\u7ec3\u65f6\u5c31\u5185\u7f6e\uff0c\u4f7f\u5f97\u6a21\u578b\u5929\u7136\u5b66\u4f1a\u4e86\u81ea\u56de\u5f52\u751f\u6210\u3002"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"BERT \u7684\u5168\u6ce8\u610f\u529b"}),"\uff1a\u6ca1\u6709\u65b9\u5411\u6027\u9650\u5236\uff0c\u6bcf\u4e2a\u4f4d\u7f6e\u53ef\u4ee5\u81ea\u7531\u5730\u5173\u6ce8\u5e8f\u5217\u4e2d\u7684\u4efb\u4f55\u5176\u4ed6\u4f4d\u7f6e\u3002\u8fd9\u79cd\u5168\u8fde\u63a5\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u8ba9 BERT \u80fd\u591f\u5efa\u7acb\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u4f46\u4e5f\u610f\u5473\u7740\u5b83\u4e0d\u80fd\u76f4\u63a5\u7528\u4e8e\u751f\u6210\u4efb\u52a1\u3002"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"T5 \u7684\u6df7\u5408\u6a21\u5f0f"}),"\uff1a\u7f16\u7801\u5668\u4f7f\u7528\u5168\u6ce8\u610f\u529b\u7406\u89e3\u8f93\u5165\uff0c\u89e3\u7801\u5668\u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\u751f\u6210\u8f93\u51fa\uff0c\u540c\u65f6\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u8ba9\u89e3\u7801\u5668\u80fd\u591f\u5173\u6ce8\u7f16\u7801\u5668\u7684\u8f93\u51fa\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5de7\u5999\u5730\u7ed3\u5408\u4e86\u7406\u89e3\u548c\u751f\u6210\u7684\u9700\u6c42\u3002"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"\u4e8c\u6838\u5fc3\u67b6\u6784\u5b9e\u73b0",children:"\u4e8c\u3001\u6838\u5fc3\u67b6\u6784\u5b9e\u73b0"}),"\n",(0,i.jsx)(e.h3,{id:"21-gpt-\u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",children:"2.1 GPT \u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0"}),"\n",(0,i.jsx)(e.p,{children:'GPT \u7684\u67b6\u6784\u8bbe\u8ba1\u9075\u5faa"\u7b80\u5355\u5373\u662f\u7f8e"\u7684\u539f\u5219\u3002\u6574\u4e2a\u6a21\u578b\u7531\u76f8\u540c\u7684 Transformer \u5757\u5806\u53e0\u800c\u6210\uff0c\u6bcf\u4e2a\u5757\u5305\u542b\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u548c\u4e00\u4e2a\u524d\u9988\u7f51\u7edc\u5c42\uff0c\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u6765\u4fdd\u8bc1\u6df1\u5c42\u7f51\u7edc\u7684\u7a33\u5b9a\u8bad\u7ec3\u3002'}),"\n",(0,i.jsxs)(e.p,{children:["GPT \u7684\u4e00\u4e2a\u91cd\u8981\u8bbe\u8ba1\u9009\u62e9\u662f ",(0,i.jsx)(e.strong,{children:"Pre-LayerNorm"}),"\uff0c\u5373\u5728\u5b50\u5c42\u4e4b\u524d\u800c\u4e0d\u662f\u4e4b\u540e\u8fdb\u884c\u5f52\u4e00\u5316\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5df2\u7ecf\u6210\u4e3a\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6807\u51c6\u914d\u7f6e\u3002"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass GPTBlock(nn.Module):\n    """\n    GPT\u7684\u6838\u5fc3\u6784\u5efa\u5757\n    \u91c7\u7528Pre-LN\u7ed3\u6784\uff1aLN \u2192 Attention \u2192 Residual \u2192 LN \u2192 FFN \u2192 Residual\n    \u8fd9\u79cd\u7ed3\u6784\u76f8\u6bd4Post-LN\u6709\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\n    """\n    \n    def __init__(self, config):\n        super().__init__()\n        # Pre-LayerNorm\uff1a\u5728attention\u548cFFN\u4e4b\u524d\u8fdb\u884c\u5f52\u4e00\u5316\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        \n        # FFN\u4f7f\u75284\u500d\u7684\u9690\u85cf\u5c42\u7ef4\u5ea6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ecf\u9a8c\u6027\u7684\u9009\u62e9\n        # \u73b0\u4ee3\u6a21\u578b\u5982LLaMA\u4f7f\u7528SwiGLU\u6fc0\u6d3b\u51fd\u6570\uff0c\u7ef4\u5ea6\u6bd4\u4f8b\u8c03\u6574\u4e3a8/3\n        self.mlp = nn.Sequential(\n            nn.Linear(config.n_embd, 4 * config.n_embd),\n            nn.GELU(),  # GPT-2\u4f7f\u7528GELU\uff0cGPT-3\u540e\u7eed\u7248\u672c\u4f7f\u7528GeLU\u7684\u8fd1\u4f3c\u7248\u672c\n            nn.Linear(4 * config.n_embd, config.n_embd),\n            nn.Dropout(config.dropout)\n        )\n    \n    def forward(self, x):\n        # \u6b8b\u5dee\u8fde\u63a5\u786e\u4fdd\u68af\u5ea6\u53ef\u4ee5\u76f4\u63a5\u6d41\u5411\u5e95\u5c42\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass CausalSelfAttention(nn.Module):\n    """\n    \u56e0\u679c\u81ea\u6ce8\u610f\u529b\u673a\u5236\n    \u6838\u5fc3\u662f\u56e0\u679c\u63a9\u7801\uff0c\u786e\u4fdd\u6bcf\u4e2a\u4f4d\u7f6e\u53ea\u80fd\u770b\u5230\u5b83\u4e4b\u524d\u7684\u4f4d\u7f6e\n    """\n    \n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = config.n_embd // config.n_head\n        \n        # QKV\u4e09\u4e2a\u6295\u5f71\u77e9\u9635\u5408\u5e76\u4e3a\u4e00\u4e2a\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        \n        # \u8f93\u51fa\u6295\u5f71\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        \n        # \u56e0\u679c\u63a9\u7801\uff1a\u4e0b\u4e09\u89d2\u77e9\u9635\uff0cregister_buffer\u786e\u4fdd\u5b83\u4e0d\u88ab\u5f53\u4f5c\u6a21\u578b\u53c2\u6570\n        self.register_buffer("bias", \n            torch.tril(torch.ones(config.block_size, config.block_size))\n            .view(1, 1, config.block_size, config.block_size))\n    \n    def forward(self, x):\n        B, T, C = x.size()  # batch, sequence length, embedding dim\n        \n        # \u4e00\u6b21\u6027\u8ba1\u7b97Q\u3001K\u3001V\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        \n        # \u91cd\u5851\u4e3a\u591a\u5934\u683c\u5f0f\uff1a(B, n_head, T, head_dim)\n        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n        \n        # \u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u4f7f\u7528\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n        \n        # \u5e94\u7528\u56e0\u679c\u63a9\u7801\uff1a\u672a\u6765\u4f4d\u7f6e\u8bbe\u4e3a-inf\uff0csoftmax\u540e\u53d8\u4e3a0\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float(\'-inf\'))\n        \n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        \n        # \u5e94\u7528\u6ce8\u610f\u529b\u6743\u91cd\u5230values\n        y = att @ v\n        \n        # \u6062\u590d\u539f\u59cb\u5f62\u72b6\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        \n        # \u8f93\u51fa\u6295\u5f71\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n'})}),"\n",(0,i.jsx)(e.h3,{id:"22-bert-\u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",children:"2.2 BERT \u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0"}),"\n",(0,i.jsxs)(e.p,{children:['BERT \u7684\u8bbe\u8ba1\u7406\u5ff5\u662f"\u6df1\u5ea6\u53cc\u5411\u7406\u89e3"\u3002\u4e0e GPT \u4e0d\u540c\uff0cBERT \u4f7f\u7528 ',(0,i.jsx)(e.strong,{children:"Post-LayerNorm"})," \u7ed3\u6784\uff0c\u8fd9\u662f\u539f\u59cb Transformer \u7684\u8bbe\u8ba1\u3002BERT \u8fd8\u5f15\u5165\u4e86\u4e09\u79cd\u5d4c\u5165\uff1a\u8bcd\u5d4c\u5165\u3001\u4f4d\u7f6e\u5d4c\u5165\u548c\u6bb5\u843d\u5d4c\u5165\uff08\u7528\u4e8e\u533a\u5206\u4e0d\u540c\u7684\u53e5\u5b50\uff09\uff0c\u8fd9\u8ba9\u5b83\u80fd\u591f\u5904\u7406\u53e5\u5bf9\u4efb\u52a1\u3002"]}),"\n",(0,i.jsx)(e.p,{children:"BERT \u6700\u5927\u7684\u521b\u65b0\u662f\u901a\u8fc7 MLM\uff08\u63a9\u7801\u8bed\u8a00\u6a21\u578b\uff09\u4efb\u52a1\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u79cd\u65b9\u5f0f\u8ba9\u6a21\u578b\u80fd\u591f\u5229\u7528\u53cc\u5411\u4e0a\u4e0b\u6587\uff0c\u4f46\u4ee3\u4ef7\u662f\u4e0d\u80fd\u76f4\u63a5\u7528\u4e8e\u751f\u6210\u4efb\u52a1\u3002"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class BERTBlock(nn.Module):\n    """\n    BERT\u7684Transformer\u7f16\u7801\u5668\u5757\n    \u91c7\u7528Post-LN\u7ed3\u6784\uff1aAttention \u2192 Residual \u2192 LN \u2192 FFN \u2192 Residual \u2192 LN\n    \u8fd9\u662f\u539f\u59cbTransformer\u7684\u7ed3\u6784\uff0c\u8bad\u7ec3\u76f8\u5bf9\u4e0d\u5982Pre-LN\u7a33\u5b9a\n    """\n    \n    def __init__(self, config):\n        super().__init__()\n        self.attention = MultiHeadSelfAttention(config)\n        self.ln1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        \n        # BERT\u7684FFN\u4f7f\u7528GELU\u6fc0\u6d3b\u51fd\u6570\uff0c\u8fd9\u5728\u5f53\u65f6\u662f\u4e00\u4e2a\u521b\u65b0\n        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.activation = nn.GELU()\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.ln2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n    \n    def forward(self, hidden_states, attention_mask=None):\n        # \u81ea\u6ce8\u610f\u529b + \u6b8b\u5dee\u8fde\u63a5\n        attention_output = self.attention(hidden_states, attention_mask)\n        hidden_states = self.ln1(hidden_states + attention_output)\n        \n        # FFN + \u6b8b\u5dee\u8fde\u63a5\n        intermediate_output = self.activation(self.intermediate(hidden_states))\n        layer_output = self.output(intermediate_output)\n        layer_output = self.dropout(layer_output)\n        hidden_states = self.ln2(hidden_states + layer_output)\n        \n        return hidden_states\n\nclass MultiHeadSelfAttention(nn.Module):\n    """\n    BERT\u7684\u591a\u5934\u81ea\u6ce8\u610f\u529b\n    \u4e0eGPT\u7684\u4e3b\u8981\u533a\u522b\u662f\u6ca1\u6709\u56e0\u679c\u63a9\u7801\uff0c\u53ef\u4ee5\u770b\u5230\u5b8c\u6574\u7684\u5e8f\u5217\n    """\n    \n    def __init__(self, config):\n        super().__init__()\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = config.hidden_size // config.num_attention_heads\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        \n        # \u5206\u79bb\u7684Q\u3001K\u3001V\u6295\u5f71\uff08\u4e0eGPT\u7684\u5408\u5e76\u65b9\u5f0f\u4e0d\u540c\uff09\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        \n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n    \n    def forward(self, hidden_states, attention_mask=None):\n        batch_size, seq_length, _ = hidden_states.size()\n        \n        # \u8ba1\u7b97Q\u3001K\u3001V\u5e76\u91cd\u5851\u4e3a\u591a\u5934\n        query_layer = self.transpose_for_scores(self.query(hidden_states))\n        key_layer = self.transpose_for_scores(self.key(hidden_states))\n        value_layer = self.transpose_for_scores(self.value(hidden_states))\n        \n        # \u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        \n        # \u5e94\u7528\u6ce8\u610f\u529b\u63a9\u7801\uff08\u4e3b\u8981\u7528\u4e8epadding\uff09\n        if attention_mask is not None:\n            # attention_mask\u662f1/0\u77e9\u9635\uff0c1\u8868\u793a\u771f\u5b9etoken\uff0c0\u8868\u793apadding\n            extended_attention_mask = (1.0 - attention_mask) * -10000.0\n            attention_scores = attention_scores + extended_attention_mask\n        \n        # \u6ce8\u610f\uff1a\u8fd9\u91cc\u6ca1\u6709\u56e0\u679c\u63a9\u7801\uff0c\u6bcf\u4e2a\u4f4d\u7f6e\u53ef\u4ee5\u770b\u5230\u6240\u6709\u4f4d\u7f6e\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n        attention_probs = self.dropout(attention_probs)\n        \n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        context_layer = context_layer.view(batch_size, seq_length, self.all_head_size)\n        \n        # \u8f93\u51fa\u6295\u5f71\n        output = self.dense(context_layer)\n        return output\n    \n    def transpose_for_scores(self, x):\n        batch_size, seq_length, _ = x.size()\n        x = x.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n        return x.permute(0, 2, 1, 3)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"23-t5-\u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0",children:"2.3 T5 \u67b6\u6784\u8be6\u89e3\u4e0e\u5b9e\u73b0"}),"\n",(0,i.jsx)(e.p,{children:'T5 \u7684\u8bbe\u8ba1\u54f2\u5b66\u662f"\u7edf\u4e00\u5373\u662f\u529b\u91cf"\u3002\u5b83\u5c06\u6240\u6709 NLP \u4efb\u52a1\u90fd\u8f6c\u6362\u4e3a\u6587\u672c\u5230\u6587\u672c\u7684\u683c\u5f0f\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u578b\u3001\u635f\u5931\u51fd\u6570\u548c\u8d85\u53c2\u6570\u3002T5 \u4f7f\u7528\u4e86\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u800c\u4e0d\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u8fd9\u8ba9\u5b83\u80fd\u591f\u66f4\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u957f\u5ea6\u7684\u5e8f\u5217\u3002'}),"\n",(0,i.jsx)(e.p,{children:"T5 \u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u9700\u8981\u5904\u7406\u4e09\u79cd\u6ce8\u610f\u529b\uff1a\u7f16\u7801\u5668\u81ea\u6ce8\u610f\u529b\uff08\u53cc\u5411\uff09\u3001\u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\uff08\u56e0\u679c\uff09\u548c\u89e3\u7801\u5668-\u7f16\u7801\u5668\u4ea4\u53c9\u6ce8\u610f\u529b\uff08\u8ba9\u89e3\u7801\u5668\u5173\u6ce8\u7f16\u7801\u5668\u7684\u8f93\u51fa\uff09\u3002"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class T5Block(nn.Module):\n    """\n    T5\u7684Transformer\u5757\uff0c\u53ef\u914d\u7f6e\u4e3a\u7f16\u7801\u5668\u6216\u89e3\u7801\u5668\n    T5\u4f7f\u7528\u7b80\u5316\u7684\u5c42\u5f52\u4e00\u5316\uff08RMSNorm\uff09\u548c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\n    """\n    \n    def __init__(self, config, is_decoder=False):\n        super().__init__()\n        self.is_decoder = is_decoder\n        \n        # T5\u4f7f\u7528Pre-LN\uff0c\u4f46\u4f7f\u7528\u7b80\u5316\u7248\u7684RMSNorm\u800c\u4e0d\u662f\u6807\u51c6LayerNorm\n        self.layer_norm_1 = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        \n        # \u81ea\u6ce8\u610f\u529b\u5c42\n        self.self_attention = T5Attention(\n            config,\n            is_decoder=is_decoder,\n            use_bias=False,  # T5\u4e0d\u4f7f\u7528bias\u4ee5\u51cf\u5c11\u53c2\u6570\n            relative_attention_bias=True  # \u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\n        )\n        self.dropout_1 = nn.Dropout(config.dropout_rate)\n        \n        # \u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff08\u4ec5\u89e3\u7801\u5668\u4f7f\u7528\uff09\n        if is_decoder:\n            self.layer_norm_2 = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n            self.cross_attention = T5Attention(\n                config,\n                is_decoder=False,  # \u4ea4\u53c9\u6ce8\u610f\u529b\u4e0d\u9700\u8981\u56e0\u679c\u63a9\u7801\n                use_bias=False,\n                relative_attention_bias=False  # \u4ea4\u53c9\u6ce8\u610f\u529b\u4e0d\u9700\u8981\u76f8\u5bf9\u4f4d\u7f6e\n            )\n            self.dropout_2 = nn.Dropout(config.dropout_rate)\n        \n        # FFN\u5c42\n        self.layer_norm_3 = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.ffn = T5FFN(config)\n        self.dropout_3 = nn.Dropout(config.dropout_rate)\n    \n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n    ):\n        # \u81ea\u6ce8\u610f\u529b\n        normed_hidden_states = self.layer_norm_1(hidden_states)\n        attention_output = self.self_attention(\n            normed_hidden_states,\n            mask=attention_mask,\n        )\n        hidden_states = hidden_states + self.dropout_1(attention_output)\n        \n        # \u4ea4\u53c9\u6ce8\u610f\u529b\uff08\u4ec5\u89e3\u7801\u5668\uff09\n        if self.is_decoder and encoder_hidden_states is not None:\n            normed_hidden_states = self.layer_norm_2(hidden_states)\n            cross_attention_output = self.cross_attention(\n                normed_hidden_states,\n                key_value_states=encoder_hidden_states,\n                mask=encoder_attention_mask,\n            )\n            hidden_states = hidden_states + self.dropout_2(cross_attention_output)\n        \n        # FFN\n        normed_hidden_states = self.layer_norm_3(hidden_states)\n        ffn_output = self.ffn(normed_hidden_states)\n        hidden_states = hidden_states + self.dropout_3(ffn_output)\n        \n        return hidden_states\n\nclass T5Attention(nn.Module):\n    """\n    T5\u7684\u6ce8\u610f\u529b\u673a\u5236\n    \u652f\u6301\u7f16\u7801\u5668\uff08\u53cc\u5411\uff09\u3001\u89e3\u7801\u5668\uff08\u56e0\u679c\uff09\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\n    \u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u800c\u4e0d\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\n    """\n    \n    def __init__(self, config, is_decoder=False, use_bias=False, relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = is_decoder\n        self.relative_attention_bias = relative_attention_bias\n        self.d_model = config.d_model\n        self.n_heads = config.num_heads\n        self.d_kv = config.d_kv  # T5\u5141\u8bb8K\u3001V\u7684\u7ef4\u5ea6\u4e0eQ\u4e0d\u540c\uff0c\u4ee5\u8282\u7701\u53c2\u6570\n        \n        # Q\u3001K\u3001V\u6295\u5f71\n        self.q = nn.Linear(self.d_model, self.d_model, bias=use_bias)\n        self.k = nn.Linear(self.d_model, self.d_kv * self.n_heads, bias=use_bias)\n        self.v = nn.Linear(self.d_model, self.d_kv * self.n_heads, bias=use_bias)\n        self.o = nn.Linear(self.d_model, self.d_model, bias=use_bias)\n        \n        # \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\n        if self.relative_attention_bias:\n            self.relative_attention_bias = nn.Embedding(32, self.n_heads)\n    \n    def forward(self, hidden_states, key_value_states=None, mask=None):\n        batch_size, seq_length = hidden_states.shape[:2]\n        \n        # \u5982\u679c\u63d0\u4f9b\u4e86key_value_states\uff0c\u8bf4\u660e\u662f\u4ea4\u53c9\u6ce8\u610f\u529b\n        if key_value_states is None:\n            key_value_states = hidden_states\n        \n        # \u8ba1\u7b97Q\u3001K\u3001V\n        q = self.q(hidden_states).view(batch_size, seq_length, self.n_heads, self.d_model // self.n_heads)\n        k = self.k(key_value_states).view(batch_size, -1, self.n_heads, self.d_kv)\n        v = self.v(key_value_states).view(batch_size, -1, self.n_heads, self.d_kv)\n        \n        # \u8f6c\u7f6e\u4e3a(batch, heads, seq_len, dim)\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n        \n        # \u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_kv)\n        \n        # \u6dfb\u52a0\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e\n        if self.relative_attention_bias:\n            position_bias = self.compute_position_bias(seq_length)\n            scores = scores + position_bias\n        \n        # \u5e94\u7528\u63a9\u7801\uff08\u56e0\u679c\u63a9\u7801\u6216padding\u63a9\u7801\uff09\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        # \u5982\u679c\u662f\u89e3\u7801\u5668\uff0c\u5e94\u7528\u56e0\u679c\u63a9\u7801\n        if self.is_decoder:\n            causal_mask = torch.tril(torch.ones(seq_length, seq_length)).to(scores.device)\n            scores = scores.masked_fill(causal_mask == 0, -1e9)\n        \n        # Softmax\u548cdropout\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = F.dropout(attn_weights, p=0.1, training=self.training)\n        \n        # \u5e94\u7528\u6ce8\u610f\u529b\u6743\u91cd\n        attn_output = torch.matmul(attn_weights, v)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(\n            batch_size, seq_length, self.d_model\n        )\n        \n        # \u8f93\u51fa\u6295\u5f71\n        attn_output = self.o(attn_output)\n        return attn_output\n    \n    def compute_position_bias(self, length):\n        """\u8ba1\u7b97\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e"""\n        # T5\u4f7f\u7528\u5b66\u4e60\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\n        # \u4f4d\u7f6e\u5dee\u5f02\u88ab\u5206\u6876\uff0c\u8fd1\u8ddd\u79bb\u6709\u66f4\u7ec6\u7c92\u5ea6\u7684\u533a\u5206\n        context_position = torch.arange(length, dtype=torch.long)[:, None]\n        memory_position = torch.arange(length, dtype=torch.long)[None, :]\n        \n        relative_position = memory_position - context_position\n        relative_position_bucket = self._relative_position_bucket(relative_position)\n        \n        values = self.relative_attention_bias(relative_position_bucket)\n        values = values.permute([2, 0, 1]).unsqueeze(0)\n        return values\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u4e09\u67b6\u6784\u7279\u6027\u5bf9\u6bd4",children:"\u4e09\u3001\u67b6\u6784\u7279\u6027\u5bf9\u6bd4"}),"\n",(0,i.jsx)(e.h3,{id:"31-\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790",children:"3.1 \u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"\u67b6\u6784\u7279\u6027"}),(0,i.jsx)(e.th,{children:"GPT"}),(0,i.jsx)(e.th,{children:"BERT"}),(0,i.jsx)(e.th,{children:"T5"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"\u65f6\u95f4\u590d\u6742\u5ea6"})}),(0,i.jsx)(e.td,{children:"O(n\xb2)"}),(0,i.jsx)(e.td,{children:"O(n\xb2)"}),(0,i.jsx)(e.td,{children:"O(n\xb2+nm)"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"\u7a7a\u95f4\u590d\u6742\u5ea6"})}),(0,i.jsx)(e.td,{children:"O(n\xb2)"}),(0,i.jsx)(e.td,{children:"O(n\xb2)"}),(0,i.jsx)(e.td,{children:"O(n\xb2+nm)"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"\u5e76\u884c\u5ea6"})}),(0,i.jsx)(e.td,{children:"\u4f4e\uff08\u751f\u6210\u65f6\u4e32\u884c\uff09"}),(0,i.jsx)(e.td,{children:"\u9ad8\uff08\u5b8c\u5168\u5e76\u884c\uff09"}),(0,i.jsx)(e.td,{children:"\u4e2d\u7b49"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"KV\u7f13\u5b58\u4f18\u5316"})}),(0,i.jsx)(e.td,{children:"\u652f\u6301"}),(0,i.jsx)(e.td,{children:"\u4e0d\u9700\u8981"}),(0,i.jsx)(e.td,{children:"\u4ec5\u89e3\u7801\u5668\u652f\u6301"})]})]})]}),"\n",(0,i.jsx)(e.p,{children:"\u5176\u4e2d n \u662f\u5e8f\u5217\u957f\u5ea6\uff0cm \u662f\u7f16\u7801\u5668\u5e8f\u5217\u957f\u5ea6\uff08\u7528\u4e8e T5 \u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\u3002"}),"\n",(0,i.jsx)(e.h3,{id:"32-\u67b6\u6784\u8bbe\u8ba1\u6743\u8861",children:"3.2 \u67b6\u6784\u8bbe\u8ba1\u6743\u8861"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"GPT \u7684\u6743\u8861"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u4f18\u52bf\uff1a\u8bad\u7ec3\u548c\u63a8\u7406\u4e00\u81f4\u6027\u597d\uff0c\u6269\u5c55\u6027\u4f18\u79c0\uff0c\u5de5\u7a0b\u5b9e\u73b0\u7b80\u5355"}),"\n",(0,i.jsx)(e.li,{children:"\u52a3\u52bf\uff1a\u4e0d\u80fd\u5229\u7528\u540e\u6587\u4fe1\u606f\uff0c\u751f\u6210\u901f\u5ea6\u53d7\u81ea\u56de\u5f52\u9650\u5236"}),"\n",(0,i.jsx)(e.li,{children:"\u8bbe\u8ba1\u54f2\u5b66\uff1a\u7b80\u5355\u7edf\u4e00\uff0c\u901a\u8fc7\u89c4\u6a21\u5f25\u8865\u67b6\u6784\u9650\u5236"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"BERT \u7684\u6743\u8861"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u4f18\u52bf\uff1a\u53cc\u5411\u7406\u89e3\u80fd\u529b\u5f3a\uff0c\u7279\u5f81\u63d0\u53d6\u6548\u679c\u597d\uff0c\u63a8\u7406\u901f\u5ea6\u5feb"}),"\n",(0,i.jsx)(e.li,{children:"\u52a3\u52bf\uff1a\u4e0d\u80fd\u76f4\u63a5\u751f\u6210\uff0c\u5b58\u5728\u9884\u8bad\u7ec3-\u5fae\u8c03\u5dee\u5f02\uff0c\u6269\u5c55\u6027\u53d7\u9650"}),"\n",(0,i.jsx)(e.li,{children:"\u8bbe\u8ba1\u54f2\u5b66\uff1a\u6df1\u5ea6\u7406\u89e3\uff0c\u4e13\u6ce8\u4e8e\u7f16\u7801\u5668\u4efb\u52a1"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"T5 \u7684\u6743\u8861"}),"\uff1a"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u4f18\u52bf\uff1a\u4efb\u52a1\u7edf\u4e00\uff0c\u65e2\u80fd\u7406\u89e3\u53c8\u80fd\u751f\u6210\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u6548\u679c\u597d"}),"\n",(0,i.jsx)(e.li,{children:"\u52a3\u52bf\uff1a\u53c2\u6570\u91cf\u5927\uff08\u53cc\u6808\u7ed3\u6784\uff09\uff0c\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u63a8\u7406\u590d\u6742"}),"\n",(0,i.jsx)(e.li,{children:"\u8bbe\u8ba1\u54f2\u5b66\uff1a\u7edf\u4e00\u6846\u67b6\uff0c\u4e00\u4e2a\u6a21\u578b\u89e3\u51b3\u6240\u6709\u4efb\u52a1"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"\u56db\u73b0\u4ee3\u53d1\u5c55\u8d8b\u52bf",children:"\u56db\u3001\u73b0\u4ee3\u53d1\u5c55\u8d8b\u52bf"}),"\n",(0,i.jsx)(e.h3,{id:"41-\u4e3a\u4ec0\u4e48-gpt-\u6210\u4e3a\u4e3b\u6d41",children:"4.1 \u4e3a\u4ec0\u4e48 GPT \u6210\u4e3a\u4e3b\u6d41"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u6269\u5c55\u5b9a\u5f8b\uff08Scaling Laws\uff09\u9a8c\u8bc1"}),"\uff1aGPT \u67b6\u6784\u5728\u53c2\u6570\u91cf\u589e\u52a0\u65f6\u6027\u80fd\u63d0\u5347\u6700\u7a33\u5b9a"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u5de5\u7a0b\u53cb\u597d"}),"\uff1a\u5355\u6808\u7ed3\u6784\u7b80\u5355\uff0c\u4f18\u5316\u6280\u672f\u6210\u719f\uff08KV cache\u3001Flash Attention\u7b49\uff09"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u751f\u6210\u80fd\u529b\u9700\u6c42"}),"\uff1a\u5f53\u524d\u5e94\u7528\u573a\u666f\uff08\u5bf9\u8bdd\u3001\u4ee3\u7801\u751f\u6210\uff09\u90fd\u9700\u8981\u5f3a\u751f\u6210\u80fd\u529b"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u8bad\u7ec3\u6548\u7387"}),"\uff1a\u76f8\u6bd4 T5 \u7684\u53cc\u6808\u7ed3\u6784\uff0c\u5355\u6808\u7684 GPT \u8bad\u7ec3\u6548\u7387\u66f4\u9ad8"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"42-\u67b6\u6784\u521b\u65b0\u65b9\u5411",children:"4.2 \u67b6\u6784\u521b\u65b0\u65b9\u5411"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u957f\u4e0a\u4e0b\u6587\u5904\u7406"}),"\uff1aRoPE\u3001ALiBi \u7b49\u4f4d\u7f6e\u7f16\u7801\u521b\u65b0\u652f\u6301\u66f4\u957f\u5e8f\u5217"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u7a00\u758f\u6ce8\u610f\u529b"}),"\uff1aFlash Attention\u3001Sparse Transformer \u964d\u4f4e\u590d\u6742\u5ea6"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09"}),"\uff1aMixtral \u7b49\u6a21\u578b\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u63d0\u5347\u5bb9\u91cf"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u67b6\u6784\u641c\u7d22"}),"\uff1a\u81ea\u52a8\u53d1\u73b0\u66f4\u4f18\u7684 Transformer \u53d8\u4f53"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"\u53c2\u8003\u8d44\u6599",children:"\u53c2\u8003\u8d44\u6599"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/1706.03762",children:"Attention Is All You Need - Transformer \u539f\u8bba\u6587"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",children:"Language Models are Unsupervised Multitask Learners - GPT-2"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/1810.04805",children:"BERT: Pre-training of Deep Bidirectional Transformers"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/1910.10683",children:"Exploring the Limits of Transfer Learning with T5"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"http://jalammar.github.io/illustrated-transformer/",children:"The Illustrated Transformer - Jay Alammar"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2304.13712",children:"Transformer \u67b6\u6784\u6f14\u8fdb\u7efc\u8ff0 2024"})}),"\n"]})]})}function c(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(_,{...n})}):_(n)}}}]);