"use strict";(globalThis.webpackChunkyiwen_blog_website=globalThis.webpackChunkyiwen_blog_website||[]).push([[58459],{28453(n,e,i){i.d(e,{R:()=>l,x:()=>r});var s=i(96540);const t={},o=s.createContext(t);function l(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:l(n.components),s.createElement(o.Provider,{value:e},n.children)}},30866(n,e,i){i.r(e),i.d(e,{assets:()=>d,contentTitle:()=>r,default:()=>c,frontMatter:()=>l,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"ai/architectures/transformer/gpt2_simple","title":"GPT-2 \u6781\u7b80\u5b9e\u73b0","description":"\u6982\u8ff0","source":"@site/docs/ai/architectures/transformer/gpt2_simple.md","sourceDirName":"ai/architectures/transformer","slug":"/ai/architectures/transformer/gpt2_simple","permalink":"/yiwen-blog-website/en/docs/ai/architectures/transformer/gpt2_simple","draft":false,"unlisted":false,"editUrl":"https://github.com/Hao-yiwen/yiwen-blog-website/tree/master/docs/ai/architectures/transformer/gpt2_simple.md","tags":[],"version":"current","lastUpdatedAt":1759968000000,"frontMatter":{"title":"GPT-2 \u6781\u7b80\u5b9e\u73b0","sidebar_label":"GPT-2 \u6781\u7b80\u5b9e\u73b0","date":"2025-10-09T00:00:00.000Z","last_update":{"date":"2025-10-09T00:00:00.000Z"}},"sidebar":"aiSidebar","previous":{"title":"FlashAttention","permalink":"/yiwen-blog-website/en/docs/ai/architectures/transformer/flash-attention"},"next":{"title":"GQA \u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b","permalink":"/yiwen-blog-website/en/docs/ai/architectures/transformer/gqa"}}');var t=i(74848),o=i(28453);const l={title:"GPT-2 \u6781\u7b80\u5b9e\u73b0",sidebar_label:"GPT-2 \u6781\u7b80\u5b9e\u73b0",date:new Date("2025-10-09T00:00:00.000Z"),last_update:{date:new Date("2025-10-09T00:00:00.000Z")}},r="GPT-2 \u6781\u7b80\u5b9e\u73b0",d={},a=[{value:"\u6982\u8ff0",id:"\u6982\u8ff0",level:2},{value:"\u5b8c\u6574\u4ee3\u7801\u5b9e\u73b0",id:"\u5b8c\u6574\u4ee3\u7801\u5b9e\u73b0",level:2},{value:"\u67b6\u6784\u8bf4\u660e",id:"\u67b6\u6784\u8bf4\u660e",level:2},{value:"1. \u5d4c\u5165\u5c42(Embedding Layer)",id:"1-\u5d4c\u5165\u5c42embedding-layer",level:3},{value:"2. Transformer Block",id:"2-transformer-block",level:3},{value:"3. \u8bed\u8a00\u6a21\u578b\u5934(LM Head)",id:"3-\u8bed\u8a00\u6a21\u578b\u5934lm-head",level:3},{value:"\u7406\u8bba\u80cc\u666f",id:"\u7406\u8bba\u80cc\u666f",level:2},{value:"\u4f7f\u7528\u8bf4\u660e",id:"\u4f7f\u7528\u8bf4\u660e",level:2}];function f(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"gpt-2-\u6781\u7b80\u5b9e\u73b0",children:"GPT-2 \u6781\u7b80\u5b9e\u73b0"})}),"\n",(0,t.jsx)(e.h2,{id:"\u6982\u8ff0",children:"\u6982\u8ff0"}),"\n",(0,t.jsx)(e.p,{children:"\u672c\u6587\u6863\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bfb\u6027\u4f18\u5148\u7684 GPT-2 \u5b8c\u6574\u5b9e\u73b0,\u65e8\u5728\u5e2e\u52a9\u7406\u89e3 Transformer \u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u67b6\u6784\u548c\u5168\u6d41\u7a0b\u5de5\u4f5c\u539f\u7406\u3002\u8be5\u5b9e\u73b0\u5305\u542b\u4ee5\u4e0b\u5173\u952e\u7ec4\u4ef6:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:"\u7edd\u5bf9\u4f4d\u7f6e\u5d4c\u5165(Absolute Position Embedding)"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:"\u9884\u5f52\u4e00\u5316(Pre-Normalization)"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:"\u906e\u7f69\u591a\u5934\u6ce8\u610f\u529b(Masked Multi-Head Attention)"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:"\u524d\u9988\u7f51\u7edc(MLP)"})}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"\u5b8c\u6574\u4ee3\u7801\u5b9e\u73b0",children:"\u5b8c\u6574\u4ee3\u7801\u5b9e\u73b0"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:'# \u4e00\u4e2a\u53ef\u8bfb\u6027\u4f18\u5148\u7684 GPT-2 \u6781\u7b80\u5b9e\u73b0\uff1a\u7edd\u5bf9\u4f4d\u7f6e\u5d4c\u5165 + \u9884\u5f52\u4e00\u5316 + \u906e\u7f69\u591a\u5934\u6ce8\u610f\u529b + MLP\n# \u4ec5\u7528\u4e8e\u7406\u89e3\u5168\u6d41\u7a0b\u4e0e\u8c03\u8bd5\uff1b\u7701\u7565\u4e86\u5e76\u884c/\u7f13\u5b58/FlashAttn \u7b49\u5de5\u7a0b\u4f18\u5316\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ======= \u914d\u7f6e =======\nclass GPTConfig:\n    def __init__(\n        self,\n        vocab_size: int = 50257,  # \u8bcd\u8868\u5927\u5c0f\n        block_size: int = 128,    # \u6700\u5927\u5e8f\u5217\u957f\u5ea6 T\n        n_layer: int = 4,         # Transformer \u5c42\u6570\n        n_head: int = 8,          # \u6ce8\u610f\u529b\u5934\u6570\n        n_embd: int = 512,        # \u6a21\u578b\u7ef4\u5ea6 d_model\n        dropout: float = 0.1,     # dropout \u6982\u7387\n        tie_weights: bool = True, # LM \u5934\u4e0e\u8bcd\u5d4c\u5165\u6743\u91cd\u5171\u4eab\uff08GPT-2 \u98ce\u683c\uff09\n        debug: bool = False,      # \u8c03\u8bd5\u5f62\u72b6\u6253\u5370\n    ):\n        self.vocab_size = vocab_size\n        self.block_size = block_size\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.n_embd = n_embd\n        self.dropout = dropout\n        self.tie_weights = tie_weights\n        self.debug = debug\n\n\n# ======= \u906e\u7f69\u591a\u5934\u81ea\u6ce8\u610f\u529b =======\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0, "n_embd \u5fc5\u987b\u80fd\u6574\u9664 n_head"\n        self.n_head = config.n_head\n        self.head_dim = config.n_embd // config.n_head\n        self.dropout = config.dropout\n        self.debug = config.debug\n\n        # \u5c06\u8f93\u5165\u4e00\u6b21\u6027\u7ebf\u6027\u6620\u5c04\u4e3a Q,K,V\uff08\u7ef4\u5ea6 3*n_embd\uff09\uff0c\u7136\u540e\u518d\u62c6\u5206\n        self.qkv = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n        # \u6ce8\u610f\u529b\u8f93\u51fa\u7684\u6295\u5f71\uff08\u5bf9\u5e94\u8bba\u6587\u91cc\u7684 W_O\uff09\n        self.proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n\n        # \u9884\u751f\u6210\u56e0\u679c\u906e\u7f69\uff0c\u4e0b\u4e09\u89d2\u4e3a True\uff08\u5141\u8bb8\u770b\uff09\uff0c\u4e0a\u4e09\u89d2\u4e3a False\uff08\u5c4f\u853d\uff09\n        mask = torch.tril(torch.ones(config.block_size, config.block_size, dtype=torch.bool))\n        # \u5f62\u72b6 [1,1,T,T] \u4ee5\u4fbf\u5e7f\u64ad\u5230 [B,h,T,T]\n        self.register_buffer("attn_mask", mask.view(1, 1, config.block_size, config.block_size), persistent=False)\n\n        self.attn_drop = nn.Dropout(self.dropout) \n        self.resid_drop = nn.Dropout(self.dropout)\n\n    def forward(self, x: torch.Tensor):\n        # x: [B, T, d_model]\n        B, T, C = x.shape\n        if self.debug:\n            print(f"[Attn] x: {x.shape}")\n\n        # \u4e00\u6b21\u6027\u5f97\u5230 q,k,v: [B, T, 3*d_model]\n        qkv = self.qkv(x)\n        if self.debug:\n            print(f"[Attn] qkv: {qkv.shape}")\n\n        # \u5207\u5206\u5e76\u91cd\u6392\u4e3a\u591a\u5934\uff1aq/k/v: [B, n_head, T, head_dim]\n        q, k, v = qkv.split(C, dim=2)\n        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B,h,T,d]\n        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B,h,T,d]\n        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B,h,T,d]\n        if self.debug:\n            print(f"[Attn] q: {q.shape}, k: {k.shape}, v: {v.shape}")\n\n        # \u6ce8\u610f\u529b\u5206\u6570: [B,h,T,T]\n        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        # \u56e0\u679c\u906e\u7f69\uff1a\u53ea\u5141\u8bb8\u770b\u5f53\u524d\u4f4d\u7f6e\u53ca\u5de6\u4fa7\n        mask = self.attn_mask[:, :, :T, :T]  # \u88c1\u526a\u5230\u5f53\u524d T\n        att = att.masked_fill(~mask, float("-inf"))\n\n        # softmax -> dropout\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        if self.debug:\n            print(f"[Attn] att(sftmx): {att.shape}, sum over last dim\u22481 -> {att[0,0,0,:5].sum().item():.3f}")\n\n        # \u52a0\u6743\u6c42\u548c\u62ff\u5230\u8f93\u51fa\uff1a [B,h,T,d]\n        y = att @ v\n\n        # \u5408\u5e76\u5934\uff1a [B,T,h*d]\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        # \u8f93\u51fa\u6295\u5f71 + dropout\n        y = self.resid_drop(self.proj(y))\n        if self.debug:\n            print(f"[Attn] out: {y.shape}")\n        return y\n\n\n# ======= \u4e24\u5c42 MLP\uff08GELU\uff09 =======\nclass MLP(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.fc1 = nn.Linear(config.n_embd, 4 * config.n_embd)  # \u6269\u5f20\n        self.fc2 = nn.Linear(4 * config.n_embd, config.n_embd)  # \u6295\u56de\n        self.act = nn.GELU()\n        self.drop = nn.Dropout(config.dropout)\n        self.debug = config.debug\n\n    def forward(self, x):\n        if self.debug:\n            print(f"[MLP] in: {x.shape}")\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        if self.debug:\n            print(f"[MLP] out: {x.shape}")\n        return x\n\n\n# ======= Transformer Block\uff08\u9884\u5f52\u4e00\u5316\uff09 =======\nclass Block(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n        self.debug = config.debug\n\n    def forward(self, x):\n        if self.debug:\n            print(f"[Block] in: {x.shape}")\n        # \u6ce8\u610f\u529b\u5b50\u5c42\uff08Pre-LN\uff09\n        x = x + self.attn(self.ln1(x))\n        # MLP \u5b50\u5c42\uff08Pre-LN\uff09\n        x = x + self.mlp(self.ln2(x))\n        if self.debug:\n            print(f"[Block] out: {x.shape}")\n        return x\n\n\n# ======= GPT \u4e3b\u4f53 =======\nclass GPT(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.config = config\n\n        # \u8bcd\u5d4c\u5165 & \u7edd\u5bf9\u4f4d\u7f6e\u5d4c\u5165\uff08GPT-2 \u98ce\u683c\uff09\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)       # token embedding\n        self.wpe = nn.Embedding(config.block_size, config.n_embd)       # position embedding\n\n        self.drop = nn.Dropout(config.dropout)\n\n        # N \u5c42 Block\n        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n\n        # \u6700\u7ec8 LayerNorm\n        self.ln_f = nn.LayerNorm(config.n_embd)\n\n        # \u8bcd\u8868\u6295\u5f71\u5934\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        # \u6743\u91cd\u5171\u4eab\uff1alm_head.weight \u4e0e wte.weight \u7ed1\u5b9a\uff08GPT-2 \u4e60\u60ef\uff09\n        if config.tie_weights:\n            self.lm_head.weight = self.wte.weight\n\n        # \u53c2\u6570\u521d\u59cb\u5316\uff08\u63a5\u8fd1 GPT-2 \u7684\u7b80\u5355\u65b9\u6848\uff09\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n        """\n        idx: [B,T] \u7684 token id\n        targets: [B,T] \u7684\u6807\u7b7e\uff08\u53ef\u9009\uff1b\u4f20\u5165\u5219\u8fd4\u56de\u4ea4\u53c9\u71b5 loss\uff09\n        """\n        B, T = idx.size()\n        assert T <= self.config.block_size, "\u5e8f\u5217\u957f\u5ea6\u8d85\u8fc7\u4e86 block_size"\n\n        if self.config.debug:\n            print(f"\\n=== Forward: idx {idx.shape} ===")\n\n        # \u6784\u9020\u4f4d\u7f6e\u7d22\u5f15 [0..T-1]\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)  # [1,T]\n\n        # \u8bcd\u5d4c\u5165 + \u4f4d\u7f6e\u5d4c\u5165\n        tok_emb = self.wte(idx)        # [B,T,d]\n        pos_emb = self.wpe(pos)        # [1,T,d]\uff0c\u4f1a\u5e7f\u64ad\u5230 [B,T,d]\n        x = self.drop(tok_emb + pos_emb)\n        if self.config.debug:\n            print(f"[Emb] tok_emb: {tok_emb.shape}, pos_emb: {pos_emb.shape}, x: {x.shape}")\n\n        # \u5806\u53e0\u7684 Transformer blocks\n        for i, block in enumerate(self.h, start=1):\n            if self.config.debug:\n                print(f"\\n-- Block {i} --")\n            x = block(x)\n\n        # \u6700\u7ec8 LN + \u8bcd\u8868\u5934\n        x = self.ln_f(x)               # [B,T,d]\n        logits = self.lm_head(x)       # [B,T,vocab]\n        if self.config.debug:\n            print(f"\\n[Head] logits: {logits.shape}")\n\n        # \u8bad\u7ec3\u65f6\u53ef\u76f4\u63a5\u7b97 CE loss\n        loss = None\n        if targets is not None:\n            # \u628a (B,T,V) \u6539\u6210 (B*T,V) \u4e0e (B*T,)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n        return logits, loss\n\n    @torch.no_grad()\n    def generate(self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0, top_k: int | None = None):\n        """\n        \u6734\u7d20\u81ea\u56de\u5f52\u751f\u6210\uff08\u4e0d\u505a KV Cache\uff0c\u9002\u5408\u5c0f\u89c4\u6a21\u6f14\u793a\uff09\n        idx: [B,T] \u521d\u59cb\u63d0\u793a\n        """\n        for _ in range(max_new_tokens):\n            # \u53ea\u4fdd\u7559\u6700\u8fd1 block_size \u4e2a token\n            idx_cond = idx[:, -self.config.block_size:]\n            logits, _ = self(idx_cond)\n            # \u53d6\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u5206\u5e03\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                thresh = v[:, -1].unsqueeze(-1)\n                logits = torch.where(logits < thresh, torch.full_like(logits, float("-inf")), logits)\n            probs = F.softmax(logits, dim=-1)\n            next_id = torch.multinomial(probs, num_samples=1)  # \u91c7\u6837\u4e00\u4e2a token\n            idx = torch.cat([idx, next_id], dim=1)\n        return idx\n\n\n# ======= \u6700\u5c0f\u53ef\u8fd0\u884c\u793a\u4f8b =======\nif __name__ == "__main__":\n    # \u5c0f\u6a21\u578b + \u5c0f\u8bcd\u8868\uff0c\u65b9\u4fbf CPU/GPU \u90fd\u80fd\u8dd1\n    cfg = GPTConfig(\n        vocab_size=100,   # \u73a9\u5177\u8bcd\u8868\n        block_size=16,    # \u6700\u957f 16 token\n        n_layer=2,\n        n_head=4,\n        n_embd=64,\n        dropout=0.1,\n        debug=True,       # \u6253\u5f00\u8c03\u8bd5\u6253\u5370\uff08\u5f62\u72b6/\u5173\u952e\u4e2d\u95f4\u91cf\uff09\n    )\n    device = "cuda" if torch.cuda.is_available() else "cpu"\n    model = GPT(cfg).to(device)\n    model.train()\n\n    # \u6784\u9020\u4e00\u6279\u5047\u6570\u636e\uff1aB=2, T=8\n    B, T = 2, 8\n    x = torch.randint(0, cfg.vocab_size, (B, T), device=device)\n    y = torch.randint(0, cfg.vocab_size, (B, T), device=device)\n\n    # \u5355\u6b21\u524d\u5411 + \u8ba1\u7b97 loss\n    logits, loss = model(x, y)\n    print(f"\\nLoss: {loss.item():.4f}")\n\n    # \u53cd\u5411\u4e0e\u4e00\u6b21\u4f18\u5316\u6b65\uff08\u6f14\u793a\uff09\n    opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    opt.zero_grad()\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # \u53ef\u9009\uff1a\u68af\u5ea6\u88c1\u526a\n    opt.step()\n    print("One optimization step done.")\n\n    # \u5207\u63a8\u7406\u6a21\u5f0f\uff0c\u6f14\u793a\u751f\u6210\n    model.eval()\n    start = torch.randint(0, cfg.vocab_size, (1, 4), device=device)  # \u63d0\u793a 4 token\n    out = model.generate(start, max_new_tokens=8, temperature=1.0, top_k=20)\n    print("Generated ids:", out.tolist())\n'})}),"\n",(0,t.jsx)(e.h2,{id:"\u67b6\u6784\u8bf4\u660e",children:"\u67b6\u6784\u8bf4\u660e"}),"\n",(0,t.jsx)(e.p,{children:"\u672c\u5b9e\u73b0\u5b8c\u6574\u590d\u73b0\u4e86 GPT-2 \u8bba\u6587\u7684\u6838\u5fc3\u67b6\u6784,\u4e3b\u8981\u5305\u542b\u4ee5\u4e0b\u6a21\u5757:"}),"\n",(0,t.jsx)(e.h3,{id:"1-\u5d4c\u5165\u5c42embedding-layer",children:"1. \u5d4c\u5165\u5c42(Embedding Layer)"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Token Embedding"}),": \u5c06\u8bcd\u6c47\u8868\u4e2d\u7684 token \u6620\u5c04\u4e3a\u5411\u91cf\u8868\u793a"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Position Embedding"}),": \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801,\u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u5b66\u4e60\u72ec\u7acb\u7684\u5d4c\u5165\u5411\u91cf"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"2-transformer-block",children:"2. Transformer Block"}),"\n",(0,t.jsx)(e.p,{children:"\u6bcf\u4e2a Transformer \u5c42\u91c7\u7528**\u9884\u5f52\u4e00\u5316(Pre-LN)**\u67b6\u6784:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u6b8b\u5dee\u8fde\u63a5(Residual Connection)"}),": \u6709\u6548\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u906e\u7f69\u591a\u5934\u6ce8\u610f\u529b(Causal Self-Attention)"}),": \u786e\u4fdd\u81ea\u56de\u5f52\u7279\u6027,\u53ea\u80fd\u5173\u6ce8\u5f53\u524d\u4f4d\u7f6e\u53ca\u4e4b\u524d\u7684\u5185\u5bb9"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u524d\u9988\u7f51\u7edc(MLP)"}),": \u4e24\u5c42\u5168\u8fde\u63a5\u7f51\u7edc,\u4e2d\u95f4\u7ef4\u5ea6\u4e3a ",(0,t.jsx)(e.code,{children:"4 * n_embd"}),",\u4f7f\u7528 GELU \u6fc0\u6d3b\u51fd\u6570"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"3-\u8bed\u8a00\u6a21\u578b\u5934lm-head",children:"3. \u8bed\u8a00\u6a21\u578b\u5934(LM Head)"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"\u5c06\u6700\u7ec8\u7684\u9690\u85cf\u72b6\u6001\u6295\u5f71\u56de\u8bcd\u8868\u5927\u5c0f\u7684 logits"}),"\n",(0,t.jsxs)(e.li,{children:["\u652f\u6301",(0,t.jsx)(e.strong,{children:"\u6743\u91cd\u5171\u4eab(Weight Tying)"}),": LM Head \u4e0e Token Embedding \u5171\u4eab\u53c2\u6570,\u51cf\u5c11\u6a21\u578b\u5927\u5c0f"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"\u7406\u8bba\u80cc\u666f",children:"\u7406\u8bba\u80cc\u666f"}),"\n",(0,t.jsxs)(e.p,{children:["\u672c\u5b9e\u73b0\u57fa\u4e8e OpenAI \u7684 GPT-2 \u8bba\u6587:\n",(0,t.jsx)(e.a,{href:"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",children:"\u300aLanguage Models are Unsupervised Multitask Learners\u300b"})]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"\u6838\u5fc3\u601d\u60f3"}),": \u901a\u8fc7\u5927\u89c4\u6a21\u65e0\u76d1\u7763\u9884\u8bad\u7ec3,\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u5230\u4e30\u5bcc\u7684\u8bed\u8a00\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b,\u4ece\u800c\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa zero-shot \u6216 few-shot \u5b66\u4e60\u80fd\u529b,\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002"]}),"\n",(0,t.jsx)(e.h2,{id:"\u4f7f\u7528\u8bf4\u660e",children:"\u4f7f\u7528\u8bf4\u660e"}),"\n",(0,t.jsx)(e.p,{children:"\u8be5\u5b9e\u73b0\u53ef\u7528\u4e8e:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"\u5b66\u4e60 GPT \u67b6\u6784\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406"}),"\n",(0,t.jsx)(e.li,{children:"\u7406\u89e3\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6d41\u7a0b"}),"\n",(0,t.jsx)(e.li,{children:"\u8c03\u8bd5\u548c\u5b9e\u9a8c\u5c0f\u89c4\u6a21 Transformer \u6a21\u578b"}),"\n",(0,t.jsx)(e.li,{children:"\u4f5c\u4e3a\u6559\u5b66\u6f14\u793a\u4ee3\u7801"}),"\n"]}),"\n",(0,t.jsx)(e.admonition,{type:"tip",children:(0,t.jsxs)(e.p,{children:["\u5f00\u542f ",(0,t.jsx)(e.code,{children:"debug=True"})," \u53ef\u4ee5\u6253\u5370\u6bcf\u5c42\u7684 tensor \u5f62\u72b6"]})})]})}function c(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(f,{...n})}):f(n)}}}]);