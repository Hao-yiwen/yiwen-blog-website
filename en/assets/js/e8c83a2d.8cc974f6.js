"use strict";(globalThis.webpackChunkyiwen_blog_website=globalThis.webpackChunkyiwen_blog_website||[]).push([[42055],{72399(e){e.exports=JSON.parse('{"tag":{"label":"attention","permalink":"/yiwen-blog-website/en/docs/tags/attention","allTagsPath":"/yiwen-blog-website/en/docs/tags","count":5,"items":[{"id":"ai/architectures/transformer/flash-attention","title":"FlashAttention \u6280\u672f\u8be6\u89e3","description":"1. \u6982\u8ff0\uff1a\u5b83\u89e3\u51b3\u4e86\u4ec0\u4e48\u95ee\u9898\uff1f","permalink":"/yiwen-blog-website/en/docs/ai/architectures/transformer/flash-attention"},{"id":"ai/architectures/transformer/gqa","title":"Grouped Query Attention (GQA)","description":"1. \u6982\u8ff0 (Overview)","permalink":"/yiwen-blog-website/en/docs/ai/architectures/transformer/gqa"},{"id":"ai/architectures/transformer/attention-mechanism-explained","title":"Transformer \u6ce8\u610f\u529b\u673a\u5236\u7684\u672c\u8d28","description":"Transformer \u67b6\u6784\u4e2d\uff0c\u6ce8\u610f\u529b\u7684\u672c\u8d28\u53ef\u4ee5\u7528\u4e00\u53e5\u8bdd\u6982\u62ec\uff1a\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u52a0\u6743\u6c42\u548c\uff08Weighted Sum based on Relevance\uff09\u3002","permalink":"/yiwen-blog-website/en/docs/ai/architectures/transformer/attention-mechanism-explained"},{"id":"ai/architectures/transformer/positional-encoding","title":"Transformer \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801","description":"1. \u80cc\u666f\uff1a\u4e3a\u4ec0\u4e48 Transformer \u9700\u8981\u4f4d\u7f6e\u7f16\u7801\uff1f","permalink":"/yiwen-blog-website/en/docs/ai/architectures/transformer/positional-encoding"},{"id":"ai/architectures/transformer/masked-self-attention","title":"\u624b\u5199\u5e26\u63a9\u7801\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236","description":"GPT \u7b49\u5927\u6a21\u578b\u4e4b\u6240\u4ee5\u5f3a\u5927\uff0c\u6838\u5fc3\u5728\u4e8e\u5b83\u4eec\u5982\u4f55\\"\u7406\u89e3\\"\u4e0a\u4e0b\u6587\u3002\u800c\u8fd9\u4e00\u5207\u7684\u57fa\u77f3\uff0c\u5c31\u85cf\u5728\u4e0d\u5230 20 \u884c\u7684 PyTorch \u4ee3\u7801\u4e2d\u3002","permalink":"/yiwen-blog-website/en/docs/ai/architectures/transformer/masked-self-attention"}],"unlisted":false}}')}}]);