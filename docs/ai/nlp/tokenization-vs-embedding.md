---
title: åˆ†è¯ä¸Embeddingï¼šä»æ–‡æœ¬åˆ°å‘é‡çš„ä¸¤ä¸ªå…³é”®æ­¥éª¤
sidebar_label: åˆ†è¯ä¸Embedding
date: 2025-11-23
last_update:
  date: 2025-11-23
tags: [NLP, Tokenization, Embedding, Transformer, LLM]
---

# åˆ†è¯ä¸Embeddingï¼šä»æ–‡æœ¬åˆ°å‘é‡çš„ä¸¤ä¸ªå…³é”®æ­¥éª¤

## æ ¸å¿ƒæ¦‚å¿µ

åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œ**åˆ†è¯ï¼ˆTokenizationï¼‰**å’Œ**Embeddingï¼ˆè¯åµŒå…¥ï¼‰**æ˜¯æ–‡æœ¬å¤„ç†çš„ä¸¤ä¸ªè¿ç»­ä½†æœ¬è´¨å®Œå…¨ä¸åŒçš„æ­¥éª¤ï¼š

- **åˆ†è¯**ï¼šå°†"äººç±»è¯­è¨€"è½¬æ¢ä¸º"æœºå™¨èƒ½ç´¢å¼•çš„ID" ğŸ“‘
- **Embedding**ï¼šå°†"ID"è½¬æ¢ä¸º"æœºå™¨èƒ½ç†è§£çš„è¯­ä¹‰å‘é‡" ğŸ§ 

è¿™å°±åƒå»å›¾ä¹¦é¦†æŸ¥èµ„æ–™ï¼š
1. **åˆ†è¯** = æŸ¥ç›®å½•æ‰¾åˆ°ä¹¦çš„ç¼–å·ï¼ˆç´¢å¼•ï¼‰
2. **Embedding** = æ‰“å¼€é‚£æœ¬ä¹¦é˜…è¯»å†…å®¹ï¼ˆç†è§£ï¼‰

## å¤„ç†æµç¨‹å¯¹æ¯”

### å›¾ä¹¦é¦†ç±»æ¯”

å‡è®¾ä½ è¦æŸ¥å…³äº"è‹¹æœ"çš„èµ„æ–™ï¼š

**æ­¥éª¤1ï¼šåˆ†è¯ï¼ˆæŸ¥ç›®å½•ï¼‰**
- **è¾“å…¥**ï¼šæ–‡æœ¬ "Apple"
- **åŠ¨ä½œ**ï¼šåœ¨é¢„å…ˆæ„å»ºçš„è¯è¡¨ï¼ˆBPEè¯è¡¨ï¼‰ä¸­æŸ¥æ‰¾
- **è¾“å‡º**ï¼šæ‰¾åˆ°å¯¹åº”çš„ **ID 502**
- **ç‰¹ç‚¹**ï¼šçº¯ç²¹çš„æŸ¥è¡¨æ“ä½œï¼Œæ— ä»»ä½•è¯­ä¹‰ç†è§£

**æ­¥éª¤2ï¼šEmbeddingï¼ˆè¯»å†…å®¹ï¼‰**
- **è¾“å…¥**ï¼š**ID 502**
- **åŠ¨ä½œ**ï¼šåœ¨æ¨¡å‹çš„EmbeddingçŸ©é˜µä¸­æ£€ç´¢ç¬¬502è¡Œ
- **è¾“å‡º**ï¼šå¾—åˆ°è¯­ä¹‰å‘é‡ `[0.8, -0.1, 0.5, ...]`
- **ç‰¹ç‚¹**ï¼šå‘é‡åŒ…å«"çº¢è‰²"ã€"åœ†å½¢"ã€"å¯é£Ÿç”¨"ç­‰è¯­ä¹‰ä¿¡æ¯

### æ•°æ®æµç¨‹å›¾

```mermaid
graph LR
    A[æ–‡æœ¬: I love AI] -->|åˆ†è¯å™¨ Tokenizer| B[Token IDs: 40, 256, 1024]
    B -->|æŸ¥è¡¨ Lookup| C[EmbeddingçŸ©é˜µ<br/>vocab_size Ã— hidden_dim]
    C -->|æ˜ å°„| D[è¯­ä¹‰å‘é‡åºåˆ—<br/>3 Ã— 768ç»´]
    D -->|è¾“å…¥| E[Transformerå±‚]

    style B fill:#ffe6e6,stroke:#ff6666,stroke-width:2px
    style D fill:#e6f3ff,stroke:#4d94ff,stroke-width:2px

    note1[ç¦»æ•£æ•´æ•°<br/>ä¸å¯å­¦ä¹ ]
    note2[è¿ç»­æµ®ç‚¹æ•°<br/>å¯è®­ç»ƒå‚æ•°]
    B -.-> note1
    D -.-> note2
```

## æ·±åº¦å¯¹æ¯”

| ç»´åº¦ | åˆ†è¯ (Tokenization) | Embedding (è¯åµŒå…¥) |
|:-----|:-------------------|:------------------|
| **æ•°æ®ç±»å‹** | ç¦»æ•£æ•´æ•°ï¼ˆDiscreteï¼‰<br/>`[40, 256, 1024]` | è¿ç»­æµ®ç‚¹æ•°ï¼ˆContinuousï¼‰<br/>`[[0.12, -0.98, ...], ...]` |
| **æ˜¯å¦å¯è®­ç»ƒ** | âŒ å›ºå®šä¸å˜<br/>è®­ç»ƒå‰å®šå¥½ï¼Œè®­ç»ƒä¸­ä¸æ”¹ | âœ… åŠ¨æ€è°ƒæ•´<br/>è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­ä¼˜åŒ– |
| **åŒ…å«è¯­ä¹‰** | âŒ æ— è¯­ä¹‰<br/>ID 10å’ŒID 11å¯èƒ½å®Œå…¨æ— å…³ | âœ… æœ‰è¯­ä¹‰<br/>ç›¸ä¼¼è¯å‘é‡è·ç¦»è¿‘ |
| **ç»´åº¦** | 1ç»´æ ‡é‡<br/>`502` | é«˜ç»´å‘é‡<br/>`768/1024/4096ç»´` |
| **æ•°å­¦è¿ç®—** | ä¸å¯ç›´æ¥è¿ç®—<br/>ä»…ä½œç´¢å¼•ä½¿ç”¨ | å¯è¿›è¡Œå‘é‡è¿ç®—<br/>åŠ å‡ä¹˜é™¤ã€ç‚¹ç§¯ç­‰ |
| **ç»™è°ç”¨** | Embeddingå±‚çš„è¾“å…¥ç´¢å¼• | Transformerå„å±‚çš„æ•°å­¦è®¡ç®— |
| **å­˜å‚¨ä½ç½®** | åˆ†è¯å™¨çš„è¯è¡¨æ–‡ä»¶<br/>`tokenizer.json` | æ¨¡å‹æƒé‡çŸ©é˜µ<br/>`embedding.weight` |

## ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨Token IDè®¡ç®—ï¼Ÿ

### é—®é¢˜æ¼”ç¤º

å‡è®¾ï¼š
- "é¦™è•‰" â†’ ID **10**
- "è‹¹æœ" â†’ ID **20**
- "è¥¿ç“œ" â†’ ID **30**

å¦‚æœç›´æ¥ç”¨IDè¿›è¡Œæ•°å­¦è¿ç®—ä¼šå‡ºç°**ä¸¥é‡é—®é¢˜**ï¼š

âŒ **å¤§å°å…³ç³»æ··ä¹±**
```python
è¥¿ç“œ(30) > è‹¹æœ(20) > é¦™è•‰(10)  # è¯æ±‡æ²¡æœ‰å¤§å°ä¹‹åˆ†ï¼
```

âŒ **è™šå‡çš„æ•°å­¦å…³ç³»**
```python
è¥¿ç“œ(30) = é¦™è•‰(10) + è‹¹æœ(20)  # å®Œå…¨æ²¡æœ‰æ„ä¹‰ï¼
```

âŒ **ç›¸é‚»IDæ— è¯­ä¹‰å…³è”**
```python
ID 100: "çŒ«"
ID 101: "é‡å­åŠ›å­¦"  # IDç›¸é‚»ï¼Œä½†æ¯«æ— å…³ç³»
```

### Embeddingçš„è§£å†³æ–¹æ¡ˆ

å°†IDæ˜ å°„åˆ°**é«˜ç»´è¯­ä¹‰ç©ºé—´**ï¼ˆé€šå¸¸768/1024/4096ç»´ï¼‰ï¼š

```python
# ä¼ªä»£ç ç¤ºä¾‹
embedding_matrix = torch.nn.Embedding(vocab_size=50257, embed_dim=768)

# åˆ†è¯ç»“æœ
token_ids = [10, 20, 30]  # [é¦™è•‰, è‹¹æœ, è¥¿ç“œ]

# è½¬æ¢ä¸ºå‘é‡
embeddings = embedding_matrix(token_ids)
# Shape: [3, 768]
# æ¯ä¸ªè¯éƒ½æ˜¯768ç»´çš„æµ®ç‚¹æ•°å‘é‡
```

åœ¨è¿™ä¸ªç©ºé—´é‡Œï¼š

âœ… **ç›¸ä¼¼è¯èšé›†**
```python
distance("è‹¹æœ", "é¦™è•‰") = 0.12  # è·ç¦»å¾ˆè¿‘ï¼ˆéƒ½æ˜¯æ°´æœï¼‰
distance("è‹¹æœ", "æ±½è½¦") = 3.45  # è·ç¦»å¾ˆè¿œ
```

âœ… **æ”¯æŒè¯­ä¹‰è¿ç®—**
```python
vector("å›½ç‹") - vector("ç”·äºº") + vector("å¥³äºº") â‰ˆ vector("å¥³ç‹")
```

## ä»£ç ç¤ºä¾‹ï¼šå®Œæ•´æµç¨‹

### PyTorchå®ç°

```python
import torch
import torch.nn as nn

# 1. æ¨¡æ‹Ÿåˆ†è¯ç»“æœï¼ˆToken IDsï¼‰
text = "I love AI"
token_ids = torch.tensor([40, 256, 1024])  # å‡è®¾è¿™æ˜¯åˆ†è¯åçš„ç»“æœ

# 2. åˆ›å»ºEmbeddingå±‚
vocab_size = 50000  # è¯è¡¨å¤§å°
embedding_dim = 768  # å‘é‡ç»´åº¦ï¼ˆBERT-baseè§„æ¨¡ï¼‰
embedding_layer = nn.Embedding(vocab_size, embedding_dim)

# 3. IDè½¬å‘é‡
embeddings = embedding_layer(token_ids)

print(f"Token IDs: {token_ids}")
print(f"Embeddings shape: {embeddings.shape}")  # [3, 768]
print(f"ç¬¬ä¸€ä¸ªtokençš„å‘é‡å‰5ç»´: {embeddings[0, :5]}")
```

### å®é™…æ¨¡å‹ä¸­çš„ä½¿ç”¨

```python
from transformers import GPT2Tokenizer, GPT2Model

# åˆå§‹åŒ–
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2Model.from_pretrained("gpt2")

# æ–‡æœ¬è¾“å…¥
text = "Hello, how are you?"

# æ­¥éª¤1ï¼šåˆ†è¯ï¼ˆTokenizationï¼‰
input_ids = tokenizer.encode(text, return_tensors="pt")
print(f"Token IDs: {input_ids}")
# Output: tensor([[15496,   11,   703,   389,   345,    30]])

# æ­¥éª¤2ï¼šEmbeddingï¼ˆåœ¨æ¨¡å‹å†…éƒ¨è‡ªåŠ¨å®Œæˆï¼‰
with torch.no_grad():
    outputs = model(input_ids)
    embeddings = outputs.last_hidden_state  # [1, 6, 768]

print(f"Embedding shape: {embeddings.shape}")
```

## å…³é”®è¦ç‚¹æ€»ç»“

### æœ¬è´¨åŒºåˆ«

```mermaid
mindmap
  root((æ–‡æœ¬å¤„ç†))
    åˆ†è¯ Tokenization
      æŸ¥è¡¨æ“ä½œ
      ç¦»æ•£ç©ºé—´
      ä¸å¯è®­ç»ƒ
      æ— è¯­ä¹‰
    Embedding
      å‘é‡æ˜ å°„
      è¿ç»­ç©ºé—´
      å¯è®­ç»ƒ
      æœ‰è¯­ä¹‰
```

### è®°å¿†å£è¯€

> **åˆ†è¯æ˜¯"ç¼–å·"ï¼ŒEmbeddingæ˜¯"å†…æ¶µ"**
>
> - åˆ†è¯ï¼šç»™æ¯ä¸ªè¯å‘å¼ **èº«ä»½è¯**ï¼ˆIDï¼‰
> - Embeddingï¼šè®°å½•æ¯ä¸ªäººçš„**è¯¦ç»†æ¡£æ¡ˆ**ï¼ˆå‘é‡ï¼‰

### è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–

| é˜¶æ®µ | åˆ†è¯å™¨ | EmbeddingçŸ©é˜µ |
|:-----|:------|:-------------|
| **é¢„è®­ç»ƒå‰** | âœ… å·²å›ºå®š<br/>åŸºäºè¯­æ–™åº“è®­ç»ƒ | ğŸ² éšæœºåˆå§‹åŒ– |
| **é¢„è®­ç»ƒä¸­** | ğŸ”’ ä¿æŒä¸å˜ | ğŸ“ˆ ä¸æ–­ä¼˜åŒ–å­¦ä¹ è¯­ä¹‰ |
| **é¢„è®­ç»ƒå** | ğŸ”’ å†»ç»“ | âœ… å­¦åˆ°ä¸°å¯Œè¯­ä¹‰è¡¨ç¤º |
| **å¾®è°ƒ** | ğŸ”’ é€šå¸¸ä¸å˜ | ğŸ”„ ç»§ç»­è°ƒæ•´é€‚åº”æ–°ä»»åŠ¡ |

## å¸¸è§è¯¯åŒº

### âŒ è¯¯åŒº1ï¼šåˆ†è¯å°±æ˜¯Embedding
```python
# é”™è¯¯ç†è§£
"åˆ†è¯åç›´æ¥å¾—åˆ°å‘é‡" âŒ
```
**æ­£ç¡®**ï¼šåˆ†è¯å¾—åˆ°çš„æ˜¯æ•´æ•°IDï¼Œéœ€è¦é€šè¿‡Embeddingå±‚è½¬æ¢ä¸ºå‘é‡ã€‚

### âŒ è¯¯åŒº2ï¼šToken IDæœ‰è¯­ä¹‰ä¿¡æ¯
```python
# é”™è¯¯ç†è§£
token_id = 502  # è®¤ä¸º502è¿™ä¸ªæ•°å­—æœ¬èº«åŒ…å«"è‹¹æœ"çš„ä¿¡æ¯ âŒ
```
**æ­£ç¡®**ï¼šIDåªæ˜¯ç´¢å¼•ï¼Œè¯­ä¹‰ä¿¡æ¯å­˜å‚¨åœ¨EmbeddingçŸ©é˜µçš„ç¬¬502è¡Œå‘é‡ä¸­ã€‚

### âŒ è¯¯åŒº3ï¼šEmbeddingå±‚åªæ˜¯ç®€å•æŸ¥è¡¨
```python
# éƒ¨åˆ†æ­£ç¡®ä½†ä¸å®Œæ•´
embeddings = embedding_matrix[token_ids]  # âœ… æŸ¥è¡¨
# ä½†EmbeddingçŸ©é˜µæ˜¯å¯è®­ç»ƒå‚æ•°ï¼Œè®­ç»ƒä¸­ä¼šæ›´æ–°ï¼âœ…
```

## æ‹“å±•é˜…è¯»

- [å­è¯åˆ†è¯æ–¹æ³•è¯¦è§£](./subword_tokenization.md)
- [Transformerå®Œæ•´å®ç°](../architectures/transformer/transformer_complete_implementation.md)
- [ä¸ºä»€ä¹ˆBPEæˆä¸ºä¸»æµ](./why-bpe-is-mainstream.md)

## å‚è€ƒèµ„æ–™

1. [Word2Vecè®ºæ–‡](https://arxiv.org/abs/1301.3781) - Mikolov et al., 2013
2. [GloVeè®ºæ–‡](https://nlp.stanford.edu/pubs/glove.pdf) - Pennington et al., 2014
3. [BERTè®ºæ–‡](https://arxiv.org/abs/1810.04805) - Devlin et al., 2018
4. [GPT-2è®ºæ–‡](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - Radford et al., 2019
