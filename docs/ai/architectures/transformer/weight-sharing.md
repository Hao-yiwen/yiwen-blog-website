---
title: Weight Sharing (æƒé‡å…±äº«)
sidebar_label: Weight Sharing æƒé‡å…±äº«
date: 2025-12-07
last_update:
  date: 2025-12-07
tags: [transformer, embedding, lm-head, weight-tying, parameter-efficiency, regularization]
---

import weightShare from '@site/static/img/weight_share.png';

# Weight Sharing (æƒé‡å…±äº«) æŠ€æœ¯æ–‡æ¡£

## 1. æ¦‚è¿° (Overview)

**æƒé‡å…±äº«ï¼ˆWeight Tying/Sharingï¼‰** æ˜¯ç°ä»£è¯­è¨€æ¨¡å‹ä¸­çš„ä¸€ç§é‡è¦ä¼˜åŒ–æŠ€æœ¯ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**æ¨¡å‹çš„è¾“å…¥å±‚ï¼ˆToken Embeddingï¼‰å’Œè¾“å‡ºå±‚ï¼ˆLanguage Model Headï¼‰å…±ç”¨åŒä¸€ä¸ªæƒé‡çŸ©é˜µã€‚**

è¿™ä¸€æŠ€æœ¯ç”± Press & Wolf (2016) åœ¨è®ºæ–‡ *"Using the Output Embedding to Improve Language Models"* ä¸­é¦–æ¬¡ç³»ç»Ÿæ€§æå‡ºï¼Œå¹¶è¢« GPT-2ã€GPT-3ã€LLaMA ç­‰ä¸»æµå¤§è¯­è¨€æ¨¡å‹å¹¿æ³›é‡‡ç”¨ã€‚

### æ ¸å¿ƒä»·å€¼

- **å‡å°‘å‚æ•°é‡ï¼š** èŠ‚çœçº¦ 30-50% çš„ Embedding ç›¸å…³å‚æ•°ï¼ˆå¯¹äºå¤§è¯è¡¨æ¨¡å‹éå¸¸å¯è§‚ï¼‰ã€‚
- **æå‡æ¨¡å‹æ•ˆæœï¼š** é€šè¿‡å¼ºåˆ¶è¯­ä¹‰å¯¹é½ï¼Œå®é™…ä¸Šèƒ½é™ä½å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ã€‚
- **æ­£åˆ™åŒ–æ•ˆæœï¼š** å‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

---

## 2. å·¥ä½œåŸç† (How It Works)

### 2.1 ä»£ç å®ç°

åœ¨ NanoGPT çš„ `GPTLanguageModel` ç±»ä¸­ï¼Œæƒé‡å…±äº«çš„å®ç°éå¸¸ç®€æ´ï¼š

```python
class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        # è¾“å…¥å±‚ï¼šToken Embedding
        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBED)

        # è¾“å‡ºå±‚ï¼šLanguage Model Head
        self.lm_head = nn.Linear(N_EMBED, vocab_size, bias=False)

        # æƒé‡å…±äº«ï¼šEmbedding ä¸ LM Head å…±ç”¨åŒä¸€æƒé‡çŸ©é˜µ
        self.lm_head.weight = self.token_embedding_table.weight
```

### 2.2 å†…å­˜å±‚é¢çš„ç†è§£

åœ¨ PyTorch ä¸­ï¼Œ`nn.Parameter` æ˜¯å¯¹å¼ é‡çš„å°è£…ã€‚å½“æ‰§è¡Œ `self.lm_head.weight = self.token_embedding_table.weight` æ—¶ï¼š

- **ç‰©ç†å­˜å‚¨ï¼š** å†…å­˜ä¸­åªå­˜åœ¨ **ä¸€ä»½** å½¢çŠ¶ä¸º `(vocab_size, N_EMBED)` çš„å¼ é‡ã€‚
- **é€»è¾‘å±‚é¢ï¼š**
  - `token_embedding_table` å°†å…¶è§†ä¸º **æŸ¥æ‰¾è¡¨ï¼ˆLook-up Tableï¼‰**
  - `lm_head` å°†å…¶è§†ä¸º **çº¿æ€§å˜æ¢çŸ©é˜µï¼ˆLinear Projectionï¼‰**
- **æ¢¯åº¦æ›´æ–°ï¼š** åå‘ä¼ æ’­æ—¶ï¼ŒåŒæ—¶ç´¯ç§¯æ¥è‡ªè¾“å…¥ç«¯å’Œè¾“å‡ºç«¯çš„æ¢¯åº¦ï¼Œæ›´æ–°æ—¶åŒæ­¥æ›´æ–°ã€‚

```mermaid
graph TB
    subgraph Memory["ç‰©ç†å†…å­˜"]
        W["æƒé‡çŸ©é˜µ W<br/>(vocab_size Ã— N_EMBED)<br/>åªå­˜ä¸€ä»½"]
    end

    subgraph Input["è¾“å…¥ç«¯"]
        E["nn.Embedding<br/>token_embedding_table"]
    end

    subgraph Output["è¾“å‡ºç«¯"]
        L["nn.Linear<br/>lm_head"]
    end

    E -->|å¼•ç”¨| W
    L -->|å¼•ç”¨| W

    style W fill:#e6f3ff,stroke:#4d94ff,stroke-width:2px
```

### 2.3 ç»´åº¦åŒ¹é…

ä½ å¯èƒ½ä¼šç–‘æƒ‘ `nn.Embedding` å’Œ `nn.Linear` çš„å½¢çŠ¶æ˜¯å¦åŒ¹é…ï¼š

| ç»„ä»¶ | æƒé‡å½¢çŠ¶ | è¯´æ˜ |
|:-----|:--------|:-----|
| `nn.Embedding(V, D).weight` | `(V, D)` | è¯è¡¨å¤§å° Ã— åµŒå…¥ç»´åº¦ |
| `nn.Linear(D, V).weight` | `(V, D)` | PyTorch å­˜å‚¨ä¸º `(out_features, in_features)` |

å®ƒä»¬åœ¨å†…å­˜ä¸­çš„å½¢çŠ¶ **å®Œå…¨ä¸€è‡´**ï¼Œå¯ä»¥ç›´æ¥èµ‹å€¼ï¼Œæ— éœ€è½¬ç½®æ“ä½œã€‚

---

## 3. ä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Ÿ

### 3.1 è¯­ä¹‰ä¸€è‡´æ€§ (Semantic Consistency)

è¿™æ˜¯æƒé‡å…±äº«æœ€æ ¸å¿ƒçš„ç†è®ºä¾æ®ï¼š

- **è¾“å…¥ç«¯ï¼š** Embedding å±‚å°† Token IDï¼ˆå¦‚ "Apple"ï¼‰è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡ $v_{apple}$
- **è¾“å‡ºç«¯ï¼š** LM Head è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ $h$ ä¸è¯è¡¨ä¸­æ‰€æœ‰è¯çš„ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰

**ç›´è§‰ç†è§£ï¼š** å¦‚æœæ¨¡å‹åœ¨è¾“å…¥ç«¯è®¤ä¸ºå‘é‡ $v$ ä»£è¡¨ "Apple"ï¼Œé‚£ä¹ˆåœ¨è¾“å‡ºç«¯ï¼Œå½“æ¨¡å‹æƒ³è¦é¢„æµ‹ "Apple" æ—¶ï¼Œå®ƒç”Ÿæˆçš„ä¸Šä¸‹æ–‡å‘é‡åº”è¯¥ä¸ $v$ æœ€ç›¸ä¼¼ã€‚

```mermaid
graph LR
    subgraph Input["è¾“å…¥ç«¯"]
        A["Token: Apple<br/>ID: 502"] -->|Embedding| B["å‘é‡ v_apple<br/>[0.8, -0.1, 0.5, ...]"]
    end

    subgraph Output["è¾“å‡ºç«¯"]
        C["ä¸Šä¸‹æ–‡å‘é‡ h<br/>[0.79, -0.12, 0.48, ...]"] -->|ç‚¹ç§¯ç›¸ä¼¼åº¦| D["é¢„æµ‹: Apple<br/>score = h Â· v_apple"]
    end

    B -.->|å…±äº«åŒä¸€å‘é‡ç©ºé—´| D

    style B fill:#ffe6e6,stroke:#ff6666
    style D fill:#e6ffe6,stroke:#66ff66
```

### 3.2 å‡å°‘å‚æ•°é‡ (Parameter Efficiency)

è¿™æ˜¯æœ€ç›´æ¥çš„å·¥ç¨‹ä¼˜åŠ¿ã€‚è¯æ±‡è¡¨é€šå¸¸å¾ˆå¤§ï¼ŒEmbedding çŸ©é˜µå ç”¨å¤§é‡å‚æ•°ï¼š

**å‚æ•°é‡è®¡ç®—ç¤ºä¾‹ï¼š**

| æ¨¡å‹ | vocab_size | N_EMBED | å•ä¸ªçŸ©é˜µå‚æ•°é‡ | èŠ‚çœé‡ |
|:-----|:-----------|:--------|:--------------|:-------|
| NanoGPT | 50,257 | 768 | ~38.6M | ~38.6M |
| GPT-2 | 50,257 | 1,024 | ~51.5M | ~51.5M |
| LLaMA-7B | 32,000 | 4,096 | ~131M | ~131M (~500MB æ˜¾å­˜) |
| LLaMA-70B | 32,000 | 8,192 | ~262M | ~262M (~1GB æ˜¾å­˜) |

$$
\text{èŠ‚çœå‚æ•°é‡} = \text{vocab\_size} \times \text{N\_EMBED}
$$

### 3.3 æ­£åˆ™åŒ–æ•ˆæœ (Regularization)

å‚æ•°é‡å‡å°‘å¸¦æ¥çš„é¢å¤–å¥½å¤„ï¼š

- **å‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼š** æ›´å°‘çš„å‚æ•°æ„å‘³ç€æ›´å°‘çš„è¿‡æ‹Ÿåˆæœºä¼š
- **å¼ºåˆ¶ç‰¹å¾å…±äº«ï¼š** æ¨¡å‹è¢«è¿«å­¦ä¹ ä¸€ä¸ªæ—¢èƒ½è¡¨ç¤ºè¾“å…¥ç‰¹å¾ï¼Œåˆèƒ½ä½œä¸ºè¾“å‡ºåˆ†ç±»ä¾æ®çš„ **é€šç”¨ç‰¹å¾ç©ºé—´**
- **åŒå‘æ¢¯åº¦æ›´æ–°ï¼š** å…±äº«æƒé‡åŒæ—¶æ¥æ”¶æ¥è‡ªè¾“å…¥ç«¯å’Œè¾“å‡ºç«¯çš„æ¢¯åº¦ï¼Œå¯¹ä½é¢‘è¯å°¤ä¸ºæœ‰ç›Š

---

## 4. å¯¹æ¨¡å‹æ•ˆæœçš„å½±å“

### 4.1 å­¦æœ¯ç»“è®º

æ ¹æ® Press & Wolf (2016) åŠåç»­ç ”ç©¶ï¼š

> åœ¨å¤§å¤šæ•°è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ï¼Œæƒé‡å…±äº« **æ˜¾è‘—é™ä½äº†å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰**ï¼Œå³æå‡äº†é¢„æµ‹å‡†ç¡®ç‡ã€‚

### 4.2 æ•ˆæœå¯¹æ¯”

| é…ç½® | å‚æ•°é‡ | å›°æƒ‘åº¦ (PPL) | è®­ç»ƒç¨³å®šæ€§ |
|:-----|:------|:------------|:----------|
| ä¸å…±äº«æƒé‡ | åŸºå‡† | åŸºå‡† | ä¸€èˆ¬ |
| å…±äº«æƒé‡ | â†“ æ˜¾è‘—å‡å°‘ | â†“ é€šå¸¸æ›´ä½ | â†‘ æ›´ç¨³å®š |

<img src={weightShare} alt="æƒé‡å…±äº«è®­ç»ƒç¨³å®šæ€§å¯¹æ¯”" />

### 4.3 ä¸ºä»€ä¹ˆé€šå¸¸æå‡æ•ˆæœï¼Ÿ

**A. å¼ºåˆ¶è¯­ä¹‰å¯¹é½ï¼ˆå¼ºæ­£åˆ™åŒ–ï¼‰**

- **ä¸å…±äº«æ—¶ï¼š** è¾“å…¥ Embedding åªéœ€å­¦ä¹ "å¦‚ä½•è¢«åç»­å±‚è¯†åˆ«"ï¼Œè¾“å‡º Linear åªéœ€å­¦ä¹ "å¦‚ä½•åˆ†ç±»"ï¼Œå¯èƒ½å­¦å‡ºä¸¤å¥—å®Œå…¨ä¸åŒçš„å‘é‡åˆ†å¸ƒ
- **å…±äº«æ—¶ï¼š** å¼ºåˆ¶è¦æ±‚è¾“å…¥è¡¨ç¤ºå’Œè¾“å‡ºé¢„æµ‹ç›®æ ‡ä½äºåŒä¸€å‡ ä½•ç©ºé—´

**B. è®­ç»ƒæ•ˆç‡æ›´é«˜**

- å…±äº«æƒé‡åŒæ—¶æ¥æ”¶æ¥è‡ªè¾“å…¥ç«¯å’Œè¾“å‡ºç«¯çš„æ¢¯åº¦ä¿¡å·
- å¯¹ **ä½é¢‘è¯ï¼ˆRare Wordsï¼‰** å°¤ä¸ºé‡è¦â€”â€”åŒå€çš„æ¢¯åº¦è®©å®ƒä»¬è¢«å­¦ä¹ å¾—æ›´å¥½

---

## 5. å®ç°ç»†èŠ‚ä¸æ³¨æ„äº‹é¡¹

### 5.1 å¿…é¡»è®¾ç½® `bias=False`

```python
self.lm_head = nn.Linear(N_EMBED, vocab_size, bias=False)  # âœ… æ­£ç¡®
self.lm_head = nn.Linear(N_EMBED, vocab_size, bias=True)   # âŒ ä¸æ¨è
```

**åŸå› ï¼š** `nn.Embedding` æ²¡æœ‰åç½®é¡¹ï¼ˆBiasï¼‰ï¼Œå¦‚æœ `lm_head` å¸¦æœ‰ Biasï¼Œè™½ç„¶æƒé‡çŸ©é˜µå¯ä»¥å…±äº«ï¼Œä½† Bias æ— æ³•å…±äº«ï¼Œå¯¼è‡´é€»è¾‘ä¸å¯¹ç§°ã€‚

### 5.2 ç¼©æ”¾æŠ€å·§ (Scaling)

åœ¨åŸå§‹ Transformer è®ºæ–‡ *"Attention Is All You Need"* ä¸­ï¼ŒEmbedding å±‚è¾“å‡ºåä¼šä¹˜ä»¥ $\sqrt{d_{model}}$ï¼š

```python
# åŸä»£ç 
tok_emb = self.token_embedding_table(idx)

# æ”¹è¿›ï¼šä¹˜ä»¥ sqrt(d_model)
tok_emb = self.token_embedding_table(idx) * math.sqrt(N_EMBED)
```

**ä¸ºä»€ä¹ˆï¼Ÿ**

- Embedding æƒé‡åˆå§‹åŒ–é€šå¸¸è¾ƒå°ï¼ˆæ–¹å·®çº¦ $1/d$ï¼‰
- ç»è¿‡å¤šå±‚ LayerNorm å’Œæ®‹å·®è¿æ¥åï¼Œæ•°å€¼åˆ†å¸ƒä¼šå˜åŒ–
- ç¼©æ”¾å¯ä»¥ä½¿ Embedding æ•°å€¼æ›´é€‚åˆè¿›å…¥åç»­ Attention å±‚ï¼Œä¿æŒæ•°å€¼ç¨³å®šæ€§

### 5.3 åˆå§‹åŒ–ç­–ç•¥

ç”±äºæƒé‡è¢«å…±äº«ï¼Œåˆå§‹åŒ–éœ€è¦åŒæ—¶è€ƒè™‘ä¸¤ä¸ªç”¨é€”ï¼š

```python
# å¸¸è§åˆå§‹åŒ–æ–¹å¼
nn.init.normal_(self.token_embedding_table.weight, mean=0.0, std=0.02)
# lm_head.weight è‡ªåŠ¨å…±äº«ï¼Œæ— éœ€å•ç‹¬åˆå§‹åŒ–
```

---

## 6. å®Œæ•´ä»£ç ç¤ºä¾‹

### 6.1 NanoGPT é£æ ¼å®ç°

```python
import torch
import torch.nn as nn
import math

class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size, n_embed, n_layer, n_head, block_size, dropout=0.1):
        super().__init__()

        # Token Embedding
        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)
        # Position Embedding
        self.position_embedding_table = nn.Embedding(block_size, n_embed)

        # Transformer Blocks
        self.blocks = nn.Sequential(*[
            TransformerBlock(n_embed, n_head, block_size, dropout)
            for _ in range(n_layer)
        ])

        # Final LayerNorm
        self.ln_f = nn.LayerNorm(n_embed)

        # Language Model Head (è¾“å‡ºå±‚)
        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)

        # ğŸ”‘ æƒé‡å…±äº«ï¼šæ ¸å¿ƒä»£ç 
        self.lm_head.weight = self.token_embedding_table.weight

        # åˆå§‹åŒ–
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        B, T = idx.shape

        # Token + Position Embedding
        tok_emb = self.token_embedding_table(idx)  # (B, T, C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, C)
        x = tok_emb + pos_emb  # (B, T, C)

        # Transformer Blocks
        x = self.blocks(x)
        x = self.ln_f(x)

        # è¾“å‡º Logits
        logits = self.lm_head(x)  # (B, T, vocab_size)

        # è®¡ç®— Loss
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss
```

### 6.2 éªŒè¯æƒé‡ç¡®å®å…±äº«

```python
model = GPTLanguageModel(vocab_size=50257, n_embed=768, ...)

# éªŒè¯ä¸¤è€…æŒ‡å‘åŒä¸€å†…å­˜åœ°å€
print(model.token_embedding_table.weight.data_ptr())
print(model.lm_head.weight.data_ptr())
# è¾“å‡ºç›¸åŒçš„å†…å­˜åœ°å€

# éªŒè¯ä¸¤è€…æ˜¯åŒä¸€å¯¹è±¡
print(model.token_embedding_table.weight is model.lm_head.weight)
# è¾“å‡º: True
```

---

## 7. ä»€ä¹ˆæ—¶å€™ä¸å…±äº«å¯èƒ½æ›´å¥½ï¼Ÿ

åœ¨æå°‘æ•°æƒ…å†µä¸‹ï¼Œç ”ç©¶äººå‘˜ä¼šé€‰æ‹©è§£ç»‘ï¼ˆDecoupleï¼‰æƒé‡ï¼š

| åœºæ™¯ | åŸå›  |
|:-----|:-----|
| **è¶…å¤§è§„æ¨¡æ¨¡å‹** | æ•°æ®å’Œç®—åŠ›æ¥è¿‘æ— é™æ—¶ï¼Œè§£ç»‘å¯èƒ½è®©æ¨¡å‹æœ‰æ›´å¤§è‡ªç”±åº¦ |
| **å¤šè¯­è¨€/ç‰¹æ®Šä»»åŠ¡** | è¾“å…¥å’Œè¾“å‡ºåˆ†å¸ƒå·®å¼‚æå¤§æ—¶ |
| **ç¼–ç å™¨-è§£ç å™¨æ¶æ„** | æŸäº› Seq2Seq æ¨¡å‹å¯èƒ½ä¸é€‚åˆå…±äº« |

ä½†å¯¹äºç»å¤§å¤šæ•°åœºæ™¯ï¼ˆç‰¹åˆ«æ˜¯ Decoder-only çš„ GPT æ¶æ„ï¼‰ï¼Œ**æƒé‡å…±äº«æ˜¯æ ‡å‡†åšæ³•**ã€‚

---

## 8. ä¸»æµæ¨¡å‹é‡‡ç”¨æƒ…å†µ

| æ¨¡å‹ | æ˜¯å¦ä½¿ç”¨æƒé‡å…±äº« | å¤‡æ³¨ |
|:-----|:---------------|:-----|
| GPT-2 | âœ… æ˜¯ | å¼€åˆ›æ€§é‡‡ç”¨ |
| GPT-3 | âœ… æ˜¯ | æ²¿ç”¨ GPT-2 è®¾è®¡ |
| LLaMA | âœ… æ˜¯ | æ˜ç¡®åœ¨è®ºæ–‡ä¸­è¯´æ˜ |
| LLaMA 2/3 | âœ… æ˜¯ | ç»§æ‰¿ LLaMA è®¾è®¡ |
| Mistral | âœ… æ˜¯ | ä¸šç•Œæ ‡å‡†åšæ³• |
| BERT | âœ… æ˜¯ | åœ¨ MLM ä»»åŠ¡ä¸­ä½¿ç”¨ |

---

## 9. å¸¸è§é—®é¢˜ (FAQ)

**Q: æƒé‡å…±äº«ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå—ï¼Ÿ**

A: é€šå¸¸ä¸ä¼šã€‚ç›¸åï¼Œç”±äºæ­£åˆ™åŒ–æ•ˆæœï¼Œè®­ç»ƒå¾€å¾€æ›´ç¨³å®šã€‚å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥å°è¯•åŠ å…¥ $\sqrt{d_{model}}$ ç¼©æ”¾ã€‚

**Q: æˆ‘å¯ä»¥åªåœ¨æ¨ç†æ—¶è§£ç»‘æƒé‡å—ï¼Ÿ**

A: æŠ€æœ¯ä¸Šå¯ä»¥ï¼ˆé€šè¿‡å¤åˆ¶æƒé‡ï¼‰ï¼Œä½†æ²¡æœ‰ä»»ä½•å¥½å¤„ï¼Œåè€Œä¼šå¢åŠ æ˜¾å­˜å ç”¨ã€‚

**Q: å…±äº«æƒé‡å¯¹æ¢¯åº¦è®¡ç®—æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ**

A: æ¢¯åº¦ä¼šè‡ªåŠ¨ç´¯ç§¯ã€‚PyTorch çš„è‡ªåŠ¨å¾®åˆ†æœºåˆ¶ä¼šæ­£ç¡®å¤„ç†å…±äº«å‚æ•°çš„æ¢¯åº¦ï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„ã€‚

---

## 10. æ€»ç»“

| æ–¹é¢ | ç»“è®º |
|:-----|:-----|
| **æ˜¯å¦æ¨è** | âœ… å¼ºçƒˆæ¨èï¼ˆç°ä»£ LLM æ ‡å‡†åšæ³•ï¼‰ |
| **å‚æ•°èŠ‚çœ** | vocab_size Ã— N_EMBEDï¼ˆé€šå¸¸æ•°åƒä¸‡å‚æ•°ï¼‰ |
| **æ•ˆæœå½±å“** | é€šå¸¸ â†‘ æå‡ï¼ˆé™ä½ PPLï¼‰ |
| **å®ç°å¤æ‚åº¦** | æä½ï¼ˆä¸€è¡Œä»£ç ï¼‰ |

**æ ¸å¿ƒä»£ç ï¼š**
```python
self.lm_head.weight = self.token_embedding_table.weight
```

---

## 11. å‚è€ƒèµ„æ–™

- [Using the Output Embedding to Improve Language Models](https://arxiv.org/abs/1608.05859) - Press & Wolf, 2016ï¼ˆæƒé‡å…±äº«åŸå§‹è®ºæ–‡ï¼‰
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al., 2017
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 è®ºæ–‡
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) - Meta AI, 2023
