---
title: 大模型 Scaling Law 与后 Scaling Law 时代的应对策略
sidebar_position: 21
tags: [Scaling Law, 大模型, MoE, RLHF, 深度学习]
---

# 大模型 Scaling Law 与后 Scaling Law 时代的应对策略

## 1. 什么是 Scaling Law

Scaling Law（扩展规律）描述了 **模型性能随规模上升的幂律增长关系**：

* 模型参数 (N) 增大 → 性能提升
* 训练数据 (D) 增多 → 性能提升
* 训练算力 (C) 增加 → 性能提升

但这些提升都遵循 **极其缓慢的幂律（Power Law）**，且存在：

* **收益递减**（越大越不划算）
* **理论损失下界 (L_\infty)**（再大也突破不了）

这意味着：

> **靠堆 dense 参数继续提升模型能力，会变得越来越贵、越来越慢。**

这就是 "Scaling Law 是悲观的" 的根源。

### 1.1 为什么说 Scaling Law 是悲观的？

"不划算"的本质在于：**投入和产出不对等**——投入呈指数级增长，而产出只呈线性增长。我们可以从**数学原理、经济成本、以及"智能体验"**三个维度来理解这个"不划算"。

#### 📐 数学原理：残酷的"幂律" (The Power Law)

Scaling Law 的核心公式通常表现为一种幂律关系。如果我们用 $L$ 代表模型预测下一个词的误差（Loss，越低越好），用 $C$ 代表算力投入（Compute），它们的关系大致如下：

$$
L(C) \propto C^{-\alpha}
$$

这里的指数 $\alpha$ 是一个很小的数（通常在 0.05 到 0.1 之间）。这句话翻译成人话就是：

> **要想让性能提升一点点（线性），你需要投入的资源必须翻好多倍（指数级）。边际效应极速递减。**

举个具体的例子（为了方便理解，数值为模拟）：

- **阶段 1（起步）**：你投入 **1张 显卡**，模型智商从 0 提升到了 60 分（提升巨大）。
- **阶段 2（进阶）**：你想把智商从 60 提升到 80 分。根据 Scaling Law，你可能需要 **100张 显卡**。
- **阶段 3（瓶颈）**：你想把智商从 80 提升到 90 分。你不再只需要加几张卡，你可能需要 **10,000张 显卡**。
- **阶段 4（悲观）**：你想从 90 分提升到 92 分。你可能需要 **整个国家一年的发电量**。

**结论**：在曲线的前端，稍微加点参数，效果立竿见影；在曲线的尾部（我们现在的阶段），即使砸进去几亿美元，Loss 可能只下降了 0.001。

#### 💰 经济成本：烧钱换来的"微小进步"

在 Scaling Law 的后期，成本不再是线性增加，而是爆炸式增长。

| 模型代际 | 估算训练成本 (美元) | 性能提升幅度 | 划算程度 |
|---------|-------------------|------------|---------|
| GPT-2 | 几万美元 | 巨大（从胡言乱语到通顺） | ⭐⭐⭐⭐⭐ (极高) |
| GPT-3 | 几百万美元 | 很大（涌现出少样本学习能力） | ⭐⭐⭐⭐ (高) |
| GPT-4 | 约 1 亿美元 | 显著（逻辑推理、代码能力质变） | ⭐⭐⭐ (中等) |
| 下一代 | 预计 10亿 - 100亿美元 | 可能只有 10%-20% 的优化 | ⭐ (极低) |

这就是"不划算"的现实写照：

> **企业为了获得最后那 1% 的准确率提升，需要建设价值千亿的数据中心。**
> **对于商业公司来说，ROI（投资回报率）会变得极低，甚至无法回本。**

#### 🎭 体验陷阱：Loss 降低 $\neq$ 变聪明

Scaling Law 还有一个隐性的"不划算"，在于**指标与体验的脱节**。

- **Scaling Law 降低的是** Next Token Prediction 的 Loss（预测下一个词更准了）。
- **人类需要的是** Reasoning & Creativity（逻辑更严密、更有创意）。

在前期，Loss 的降低能直接带来逻辑的涌现。但在后期，模型可能只是背住了更多生僻的百科知识（导致 Loss 降低），但在解决复杂的数学题或逻辑陷阱时，并没有本质的变强。

**比喻**：

就像备考。从 30 分考到 80 分，只需要把课本看一遍（Scaling 很容易）。但想从 98 分考到 100 分，你可能需要背下整本字典。

> **背字典确实让你"掌握了更多词"（Loss 降低了），但并没有让你变得"更聪明"（逻辑没变）。**

---

## 2. Post-Scaling Law（后 Scaling Law）时代的挑战

随着 GPT-4、Claude 3.5、Gemini 3 Pro、Llama3.1 级别的模型陆续逼近 compute-optimal 区域，
企业和研究团队面临三大问题：

1. **进一步 scale dense 模型的成本指数级上升**
2. **性能提升变得极慢（幂律尾部）**
3. **无法仅通过"变大"获得质变能力**

因此行业开始寻找 **绕开 scaling law 成本、提高能力密度的技术路径**。

---

## 3. 后 Scaling Law 时代的主要策略（行业共识方向）

下面列出实际有效、业界主流的规避 Scaling Law 成本的方向。

---

## **3.1 稀疏化模型结构：MoE（Mixture of Experts）**

**核心作用**：

* 提升总模型容量（Total Params）
* 但每个 token 实际计算的 expert 数极少（Active Params 低）

**对 scaling 的意义**：

> 在几乎不增加 FLOPs 的前提下扩展 N，降低 scaling law 的成本。

MoE 已成为高端模型的主要技术路线之一。

---

## **3.2 强化学习 / 偏好学习：RLHF、DPO、RLAIF**

**核心作用**：

* 显著提高"推理质量、行为一致性、任务执行力"
* 不需要继续无脑 scale 参数

**对 scaling 的意义**：

> 通过"训练方式"增强能力，而不是"参数规模"增强能力。

RLHF/DPO 已成为最强模型能力提升的关键来源之一。

---

## **3.3 数据质量优化与混合策略（Data Optimization）**

包括：

* 高质量数据抽取
* Data filtering / dedup / rerank
* 合成偏好数据（AIF），自蒸馏（self-bootstrapping）
* curriculum learning

**意义**：

> 用更少的数据获得更强的模型效果，提高"token 价值密度"。

高质量数据的提升往往比硬扩模型更划算。

---

## **3.4 长上下文 + 记忆机制（Memory / Retrieval / RAG）**

方向包括：

* RAG（检索增强生成）
* Document memory
* 长上下文（100K–1M tokens）
* External tool memory（长程知识库）

**意义**：

> 不靠参数存所有知识，将知识外置，提高模型能力密度。

也被视为突破 scaling law 的重要路线。

---

## **3.5 Tool Use / Program Synthesis / Code Interpreter**

让模型学会：

* 调用 API
* 搜索
* 运行代码
* 操纵外部工具

**意义**：

> 通过"扩展工具能力"获得非参数性的智能提升。

这类提升不依赖 dense 参数，性价比极高。

---

## **3.6 结构创新（Architecture Innovation）**

包括：

* MoA（Mixture of Attention）
* Mamba 2 / RWKV 6
* Linear attention
* Dual-layer transformer
* Speculative decoding / multi-step reasoning
* 神经符号混合模型

**意义**：

> 靠结构设计逃避 dense transformer 的 scaling 限制。

---

# 4. 总结

> **后 Scaling Law 时代不再依赖单纯扩大 dense 模型规模，而是通过 MoE、强化学习、数据质量、工具使用、检索记忆和结构创新来提升模型能力，从而在有限算力下获得更高的性价比与更强的智能表现。**
