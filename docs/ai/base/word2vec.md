---
title: Word2Vec 词嵌入详解
sidebar_label: Word2Vec
date: 2025-11-30
tags: [nlp, word2vec, word-embeddings, skip-gram, cbow, negative-sampling]
---

# Word2Vec 词嵌入详解

**Word2Vec** 是自然语言处理（NLP）领域中一个里程碑式的技术。简单来说，它是一种让计算机理解词语含义，并把词语转换成**数字向量**（Word Embeddings）的工具。

你可以把它想象成**将字典里的词语映射到一个巨大的多维空间地图上**。在这个地图上，意思相近的词（比如"猫"和"狗"）距离会很近，而意思无关的词（比如"猫"和"香蕉"）距离会很远。

---

## 1. 核心理念：词语的含义由上下文决定

Word2Vec 的核心思想基于语言学中的一个著名假设：**"通过一个词周围的词，你可以了解这个词的含义。"**

- 如果你看到句子："小明每天早上都要喝一杯**牛奶**。"
- 如果你看到句子："小明每天早上都要喝一杯**咖啡**。"

因为"牛奶"和"咖啡"经常出现在极其相似的上下文（Context）中，Word2Vec 就会认为这两个词在语义上是相似的，并把它们在数学空间中放得很近。

---

## 2. 两种训练架构

Word2Vec 其实是一个简单的浅层神经网络，它有两种训练模式，就像是两种填空游戏：

### 2.1 CBOW (Continuous Bag of Words) —— 完形填空

**逻辑**：根据**上下文**来预测**中间的词**。

- **例子**：输入是 `["今天", "天气", "___", "好"]`。
- **任务**：模型需要算出中间这个词是 `"真"` 或 `"非常"` 的概率。
- **适用场景**：适合处理小型数据集，训练速度快。

### 2.2 Skip-gram —— 举一反三

**逻辑**：根据**中间的词**来预测**上下文**。

- **例子**：输入是 `"苹果"`。
- **任务**：模型需要预测它周围可能出现 `"手机"`（科技语境）或者 `"好吃"`（水果语境）。
- **适用场景**：在大型数据集上表现更好，对生僻字的处理更佳。

---

## 3. 网络结构：三层神经网络

Word2Vec 是一个只有三层的神经网络（Input → Projection → Output）。

假设我们词汇表里有 $V$ 个单词（例如 10,000 个），我们要将单词映射成 $N$ 维的向量（例如 300 维）。

### 3.1 输入层 (Input Layer)

- 输入是一个单词的 **One-Hot 向量**。
- 维度是 $1 \times V$。只有对应的单词位置是 1，其余是 0。

### 3.2 隐藏层 / 投影层 (Projection Layer)

- 这是最关键的一层。这里有一个权重矩阵 $W$，维度是 $V \times N$。
- **重点**：这一层**没有**激活函数（如 Sigmoid 或 ReLU）。它只是简单的线性映射。
- 当 One-Hot 向量乘以这个矩阵 $W$ 时，其实就是在"查表"，直接取出了矩阵中对应的那一行。**这一行就是该单词目前的词向量**。

### 3.3 输出层 (Output Layer)

- 这里有另一个权重矩阵 $W'$，维度是 $N \times V$。
- 输出经过 **Softmax** 激活函数，变成一个概率分布。
- 维度是 $1 \times V$。每一维代表预测是词汇表中某个单词的概率。

---

## 4. 两种架构的数学原理

### 4.1 CBOW 的计算流程

**任务**：已知上下文词 $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$，预测中心词 $w_t$。

1. **输入**：将上下文 $C$ 个单词的 One-Hot 向量分别乘以矩阵 $W$，得到 $C$ 个 $N$ 维向量。
2. **求和/平均**：将这 $C$ 个向量相加（或取平均），得到一个 $N$ 维的隐藏层向量 $h$。
   - 这就是 "Bag-of-Words" 的由来，因为求和后词序信息丢失了，只保留了语义总和。
3. **预测**：$h$ 乘以输出矩阵 $W'$，得到 $1 \times V$ 的分数向量。
4. **Softmax**：将分数转化为概率，即使得 $P(w_t | \text{context})$ 最大化。

### 4.2 Skip-gram 的计算流程

**任务**：已知中心词 $w_t$，预测上下文词 $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$。

1. **输入**：中心词 $w_t$ 的 One-Hot 向量乘以 $W$，得到 $N$ 维向量 $v_{w_t}$。
2. **预测**：直接用 $v_{w_t}$ 乘以输出矩阵 $W'$，计算出概率分布。
3. **目标**：实际上是进行多次预测。我们希望对于每一个上下文单词 $w_{c}$，模型预测出的概率 $P(w_{c} | w_t)$ 都要尽可能大。
   - Skip-gram 相当于把一个样本 `(Context, Target)` 拆成了 $C$ 个独立的样本 `(Target, Context_1)`, `(Target, Context_2)`... 因此它的训练数据量比 CBOW 大得多。

---

## 5. 计算瓶颈与优化技术

### 5.1 问题：Softmax 计算量太大

如果直接按上面的逻辑训练，计算量会大到无法接受。问题出在 Output 层的 Softmax 上：

$$
P(w_o|w_i) = \frac{\exp({v'_{w_o}}^T v_{w_i})}{\sum_{j=1}^{V} \exp({v'_{w_j}}^T v_{w_i})}
$$

看分母部分：为了计算一个单词的概率，你需要遍历**整个词汇表 $V$**（比如 100万 个词），算出所有词的得分并求和。每一轮训练、每一个样本都要做这 100万 次计算，这是不可能完成的。

### 5.2 优化一：层次 Softmax (Hierarchical Softmax)

**原理**：将扁平的 $V$ 分类问题，转化成**二叉树**（Huffman Tree）路径寻找问题。

1. **结构**：把所有单词作为叶子节点，构建一颗哈夫曼树（高频词靠近根节点，路径短；低频词远离根节点，路径长）。
2. **预测过程**：不再是一次性预测 1/V，而是从根节点开始走。每遇到一个分支节点，就做一个**二分类**（向左走还是向右走？）。
   - 比如要预测"足球"，可能路径是：左→右→左。
3. **计算量**：从 $O(V)$ 降低到了 $O(\log_2 V)$。从 100万次计算降到了约 20 次。

### 5.3 优化二：负采样 (Negative Sampling)

**原理**：如果你只是想让"猫"和"鱼"的匹配度变高，你不需要同时压低"猫"和字典里其它 999,999 个词的匹配度。你只需要随机抽取几个"**负样本**"压低即可。

1. **正样本**：`("猫", "鱼")` —— 目标是让 sigmoid 输出接近 1。
2. **负样本**：随机抽取 $K$ 个（例如 5-20 个）无关词，如 `("猫", "桌子")`, `("猫", "飞机")` —— 目标是让 sigmoid 输出接近 0。
3. **效果**：我们只更新正样本和这 $K$ 个负样本对应的权重，其他几十万个词的权重保持不变。这极大地提升了训练速度。

---

## 6. 高频词二次采样 (Subsampling of Frequent Words)

除了训练阶段的优化，Word2Vec 还有一个重要的数据预处理技巧：**高频词的二次采样**。

### 6.1 为什么要过滤高频词？

在海量的文本数据中，单词出现的频率分布通常符合 **Zipf's Law（齐夫定律）**：极少数的词（如 "the"）出现了无数次，而绝大多数有实际含义的词（如 "algorithm"）出现频率较低。

如果不处理高频词，会带来两个问题：

1. **信息量极低**：在句子 "Paris is the capital of France" 中，"Paris" 和 "France" 的共现关系非常有价值。但 "France" 和 "the" 的共现关系几乎没有价值。
2. **向量早已饱和**：像 "the" 这种词，可能经过几千次训练它的向量就已经很准确了。再训练几百万次会挤占稀有词的更新机会。

### 6.2 概率性丢弃

Word2Vec 采用**概率性丢弃**策略。对于训练集中的每一个词 $w_i$，它都有一定的概率被剔除：

$$
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}
$$

其中：
- $f(w_i)$ 是该单词在整个语料库中出现的**频率**。
- $t$ 是一个设定的**阈值**（threshold），通常设为 $10^{-5}$ 左右。

**公式含义**：
- 如果 $f(w_i)$ 很大（"the" 频率很高），$\frac{t}{f(w_i)}$ 就很小，丢弃概率接近 1 —— **大概率被扔掉**。
- 如果 $f(w_i)$ 很小（"giraffe" 频率很低），丢弃概率可能为 0 —— **一定被保留**。

### 6.3 隐形好处：扩大有效上下文窗口

假设窗口大小是 2，句子是：`The quick brown fox jumps...`

- **不采样**：对于 "quick"，上下文是 `["The", "brown"]`。
- **采样后**（假设 "The" 被踢出）：上下文变成 `["brown", "fox"]`。

通过踢掉无意义的高频词 "The"，"quick" 直接和更远的实义词 "fox" 建立了联系，让模型能够学习到更长距离的语义依赖。

---

## 7. Word2Vec 的"魔法"：语义运算

Word2Vec 最让人惊叹的地方在于，转换后的向量具有**数学上的逻辑关系**。你可以对词语进行加减法运算！

最经典的例子：

$$
\text{King} - \text{Man} + \text{Woman} \approx \text{Queen}
$$

这意味着，如果你把"国王"的向量减去"男人"的特征，再加上"女人"的特征，结果在空间中离"女王"这个词最近。这证明了模型不仅仅是死记硬背，而是真正捕捉到了词语之间的**语义关系**（如性别、时态、国家与首都等）。

---

## 8. 对比 One-Hot 编码

在 Word2Vec 出现之前，计算机通常使用 **One-Hot 编码** 来表示词语。

| 特性 | One-Hot | Word2Vec |
|------|---------|----------|
| **维度** | $V$（词汇表大小，如 10 万） | $N$（通常 100-300） |
| **稀疏性** | 极度稀疏（只有一位是 1） | 稠密向量 |
| **语义信息** | 无（"猫"和"狗"没有数学相似性） | 有（相似词距离近） |
| **存储效率** | 低 | 高 |

---

## 9. 局限性：静态词向量

虽然 Word2Vec 非常强大，但它有一个明显的缺点：**静态性（Static）**。

**问题**：同一个词在不同句子里意思不同（一词多义），但 Word2Vec 给它的向量是固定的。

比如"**苹果**"这个词：
1. "我爱吃**苹果**。"（水果）
2. "**苹果**发布了新手机。"（公司）

在 Word2Vec 中，"苹果"只有一个固定的向量，它不得不混合这两种含义。这导致了后来 **ELMo**、**BERT** 和 **GPT** 等技术的出现，它们可以根据上下文动态地生成词向量（Contextualized Word Embeddings）。

---

## 10. 训练产物

很多初学者容易误解，以为我们想要的是那个预测准确的模型。其实不是。

- 训练完成后，用于预测上下文的 Output 矩阵 $W'$ 通常被丢弃。
- 我们真正要的是 **Input 层的权重矩阵 $W$**（维度 $V \times N$）。
- 矩阵的每一行，就是对应单词的 **Word Embedding（词向量）**。

---

## 11. 总结：Word2Vec 高效的三驾马车

Word2Vec 的高效是由三个关键技术共同支撑的：

| 层面 | 技术 | 解决的问题 |
|------|------|-----------|
| **模型架构** | CBOW / Skip-gram | 奠定基础，定义训练任务 |
| **训练优化** | Negative Sampling 或 Hierarchical Softmax | 解决 Softmax 分母计算太慢的问题 |
| **数据优化** | Subsampling | 解决高频词信息量低且拖慢速度的问题 |

Word2Vec 是连接人类语言与机器计算的桥梁。它抛弃了死板的编号，通过**上下文**挖掘出了词语的**语义**，为后来的人工智能大模型（如 ChatGPT）打下了坚实的基础。

---

## 参考资料

- [Mikolov et al., 2013 - Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- [Mikolov et al., 2013 - Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
