---
title: 卷积基础原理
sidebar_label: 卷积基础原理
---

# 卷积基础原理

## 为什么需要卷积？

全连接层处理图像存在严重问题：
- **参数爆炸**：一张 1 百万像素的图像使用全连接层会产生数十亿参数
- **训练困难**：需要海量数据和计算资源
- **忽视结构**：图像具有空间结构，全连接层无法有效利用

## 卷积的两大核心原则

### 1. 平移不变性（Translation Invariance）

检测对象不应依赖于它在图像中的位置。就像"沃尔多在哪里"游戏中，无论沃尔多出现在图片的哪个位置，我们都应该能够用相同的方法找到他。

**数学表示**：应用平移不变性后，权重从 $\mathsf{W}_{i,j,k,l}$ 简化为 $\mathbf{V}_{a,b}$，不再依赖绝对位置 $(i,j)$，只依赖相对位置 $(a,b)$。

### 2. 局部性（Locality）

神经网络的早期层只需关注局部区域，不必考虑相隔很远的像素关系。例如，检测边缘只需要相邻像素的信息。

**数学表示**：限制范围 $|a| \leq \Delta, |b| \leq \Delta$，只在局部窗口内计算。

## 卷积运算详解

### 互相关 vs 卷积

实际上，深度学习中的"卷积"严格来说是**互相关运算**（cross-correlation）：

$$[\mathbf{H}]_{i,j} = \sum_{a}\sum_{b} [\mathbf{V}]_{a,b} \cdot [\mathbf{X}]_{i+a,j+b}$$

真正的卷积需要翻转卷积核，但在学习过程中这个差别不影响结果，因为卷积核本身是学习得到的。

### 基本参数计算

输入大小：$n_h \times n_w$
卷积核大小：$k_h \times k_w$
输出大小：$(n_h - k_h + 1) \times (n_w - k_w + 1)$

### 实际应用示例：边缘检测

使用卷积核 `[1, -1]` 可以检测垂直边缘：
- 输出为正值：白到黑的边缘
- 输出为负值：黑到白的边缘
- 输出接近 0：颜色均匀区域

```python
# 示例：边缘检测卷积核
kernel = torch.tensor([[1.0, -1.0]])
# 对图像进行卷积运算
output = conv2d(input_image, kernel)
```

## 填充（Padding）

### 为什么需要填充？

- **信息丢失**：多层卷积会逐渐缩小特征图（如 240×240 经过 10 层 5×5 卷积变成 200×200）
- **边缘忽视**：边缘像素参与计算的次数远少于中心像素

### 填充后的输出大小

$$\text{output size} = (n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)$$

其中 $p_h$、$p_w$ 是垂直和水平填充的行/列数。

### 常见做法

设置 $p_h = k_h - 1$，$p_w = k_w - 1$ 可使输入输出大小相同。

**为什么常用奇数卷积核（1, 3, 5, 7）？**
- 便于在四周均匀填充
- 具有明确的中心像素

## 步幅（Stride）

### 作用

- **减小输出尺寸**：每次滑动多个元素而非一个
- **降低计算量**：减少卷积操作次数
- **增大感受野**：以更粗粒度采样特征

### 步幅下的输出大小

$$\text{output size} = \left\lfloor\frac{n_h - k_h + p_h + s_h}{s_h}\right\rfloor \times \left\lfloor\frac{n_w - k_w + p_w + s_w}{s_w}\right\rfloor$$

其中 $s_h$、$s_w$ 是垂直和水平步幅。

### 示例

- 步幅为 2：输出尺寸减半（8×8 → 4×4）
- 步幅为 3：输出尺寸约为原来的 1/3

## 参数优势

**全连接层**：处理 1M 像素图像可能需要数十亿参数
**卷积层**：通过平移不变性和局部性，参数减少到几百个

**代价**：引入归纳偏置（inductive bias）—— 假设图像具有局部性和平移不变性

## 总结

卷积神经网络通过三个关键机制高效处理图像：
1. **平移不变性**：共享权重，同一特征检测器可用于图像任意位置
2. **局部性**：只关注局部区域，减少参数
3. **灵活控制**：通过填充和步幅调整特征图尺寸

这些原理是理解 LeNet、AlexNet、VGG、ResNet 等经典 CNN 架构的基础。
