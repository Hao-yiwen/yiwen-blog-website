# 残差网络

## 函数

H(x) = F(x) + x

## 由来

在传统的全连接神经网络中，输入数据需要逐层传递到下一层。然而，当网络的层数增加到超过 20 层的深度时，输入对深层的影响会显著减弱，甚至可能出现梯度消失或梯度爆炸的问题，导致训练无法有效进行。为了解决这一问题，提升输入在深层网络中的权重和影响变得至关重要，于是残差网络应运而生。

2015 年，华人科学家何恺明提出了残差网络（ResNet），并在当年的图像识别大赛 ImageNet 中取得了惊人的成绩，帮助深度学习的网络层数首次突破 100 层。残差网络的提出成为深度学习领域的重要里程碑，其创新设计大大提升了深层网络的可训练性和性能。

## 原理

残差网络的核心思想是通过引入 跳跃连接（Skip Connection），将输入 ￼ 直接传递到深层网络。这种连接方式使得网络学习的是一个残差函数 ￼，而非直接学习目标映射 ￼。通过这种设计，即使网络加深，输入信息也能够快速传递到深层，从而有效避免梯度消失问题，提升网络的优化效率和训练稳定性。