---
title: Transformer 绝对位置编码
sidebar_label: 绝对位置编码
date: 2025-11-28
last_update:
  date: 2025-11-28
tags: [transformer, positional-encoding, attention]
---

import transfomer_float from "@site/static/img/transfomer_float.png";

# Transformer 绝对位置编码 (Sinusoidal Positional Encoding) 原理总结

## 1. 背景：为什么 Transformer 需要位置编码？

与 RNN（循环神经网络）不同，Transformer 模型的核心——自注意力机制（Self-Attention）——在处理输入序列时是**并行**的。它一次性查看所有词元（Token），计算它们两两之间的相关性。

这就导致了一个根本问题：**Transformer 天生是"乱序"的（Permutation-Invariant）**。

* 如果不加位置信息，对于模型来说，输入序列 `["我", "爱", "你"]` 和 `["你", "爱", "我"]` 在计算注意力时是完全一样的。

为了让模型理解语言的顺序结构（哪个词在前，哪个词在后），必须在输入层显式地注入位置信息。

## 2. 核心方案：正弦绝对位置编码

原始 Transformer 论文 ("Attention Is All You Need") 提出了一种基于三角函数（正弦和余弦）的**绝对位置编码**方案。

### 2.1 基本思想

给序列中的每一个绝对位置索引（pos = 0, 1, 2...），生成一个**固定且唯一**的向量（Position Embedding, PE）。然后将这个向量直接**加**到对应的词向量（Word Embedding）上。

$$\text{最终输入向量} = \text{词向量}(\text{Word Embedding}) + \text{位置向量}(\text{Positional Embedding})$$

### 2.2 生成公式

位置向量不是通过训练学出来的，而是通过以下公式直接计算得到的：

对于位置 $pos$ 的词，其位置向量的第 $i$ 个维度（偶数维用 sin，奇数维用 cos）的值为：

* **偶数维度 ($2i$):** $PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$
* **奇数维度 ($2i+1$):** $PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$

其中：
* $pos$：序列中的位置索引（如第 5 个词）。
* $d_{model}$：向量的总维度（如 512 或热力图中的 128）。
* $i$：当前的维度索引。

### 2.3 机制解析：多频率"时钟"系统

这个公式的核心在于分母中的频率项 ($10000^{2i/d_{model}}$)。它使得向量中不同维度的波长（频率）各不相同：

* **在低维度（$i$ 较小）：** 分母小，频率极高，波长极短。数值随位置变化剧烈。
* **在高维度（$i$ 较大）：** 分母大（接近 10000），频率极低，波长极长。数值随位置变化缓慢。

这就像一个拥有几百个指针的时钟系统，有的指针（低维）转得飞快（像秒针），有的指针（高维）几乎不动（像时针）。组合起来，就能唯一确定一个时刻（位置）。

## 3. 可视化解读：热力图分析

<img src={transfomer_float} alt="Transformer 正弦位置编码示意图" style={{width: '80%', display: 'block', margin: '32px auto'}} />

结合热力图，我们可以直观地理解位置编码的原理：

* **纵轴 (Y-axis):** 序列位置 (pos)，从 0 到 50。
* **横轴 (X-axis):** 嵌入维度 (depth)，从 0 到 128。
* **颜色:** 代表正弦/余弦函数的值，黄色为正(+1)，蓝色为负(-1)。

### 3.1 左侧区域（低嵌入维度）—— 高频区

* **观察：** 我们看到非常密集、窄细的蓝黄交替条纹。
* **含义：** 这里的频率非常高。对于相邻的两个位置（比如 pos=10 和 pos=11），它们在这些维度上的颜色（数值）差异巨大。
* **作用：** 用于**精确定位**，区分非常相邻的词。

### 3.2 右侧区域（高嵌入维度）—— 低频区

* **观察：** 条纹变得非常宽，颜色渐变极其缓慢。
* **含义：** 这里的频率非常低。对于相邻的位置，颜色几乎没有变化；只有跨越很长的距离（比如 pos=0 和 pos=50），颜色才有明显的区别。
* **作用：** 用于**粗略定位**，捕捉长距离的全局位置信息。

### 3.3 总结

横向上看，每一行（代表一个位置的 PE 向量）都是一个独特的"指纹"。这个指纹由从快到慢变化的多种波形组合而成，保证了任意两个位置的编码都不会重复。

## 4. 为什么选择这种方式？（核心优势）

1. **无需训练 (Deterministic)：** 计算简单，不需要占用模型参数。

2. **支持一定程度的外推性：** 虽然是绝对位置编码，但由于三角函数的周期性和连续性，理论上模型处理比训练集稍长的序列时不会完全崩溃（相比于学习出来的绝对位置编码）。

3. **隐式捕捉相对距离（关键）：**
   这是该设计最精妙之处。利用三角函数的线性表示性质：

   $$\sin(pos+k) = \sin(pos)\cos(k) + \cos(pos)\sin(k)$$

   这意味着，位置 $pos+k$ 的编码可以由位置 $pos$ 的编码经过一个线性的旋转变换得到。这使得模型在计算注意力（点积）时，能够**隐式地感知到词与词之间的相对距离 $k$**，尽管输入的是绝对位置。

## 5. 与其他位置编码方式的对比

| 编码方式 | 特点 | 优势 | 劣势 |
|---------|------|------|------|
| **正弦绝对位置编码** | 固定公式计算 | 无需训练、支持外推 | 仅编码绝对位置 |
| **可学习位置编码** | 训练得到位置向量 | 灵活适应任务 | 难以外推到训练长度之外 |
| **相对位置编码** | 编码词对之间的距离 | 更好地捕捉相对关系 | 计算复杂度较高 |
| **旋转位置编码 (RoPE)** | 通过旋转矩阵编码位置 | 同时具有绝对和相对位置信息 | 实现稍复杂 |

## 6. 参考资料

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - 原始 Transformer 论文
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - 图解 Transformer
