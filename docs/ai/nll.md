# 📘 概念文档：θ 与负对数似然 (NLL)

## 1. θ（模型参数）
- **定义**：θ 表示模型的所有可调参数。  
- **在神经网络里**：θ = 权重矩阵 + 偏置向量。  
- **作用**：不同的 θ 会导致模型输出不同的概率分布。  
- **目标**：找到一个最优的 θ，使得模型对训练数据的预测最合理。  

---

## 2. 似然 (Likelihood)
- 表示在参数 θ 下，整个数据集出现的概率。  
- 公式：  
  \[
  L(\theta) = \prod_{i=1}^N P_\theta(y_i|x_i)
  \]

---

## 3. 对数似然 (Log-likelihood)
- 对似然取对数，方便计算和求导：  
  \[
  \log L(\theta) = \sum_{i=1}^N \log P_\theta(y_i|x_i)
  \]
- **好处**：  
  1. 把乘法变加法，更稳定  
  2. 避免数值下溢  
  3. 求导更方便  

---

## 4. 负对数似然 (Negative Log-likelihood, NLL)
- 在机器学习里常用作损失函数：  
  \[
  \text{NLL}(\theta) = - \sum_{i=1}^N \log P_\theta(y_i|x_i)
  \]
- **直觉**：  
  - 如果模型对真实标签给出高概率 → NLL 小  
  - 如果模型对真实标签给出低概率 → NLL 大  

---

## 5. NLL 的意义
- **训练目标**：最小化 NLL = 最大化似然  
- **解释**：NLL 度量“模型对真实数据有多惊讶”。  
  - 小 NLL → 模型很“淡定”，说明学得好  
  - 大 NLL → 模型很“惊讶”，说明学得差  
- **应用**：几乎所有分类任务的损失函数（交叉熵）都是 NLL 的变形。  

---

✅ **一句话总结**：  
θ = 模型参数（权重+偏置）；  
NLL = 衡量 θ 好坏的指标，NLL 越小，模型越贴近真实数据，效果越好。  