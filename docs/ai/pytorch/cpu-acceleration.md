---
title: PyTorch CPU åŠ é€Ÿæ•ˆæœè¯¦è§£
sidebar_label: CPU åŠ é€Ÿæ•ˆæœ
date: 2025-11-09
last_update:
  date: 2025-11-09
---

# PyTorch CPU åŠ é€Ÿæ•ˆæœè¯¦è§£

## æ ¸å¿ƒç»“è®º

æ˜¯çš„ï¼PyTorch åœ¨ CPU ä¸Šä¹Ÿæœ‰æ˜æ˜¾åŠ é€Ÿæ•ˆæœã€‚

ç›¸æ¯”çº¯ Python ä»£ç ï¼ŒPyTorch (å³ä½¿åœ¨ CPU ä¸Š) ä¹Ÿèƒ½æœ‰**å‡ ååˆ°ä¸Šç™¾å€**çš„åŠ é€Ÿã€‚

---

## å®é™…æµ‹è¯•å¯¹æ¯”

```python
import torch
import time
import numpy as np

n = 10000

# 1. çº¯ Python (æœ€æ…¢)
def pure_python_matmul(a, b):
    result = [[0] * n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            for k in range(n):
                result[i][j] += a[i][k] * b[k][j]
    return result

# 2. NumPy (å¿«)
a_np = np.random.randn(n, n)
b_np = np.random.randn(n, n)

start = time.time()
c_np = np.dot(a_np, b_np)
print(f"NumPy (CPU): {time.time() - start:.4f}ç§’")

# 3. PyTorch CPU (å·®ä¸å¤šå¿«)
a_torch = torch.randn(n, n)
b_torch = torch.randn(n, n)

start = time.time()
c_torch = torch.mm(a_torch, b_torch)
print(f"PyTorch (CPU): {time.time() - start:.4f}ç§’")

# 4. PyTorch GPU (è¶…å¿«ï¼)
if torch.cuda.is_available():
    a_gpu = a_torch.cuda()
    b_gpu = b_torch.cuda()

    start = time.time()
    c_gpu = torch.mm(a_gpu, b_gpu)
    torch.cuda.synchronize()
    print(f"PyTorch (GPU): {time.time() - start:.4f}ç§’")
```

### å…¸å‹ç»“æœ (10000Ã—10000 çŸ©é˜µä¹˜æ³•)

```
çº¯ Python:      ~30åˆ†é’Ÿ (å¤ªæ…¢äº†ï¼Œä¸€èˆ¬ä¸ä¼šçœŸè·‘å®Œ)
NumPy (CPU):    ~2ç§’
PyTorch (CPU):  ~2ç§’
PyTorch (GPU):  ~0.05ç§’
```

---

## ä¸ºä»€ä¹ˆ PyTorch CPU ä¹Ÿå¿«ï¼Ÿ

### 1. åº•å±‚ç”¨ C++ å®ç°

```python
# çœ‹èµ·æ¥æ˜¯ Pythonï¼Œå®é™…è°ƒç”¨ C++ ä»£ç 
result = torch.mm(a, b)  # åº•å±‚æ˜¯é«˜åº¦ä¼˜åŒ–çš„ C++
```

### 2. ä½¿ç”¨ä¼˜åŒ–çš„æ•°å­¦åº“

- **Intel MKL** (Math Kernel Library)
- **OpenBLAS**
- è¿™äº›åº“ç»è¿‡å‡ åå¹´ä¼˜åŒ–ï¼Œç”¨äº† SIMD æŒ‡ä»¤ç­‰

### 3. å‘é‡åŒ–æ“ä½œ

```python
# çº¯ Python: é€å…ƒç´ å¾ªç¯ (æ…¢)
for i in range(len(a)):
    c[i] = a[i] + b[i]

# PyTorch: å‘é‡åŒ– (å¿«)
c = a + b  # ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªæ•°ç»„
```

### 4. å¤šçº¿ç¨‹å¹¶è¡Œ

PyTorch ä¼šè‡ªåŠ¨ä½¿ç”¨å¤šä¸ª CPU æ ¸å¿ƒ

---

## PyTorch CPU vs NumPy

### CPU æ€§èƒ½å¯¹æ¯”

**æ€§èƒ½å·®ä¸å¤š**ï¼Œå› ä¸ºï¼š
- éƒ½ç”¨ç±»ä¼¼çš„åº•å±‚åº“ (BLAS, LAPACK)
- éƒ½æ˜¯å‘é‡åŒ–æ“ä½œ
- éƒ½æ”¯æŒå¤šçº¿ç¨‹

### âš ï¸ å…³é”®åŒºåˆ«ï¼šGPU æ”¯æŒ

**NumPy çš„é™åˆ¶ï¼š**
- âŒ **åªæ”¯æŒ CPU**ï¼Œå®Œå…¨ä¸æ”¯æŒ GPU åŠ é€Ÿ
- âŒ å³ä½¿æœ‰ GPUï¼ŒNumPy ä¹Ÿæ— æ³•ä½¿ç”¨
- âŒ å¤§è§„æ¨¡çŸ©é˜µè¿ç®—å—é™äº CPU æ€§èƒ½

**PyTorch çš„ä¼˜åŠ¿ï¼š**
- âœ… åŒæ—¶æ”¯æŒ CPU å’Œ GPU
- âœ… ä»£ç å‡ ä¹ä¸ç”¨æ”¹ï¼Œåªéœ€ `.cuda()` æˆ– `.to(device)`
- âœ… GPU å¯ä»¥å¸¦æ¥ 50-100 å€çš„åŠ é€Ÿ

```python
import numpy as np
import torch

# NumPy: åªèƒ½åœ¨ CPU ä¸Šè¿è¡Œ
a_np = np.random.randn(10000, 10000)
b_np = np.random.randn(10000, 10000)
c_np = np.dot(a_np, b_np)  # âŒ æ— æ³•ä½¿ç”¨ GPUï¼Œçº¦ 2 ç§’

# PyTorch: å¯ä»¥é€‰æ‹© CPU æˆ– GPU
a_torch = torch.randn(10000, 10000)
b_torch = torch.randn(10000, 10000)

# CPU æ¨¡å¼ï¼ˆå’Œ NumPy å·®ä¸å¤šï¼‰
c_cpu = torch.mm(a_torch, b_torch)  # çº¦ 2 ç§’

# GPU æ¨¡å¼ï¼ˆå¿«å¾—å¤šï¼ï¼‰
if torch.cuda.is_available():
    a_gpu = a_torch.cuda()
    b_gpu = b_torch.cuda()
    c_gpu = torch.mm(a_gpu, b_gpu)  # âœ… çº¦ 0.05 ç§’ï¼Œå¿« 40 å€ï¼
```

### ğŸ“Œ é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èå·¥å…· | åŸå›  |
|------|---------|------|
| åªåšæ•°å€¼è®¡ç®—ï¼Œæ•°æ®é‡å° | NumPy | æ›´è½»é‡ï¼Œç”Ÿæ€æˆç†Ÿ |
| éœ€è¦è®­ç»ƒç¥ç»ç½‘ç»œ | PyTorch | è‡ªåŠ¨å¾®åˆ†ï¼Œçµæ´» |
| **éœ€è¦ GPU åŠ é€Ÿ** | **PyTorch** | **NumPy ä¸æ”¯æŒ GPU** |
| å¤§è§„æ¨¡çŸ©é˜µè¿ç®— | PyTorch (GPU) | æ€§èƒ½è¿œè¶… NumPy |
| ç§‘å­¦è®¡ç®—ï¼ˆSciPy ç”Ÿæ€ï¼‰ | NumPy | å·¥å…·é“¾å®Œå–„ |

### ğŸ’¡ ä» NumPy è¿ç§»åˆ° PyTorch

å¦‚æœä½ çš„ NumPy ä»£ç éœ€è¦ GPU åŠ é€Ÿï¼Œè¿ç§»å¾ˆç®€å•ï¼š

```python
# NumPy ä»£ç 
import numpy as np
a = np.random.randn(1000, 1000)
b = np.random.randn(1000, 1000)
c = np.dot(a, b)
result = np.sum(c)

# ç­‰ä»·çš„ PyTorch ä»£ç ï¼ˆCPUï¼‰
import torch
a = torch.randn(1000, 1000)
b = torch.randn(1000, 1000)
c = torch.mm(a, b)
result = torch.sum(c)

# ä¸€è¡Œä»£ç åˆ‡æ¢åˆ° GPU
a = torch.randn(1000, 1000).cuda()
b = torch.randn(1000, 1000).cuda()
c = torch.mm(a, b)  # è‡ªåŠ¨åœ¨ GPU ä¸Šè¿è¡Œ
result = torch.sum(c)
```

**å…³é”®ç‚¹ï¼š**
- PyTorch çš„ API å’Œ NumPy éå¸¸ç›¸ä¼¼ï¼ˆ`np.dot` â†’ `torch.mm`ï¼‰
- NumPy ä¸æ”¯æŒ GPUï¼Œå¦‚æœéœ€è¦ GPU åŠ é€Ÿå¿…é¡»ç”¨ PyTorch æˆ–å…¶ä»–æ¡†æ¶
- PyTorch åœ¨ CPU ä¸Šæ€§èƒ½å’Œ NumPy ç›¸å½“ï¼Œä½†åœ¨ GPU ä¸Šå¿«å‡ åå€

---

## æ€»ç»“

| | çº¯ Python | NumPy/PyTorch (CPU) | PyTorch (GPU) |
|---|---|---|---|
| çŸ©é˜µä¹˜æ³• | 100å€æ—¶é—´ | 1å€æ—¶é—´ (åŸºå‡†) | **50-100å€æ›´å¿«** |
| å®ç° | Python å¾ªç¯ | C++/Fortran | CUDA |

**ç»“è®º**: PyTorch åœ¨ CPU ä¸Šä¹Ÿæ¯”çº¯ Python å¿«å¾—å¤šï¼Œä½† GPU æ‰æ˜¯çœŸæ­£çš„"æ ¸æ­¦å™¨" ğŸš€

---

## ä»€ä¹ˆæ—¶å€™ç”¨ CPUï¼Œä»€ä¹ˆæ—¶å€™ç”¨ GPUï¼Ÿ

### âœ… é€‚åˆç”¨ CPU çš„åœºæ™¯

1. **æ•°æ®é‡å°**
   ```python
   # å°æ¨¡å‹ + å°æ•°æ®ï¼ŒGPU ä¸åˆ’ç®—
   x = torch.randn(32, 10)  # 32 æ ·æœ¬ï¼Œ10 ç‰¹å¾
   model = nn.Linear(10, 1)  # åªæœ‰ 11 ä¸ªå‚æ•°
   ```

2. **è°ƒè¯•ä»£ç **
   ```python
   # å¼€å‘é˜¶æ®µç”¨ CPU æ›´æ–¹ä¾¿
   model = MyModel()  # ä¸ç”¨ .cuda()
   output = model(data)
   ```

3. **ä¸æ”¯æŒ GPU çš„æ“ä½œ**
   ```python
   # æŸäº›æ“ä½œåªèƒ½åœ¨ CPU ä¸Šè¿è¡Œ
   cpu_data = gpu_data.cpu()
   numpy_array = cpu_data.numpy()
   ```

### âœ… é€‚åˆç”¨ GPU çš„åœºæ™¯

1. **å¤§æ¨¡å‹è®­ç»ƒ**
   ```python
   model = TransformerModel(layers=12, hidden=768)  # ç™¾ä¸‡å‚æ•°
   data = torch.randn(128, 512, 768)  # å¤§ batch size
   ```

2. **æ‰¹é‡æ¨ç†**
   ```python
   # ä¸€æ¬¡å¤„ç†å¤§é‡æ•°æ®
   images = torch.randn(1000, 3, 224, 224).cuda()
   predictions = model(images)
   ```

3. **çŸ©é˜µå¯†é›†è¿ç®—**
   ```python
   # å·ç§¯ã€å…¨è¿æ¥ç­‰å¯†é›†è¿ç®—
   conv = nn.Conv2d(64, 128, 3).cuda()
   x = torch.randn(32, 64, 224, 224).cuda()
   output = conv(x)  # GPU å¿« 50-100 å€
   ```

---

## æœ€ä½³å®è·µ

### 1. å¼€å‘æ—¶ç”¨ CPUï¼Œè®­ç»ƒæ—¶ç”¨ GPU

```python
# ä½¿ç”¨ device å‚æ•°ï¼Œæ–¹ä¾¿åˆ‡æ¢
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MyModel().to(device)
data = data.to(device)
```

### 2. å°å¿ƒ CPU-GPU ä¼ è¾“å¼€é”€

```python
# âŒ é¢‘ç¹ä¼ è¾“ï¼Œå¾ˆæ…¢
for data in dataloader:
    data = data.cuda()  # æ¯æ¬¡éƒ½ä¼ è¾“ï¼Œæ…¢
    output = model(data)

# âœ… ä½¿ç”¨ pin_memory åŠ é€Ÿ
dataloader = DataLoader(dataset, pin_memory=True, num_workers=4)
```

### 3. æ··åˆä½¿ç”¨

```python
# å¤æ‚æ§åˆ¶æµåœ¨ CPUï¼Œå¯†é›†è®¡ç®—åœ¨ GPU
if some_condition:  # CPU åˆ¤æ–­
    x = x.cuda()
    x = heavy_computation(x)  # GPU è®¡ç®—
    x = x.cpu()  # ä¼ å› CPU
```

---

## å‚è€ƒèµ„æ–™

- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [Intel MKL for PyTorch](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-math-kernel-library.html)
