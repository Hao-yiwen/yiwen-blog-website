---
title: PyTorch CPU åŠ é€Ÿæ•ˆæœè¯¦è§£
sidebar_label: CPU åŠ é€Ÿæ•ˆæœ
date: 2025-11-09
last_update:
  date: 2025-11-09
---

# PyTorch CPU åŠ é€Ÿæ•ˆæœè¯¦è§£

## æ ¸å¿ƒç»“è®º

æ˜¯çš„ï¼PyTorch åœ¨ CPU ä¸Šä¹Ÿæœ‰æ˜æ˜¾åŠ é€Ÿæ•ˆæœã€‚

ç›¸æ¯”çº¯ Python ä»£ç ï¼ŒPyTorch (å³ä½¿åœ¨ CPU ä¸Š) ä¹Ÿèƒ½æœ‰**å‡ ååˆ°ä¸Šç™¾å€**çš„åŠ é€Ÿã€‚

---

## å®é™…æµ‹è¯•å¯¹æ¯”

```python
import torch
import time
import numpy as np

n = 10000

# 1. çº¯ Python (æœ€æ…¢)
def pure_python_matmul(a, b):
    result = [[0] * n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            for k in range(n):
                result[i][j] += a[i][k] * b[k][j]
    return result

# 2. NumPy (å¿«)
a_np = np.random.randn(n, n)
b_np = np.random.randn(n, n)

start = time.time()
c_np = np.dot(a_np, b_np)
print(f"NumPy (CPU): {time.time() - start:.4f}ç§’")

# 3. PyTorch CPU (å·®ä¸å¤šå¿«)
a_torch = torch.randn(n, n)
b_torch = torch.randn(n, n)

start = time.time()
c_torch = torch.mm(a_torch, b_torch)
print(f"PyTorch (CPU): {time.time() - start:.4f}ç§’")

# 4. PyTorch GPU (è¶…å¿«ï¼)
if torch.cuda.is_available():
    a_gpu = a_torch.cuda()
    b_gpu = b_torch.cuda()

    start = time.time()
    c_gpu = torch.mm(a_gpu, b_gpu)
    torch.cuda.synchronize()
    print(f"PyTorch (GPU): {time.time() - start:.4f}ç§’")
```

### å…¸å‹ç»“æœ (10000Ã—10000 çŸ©é˜µä¹˜æ³•)

```
çº¯ Python:      ~30åˆ†é’Ÿ (å¤ªæ…¢äº†ï¼Œä¸€èˆ¬ä¸ä¼šçœŸè·‘å®Œ)
NumPy (CPU):    ~2ç§’
PyTorch (CPU):  ~2ç§’
PyTorch (GPU):  ~0.05ç§’
```

---

## ä¸ºä»€ä¹ˆ PyTorch CPU ä¹Ÿå¿«ï¼Ÿ

### 1. åº•å±‚ç”¨ C++ å®ç°

```python
# çœ‹èµ·æ¥æ˜¯ Pythonï¼Œå®é™…è°ƒç”¨ C++ ä»£ç 
result = torch.mm(a, b)  # åº•å±‚æ˜¯é«˜åº¦ä¼˜åŒ–çš„ C++
```

### 2. ä½¿ç”¨ä¼˜åŒ–çš„æ•°å­¦åº“

- **Intel MKL** (Math Kernel Library)
- **OpenBLAS**
- è¿™äº›åº“ç»è¿‡å‡ åå¹´ä¼˜åŒ–ï¼Œç”¨äº† SIMD æŒ‡ä»¤ç­‰

### 3. å‘é‡åŒ–æ“ä½œ

```python
# çº¯ Python: é€å…ƒç´ å¾ªç¯ (æ…¢)
for i in range(len(a)):
    c[i] = a[i] + b[i]

# PyTorch: å‘é‡åŒ– (å¿«)
c = a + b  # ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªæ•°ç»„
```

### 4. å¤šçº¿ç¨‹å¹¶è¡Œ

PyTorch ä¼šè‡ªåŠ¨ä½¿ç”¨å¤šä¸ª CPU æ ¸å¿ƒ

---

## PyTorch CPU vs NumPy

**æ€§èƒ½å·®ä¸å¤š**ï¼Œå› ä¸ºï¼š
- éƒ½ç”¨ç±»ä¼¼çš„åº•å±‚åº“ (BLAS, LAPACK)
- éƒ½æ˜¯å‘é‡åŒ–æ“ä½œ
- éƒ½æ”¯æŒå¤šçº¿ç¨‹

**é€‰æ‹©å»ºè®®ï¼š**
- å¦‚æœåªåšæ•°å€¼è®¡ç®— â†’ NumPy æ›´è½»é‡
- å¦‚æœè¦è®­ç»ƒç¥ç»ç½‘ç»œ/è‡ªåŠ¨æ±‚å¯¼ â†’ PyTorch
- å¦‚æœå¯èƒ½ç”¨ GPU â†’ PyTorch

---

## æ€»ç»“

| | çº¯ Python | NumPy/PyTorch (CPU) | PyTorch (GPU) |
|---|---|---|---|
| çŸ©é˜µä¹˜æ³• | 100å€æ—¶é—´ | 1å€æ—¶é—´ (åŸºå‡†) | **50-100å€æ›´å¿«** |
| å®ç° | Python å¾ªç¯ | C++/Fortran | CUDA |

**ç»“è®º**: PyTorch åœ¨ CPU ä¸Šä¹Ÿæ¯”çº¯ Python å¿«å¾—å¤šï¼Œä½† GPU æ‰æ˜¯çœŸæ­£çš„"æ ¸æ­¦å™¨" ğŸš€

---

## ä»€ä¹ˆæ—¶å€™ç”¨ CPUï¼Œä»€ä¹ˆæ—¶å€™ç”¨ GPUï¼Ÿ

### âœ… é€‚åˆç”¨ CPU çš„åœºæ™¯

1. **æ•°æ®é‡å°**
   ```python
   # å°æ¨¡å‹ + å°æ•°æ®ï¼ŒGPU ä¸åˆ’ç®—
   x = torch.randn(32, 10)  # 32 æ ·æœ¬ï¼Œ10 ç‰¹å¾
   model = nn.Linear(10, 1)  # åªæœ‰ 11 ä¸ªå‚æ•°
   ```

2. **è°ƒè¯•ä»£ç **
   ```python
   # å¼€å‘é˜¶æ®µç”¨ CPU æ›´æ–¹ä¾¿
   model = MyModel()  # ä¸ç”¨ .cuda()
   output = model(data)
   ```

3. **ä¸æ”¯æŒ GPU çš„æ“ä½œ**
   ```python
   # æŸäº›æ“ä½œåªèƒ½åœ¨ CPU ä¸Šè¿è¡Œ
   cpu_data = gpu_data.cpu()
   numpy_array = cpu_data.numpy()
   ```

### âœ… é€‚åˆç”¨ GPU çš„åœºæ™¯

1. **å¤§æ¨¡å‹è®­ç»ƒ**
   ```python
   model = TransformerModel(layers=12, hidden=768)  # ç™¾ä¸‡å‚æ•°
   data = torch.randn(128, 512, 768)  # å¤§ batch size
   ```

2. **æ‰¹é‡æ¨ç†**
   ```python
   # ä¸€æ¬¡å¤„ç†å¤§é‡æ•°æ®
   images = torch.randn(1000, 3, 224, 224).cuda()
   predictions = model(images)
   ```

3. **çŸ©é˜µå¯†é›†è¿ç®—**
   ```python
   # å·ç§¯ã€å…¨è¿æ¥ç­‰å¯†é›†è¿ç®—
   conv = nn.Conv2d(64, 128, 3).cuda()
   x = torch.randn(32, 64, 224, 224).cuda()
   output = conv(x)  # GPU å¿« 50-100 å€
   ```

---

## æœ€ä½³å®è·µ

### 1. å¼€å‘æ—¶ç”¨ CPUï¼Œè®­ç»ƒæ—¶ç”¨ GPU

```python
# ä½¿ç”¨ device å‚æ•°ï¼Œæ–¹ä¾¿åˆ‡æ¢
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MyModel().to(device)
data = data.to(device)
```

### 2. å°å¿ƒ CPU-GPU ä¼ è¾“å¼€é”€

```python
# âŒ é¢‘ç¹ä¼ è¾“ï¼Œå¾ˆæ…¢
for data in dataloader:
    data = data.cuda()  # æ¯æ¬¡éƒ½ä¼ è¾“ï¼Œæ…¢
    output = model(data)

# âœ… ä½¿ç”¨ pin_memory åŠ é€Ÿ
dataloader = DataLoader(dataset, pin_memory=True, num_workers=4)
```

### 3. æ··åˆä½¿ç”¨

```python
# å¤æ‚æ§åˆ¶æµåœ¨ CPUï¼Œå¯†é›†è®¡ç®—åœ¨ GPU
if some_condition:  # CPU åˆ¤æ–­
    x = x.cuda()
    x = heavy_computation(x)  # GPU è®¡ç®—
    x = x.cpu()  # ä¼ å› CPU
```

---

## å‚è€ƒèµ„æ–™

- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [Intel MKL for PyTorch](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-math-kernel-library.html)
