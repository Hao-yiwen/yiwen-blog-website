# subword tokenization（子词分词器）

## BPE

BPE就是将文本先拆分成模型能处理的词表，不是简单的按字母或者词进行切割，而是按照常见的“字节/字词组合”来切。

### 怎么做的
1.	一开始，把句子拆成最小单位（通常是单个字母或字符）。
    -	例如 "lower" → ["l", "o", "w", "e", "r"]。
2.	统计所有相邻字符的出现频率。
    -	比如 "lo", "ow", "we", "er" 哪个出现得多。
3.	把出现最频繁的一对字符合并成一个新符号。
    - 	比如把 "e" + "r" 合成 "er"。
4.	重复这个过程，不断合并常见的组合。
    -	"lo" → "lo", "wer" → "wer"，最后可能得到 "lower" 整体。
5.	最终形成一个“子词词表”，既能覆盖常见词，也能把生僻词拆成小块。

### 好处
-	词表不至于爆炸（不像每个完整词都要一个 id）。
-	常见词能整体存储（效率高）。
-	生僻词还能拆开处理（不丢失信息）。

## WordPiece

### 背景
-	BPE：完全靠“出现次数”来决定哪些字符/子词合并。
-	WordPiece：除了考虑出现次数，还会看 合并后能不能提升句子的概率（信息增益）。

### WordPiece 的做法
1.	一开始也是把词拆成最小单位（通常是字符）。
-	例如 "playing" → ["p", "l", "a", "y", "i", "n", "g"]。
2.	在大语料里训练，尝试把字符或子词合并。
-	但它不是只看频率，而是算 合并前后句子的概率提升（用语言模型概率来衡量）。
-	如果合并能让模型更“合理”，就保留这个合并。
3.	最后得到一个子词表。常见词可以整体编码，生僻词会被拆成前缀 + 子块。

### 应用
-	WordPiece 最早是 Google 在 BERT 里用的。
-	所以看到 BERT 的分词器（比如 ##ing, ##ly）就是 WordPiece 生成的结果。


## Unigram

Unigram（单元语法模型）是一种 基于概率的子词分词方法，由 Google 提出的 SentencePiece 常用这个。

### 怎么做
1.	准备一个超大的候选词表（比如几十万子词，里面有各种可能的子词）。
2.	假设每个子词有一个概率，句子的概率 = 把这个句子拆成若干子词后，它们概率的乘积。
3.	训练过程：
-	不断尝试移除一些子词，看会不会让句子整体概率降低。
-	把不重要的子词丢掉，最后留下一个合适大小的词表（比如 32k）。
4.	分词时：
-	给一句话，算法会在所有可能的切分方式里，找出 概率最大的拆法。

## 现在使用情况

1.	BPE / Byte-level BPE（Hugging Face tokenizers）
    -	GPT-2、GPT-3、GPT-4、LLaMA 系列、Qwen、Bloom、Falcon … 基本都用。
    -	现在大部分 开源/商用大模型的默认选择。
    -	原因：简单高效、跨语言友好、HF 的 tokenizers 提供超快实现。
2.	SentencePiece（Unigram / BPE）
    -	Google 系：T5、mT5、ALBERT、XLNet …
    -	优势：多语言支持特别强，不依赖空格，训练词表方便。
    -	在学术界和 Google 系模型里用得多，但在社区开源模型里影响力不如 HF tokenizers。
3.	WordPiece
    -	BERT、RoBERTa、DistilBERT 这类老一代 Transformer 模型用的。
    -	优点：基于概率建模，比较精细。
    -	现状：已经算“老技术”，新模型基本不用了。