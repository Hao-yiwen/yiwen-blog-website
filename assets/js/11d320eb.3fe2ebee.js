"use strict";(globalThis.webpackChunkyiwen_blog_website=globalThis.webpackChunkyiwen_blog_website||[]).push([[45217],{7969:(s,e,n)=>{n.r(e),n.d(e,{assets:()=>m,contentTitle:()=>d,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>o});const a=JSON.parse('{"id":"ai/architectures/transformer/transformer_complete_implementation","title":"Transformer \u5b8c\u6574\u67b6\u6784\u8be6\u89e3\u4e0e PyTorch \u5b9e\u73b0","description":"\u8fd9\u662f\u4e00\u4efd\u5b8c\u6574\u7684 Transformer \u67b6\u6784\u4ee3\u7801\u7b14\u8bb0\uff0c\u6574\u5408\u4e86\u4ece\u5e95\u5c42\u7684\u591a\u5934\u6ce8\u610f\u529b\uff08QKV\uff09\u5230\u4e2d\u95f4\u7684\u7f16\u7801\u5668/\u89e3\u7801\u5668\u5757\uff0c\u518d\u5230\u6574\u4f53\u67b6\u6784\u548c\u63a9\u7801\u673a\u5236\u7684\u6240\u6709\u6838\u5fc3\u6982\u5ff5\u3002\u672c\u6587\u5305\u542b\u6781\u5176\u8be6\u5c3d\u7684\u4e2d\u6587\u6ce8\u91ca\uff0c\u5e2e\u52a9\u4f60\u6df1\u5165\u7406\u89e3 Transformer \u7684\u5de5\u4f5c\u539f\u7406\u3002","source":"@site/docs/ai/architectures/transformer/transformer_complete_implementation.md","sourceDirName":"ai/architectures/transformer","slug":"/ai/architectures/transformer/transformer_complete_implementation","permalink":"/yiwen-blog-website/docs/ai/architectures/transformer/transformer_complete_implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/Hao-yiwen/yiwen-blog-website/tree/master/docs/ai/architectures/transformer/transformer_complete_implementation.md","tags":[{"inline":true,"label":"Transformer","permalink":"/yiwen-blog-website/docs/tags/transformer"},{"inline":true,"label":"PyTorch","permalink":"/yiwen-blog-website/docs/tags/py-torch"},{"inline":true,"label":"\u6df1\u5ea6\u5b66\u4e60","permalink":"/yiwen-blog-website/docs/tags/\u6df1\u5ea6\u5b66\u4e60"},{"inline":true,"label":"\u6ce8\u610f\u529b\u673a\u5236","permalink":"/yiwen-blog-website/docs/tags/\u6ce8\u610f\u529b\u673a\u5236"},{"inline":true,"label":"NLP","permalink":"/yiwen-blog-website/docs/tags/nlp"}],"version":"current","lastUpdatedAt":1737504000000,"sidebarPosition":2,"frontMatter":{"title":"Transformer \u5b8c\u6574\u67b6\u6784\u8be6\u89e3\u4e0e PyTorch \u5b9e\u73b0","sidebar_label":"Transformer \u5b8c\u6574\u5b9e\u73b0","sidebar_position":2,"tags":["Transformer","PyTorch","\u6df1\u5ea6\u5b66\u4e60","\u6ce8\u610f\u529b\u673a\u5236","NLP"],"date":"2025-01-22T00:00:00.000Z","last_update":{"date":"2025-01-22T00:00:00.000Z"}},"sidebar":"aiSidebar","previous":{"title":"Transformer","permalink":"/yiwen-blog-website/docs/category/transformer"},"next":{"title":"DPO \u76f4\u63a5\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u6307\u5357","permalink":"/yiwen-blog-website/docs/ai/architectures/transformer/dpo-training-guide"}}');var t=n(74848),r=n(28453);const l=n.p+"assets/images/transformer-29143c6dc1fb5e7c06a2a1cd260689a2.png",i={title:"Transformer \u5b8c\u6574\u67b6\u6784\u8be6\u89e3\u4e0e PyTorch \u5b9e\u73b0",sidebar_label:"Transformer \u5b8c\u6574\u5b9e\u73b0",sidebar_position:2,tags:["Transformer","PyTorch","\u6df1\u5ea6\u5b66\u4e60","\u6ce8\u610f\u529b\u673a\u5236","NLP"],date:new Date("2025-01-22T00:00:00.000Z"),last_update:{date:new Date("2025-01-22T00:00:00.000Z")}},d="Transformer \u5b8c\u6574\u67b6\u6784\u8be6\u89e3\u4e0e PyTorch \u5b9e\u73b0",m={},o=[{value:"\u6838\u5fc3\u601d\u60f3\u56de\u987e\uff1a\u4ece\u4e32\u884c\u5230\u5e76\u884c\u7684\u9769\u547d",id:"\u6838\u5fc3\u601d\u60f3\u56de\u987e\u4ece\u4e32\u884c\u5230\u5e76\u884c\u7684\u9769\u547d",level:2},{value:"\u67b6\u6784\u6838\u5fc3\u601d\u60f3",id:"\u67b6\u6784\u6838\u5fc3\u601d\u60f3",level:3},{value:"\u4ee3\u7801\u7ed3\u6784\u5b8f\u89c2\u5bfc\u822a\uff1a\u4e24\u5ea7\u72ec\u7acb\u7684&quot;\u5927\u697c&quot;",id:"\u4ee3\u7801\u7ed3\u6784\u5b8f\u89c2\u5bfc\u822a\u4e24\u5ea7\u72ec\u7acb\u7684\u5927\u697c",level:2},{value:"1. \u7f16\u7801\u5668\u5854 (Encoder Tower) \u2014\u2014 &quot;\u5168\u77e5\u7684\u8bfb\u4e66\u4eba&quot;",id:"1-\u7f16\u7801\u5668\u5854-encoder-tower--\u5168\u77e5\u7684\u8bfb\u4e66\u4eba",level:3},{value:"2. \u89e3\u7801\u5668\u5854 (Decoder Tower) \u2014\u2014 &quot;\u6234\u7740\u9563\u94d0\u7684\u521b\u4f5c\u8005&quot;",id:"2-\u89e3\u7801\u5668\u5854-decoder-tower--\u6234\u7740\u9563\u94d0\u7684\u521b\u4f5c\u8005",level:3},{value:"\u5b8c\u6574\u4ee3\u7801\u5b9e\u73b0",id:"\u5b8c\u6574\u4ee3\u7801\u5b9e\u73b0",level:2},{value:"\u8fd0\u884c\u793a\u4f8b",id:"\u8fd0\u884c\u793a\u4f8b",level:2},{value:"\u6838\u5fc3\u516c\u5f0f",id:"\u6838\u5fc3\u516c\u5f0f",level:2},{value:"\u4ee3\u7801\u9605\u8bfb\u6307\u5357",id:"\u4ee3\u7801\u9605\u8bfb\u6307\u5357",level:2},{value:"\u5173\u952e\u8981\u70b9\u603b\u7ed3",id:"\u5173\u952e\u8981\u70b9\u603b\u7ed3",level:2},{value:"\u53c2\u8003\u8d44\u6599",id:"\u53c2\u8003\u8d44\u6599",level:2}];function c(s){const e={a:"a",annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mo:"mo",mrow:"mrow",msqrt:"msqrt",msub:"msub",msup:"msup",mtext:"mtext",ol:"ol",p:"p",path:"path",pre:"pre",semantics:"semantics",span:"span",strong:"strong",svg:"svg",ul:"ul",...(0,r.R)(),...s.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"transformer-\u5b8c\u6574\u67b6\u6784\u8be6\u89e3\u4e0e-pytorch-\u5b9e\u73b0",children:"Transformer \u5b8c\u6574\u67b6\u6784\u8be6\u89e3\u4e0e PyTorch \u5b9e\u73b0"})}),"\n",(0,t.jsx)(e.p,{children:"\u8fd9\u662f\u4e00\u4efd\u5b8c\u6574\u7684 Transformer \u67b6\u6784\u4ee3\u7801\u7b14\u8bb0\uff0c\u6574\u5408\u4e86\u4ece\u5e95\u5c42\u7684\u591a\u5934\u6ce8\u610f\u529b\uff08QKV\uff09\u5230\u4e2d\u95f4\u7684\u7f16\u7801\u5668/\u89e3\u7801\u5668\u5757\uff0c\u518d\u5230\u6574\u4f53\u67b6\u6784\u548c\u63a9\u7801\u673a\u5236\u7684\u6240\u6709\u6838\u5fc3\u6982\u5ff5\u3002\u672c\u6587\u5305\u542b\u6781\u5176\u8be6\u5c3d\u7684\u4e2d\u6587\u6ce8\u91ca\uff0c\u5e2e\u52a9\u4f60\u6df1\u5165\u7406\u89e3 Transformer \u7684\u5de5\u4f5c\u539f\u7406\u3002"}),"\n",(0,t.jsx)(e.h2,{id:"\u6838\u5fc3\u601d\u60f3\u56de\u987e\u4ece\u4e32\u884c\u5230\u5e76\u884c\u7684\u9769\u547d",children:"\u6838\u5fc3\u601d\u60f3\u56de\u987e\uff1a\u4ece\u4e32\u884c\u5230\u5e76\u884c\u7684\u9769\u547d"}),"\n",(0,t.jsx)(e.p,{children:"\u8fd9\u6bb5\u4ee3\u7801\u590d\u73b0\u4e86 2017 \u5e74 Google \u8bba\u6587\u300aAttention Is All You Need\u300b\u4e2d\u63d0\u51fa\u7684\u6807\u51c6 Transformer \u67b6\u6784\u3002\u5b83\u7684\u6838\u5fc3\u4f7f\u547d\u662f\u89e3\u51b3\u4ee5\u5f80 RNN/LSTM \u6a21\u578b\u65e0\u6cd5\u5e76\u884c\u8ba1\u7b97\u4e14\u5bb9\u6613\u9057\u5fd8\u957f\u8ddd\u79bb\u4fe1\u606f\u7684\u75db\u70b9\u3002"}),"\n",(0,t.jsxs)(e.p,{children:["Transformer \u6210\u529f\u7684\u79d8\u8bc0\u5728\u4e8e\u5b8c\u5168\u629b\u5f03\u4e86\u5faa\u73af\u7ed3\u6784\uff0c\u8f6c\u800c\u4f9d\u8d56\u5f3a\u5927\u7684",(0,t.jsx)(e.strong,{children:"\u6ce8\u610f\u529b\u673a\u5236 (Attention Mechanism)"})," \u6765\u6355\u6349\u5e8f\u5217\u4e2d\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002"]}),"\n",(0,t.jsx)(e.h3,{id:"\u67b6\u6784\u6838\u5fc3\u601d\u60f3",children:"\u67b6\u6784\u6838\u5fc3\u601d\u60f3"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u629b\u5f03 RNN"}),"\uff1a\u4f7f\u7528\u7eaf\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u4f4d\u7f6e\u7f16\u7801"}),"\uff1a\u5f25\u8865\u6ca1\u6709\u5faa\u73af\u7ed3\u6784\u5bfc\u81f4\u7684\u4f4d\u7f6e\u4fe1\u606f\u4e22\u5931"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u591a\u5934\u6ce8\u610f\u529b"}),'\uff1a\u8ba9\u6a21\u578b\u4ece\u591a\u4e2a"\u89c6\u89d2"\u7406\u89e3\u8bcd\u8bed\u95f4\u7684\u5173\u8054']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Encoder-Decoder \u7ed3\u6784"}),"\uff1a","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Encoder (\u7f16\u7801\u5668\u5854)"}),'\uff1a\u8d1f\u8d23\u6df1\u523b"\u8bfb\u61c2"\u6e90\u8f93\u5165\uff0c\u4ea7\u51fa"\u6700\u7ec8\u7b14\u8bb0"(Memory)']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Decoder (\u89e3\u7801\u5668\u5854)"}),'\uff1a\u8d1f\u8d23"\u770b\u7740\u7b14\u8bb0\u5199\u4f5c\u6587"\uff0c\u81ea\u56de\u5f52\u5730\u751f\u6210\u76ee\u6807\u8f93\u51fa']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"\u4ee3\u7801\u7ed3\u6784\u5b8f\u89c2\u5bfc\u822a\u4e24\u5ea7\u72ec\u7acb\u7684\u5927\u697c",children:'\u4ee3\u7801\u7ed3\u6784\u5b8f\u89c2\u5bfc\u822a\uff1a\u4e24\u5ea7\u72ec\u7acb\u7684"\u5927\u697c"'}),"\n",(0,t.jsx)("img",{src:l,alt:"Transformer \u67b6\u6784\u56fe"}),"\n",(0,t.jsx)(e.p,{children:'\u5728\u9605\u8bfb\u8fd9\u4efd\u4ee3\u7801\u65f6\uff0c\u8bf7\u52a1\u5fc5\u7262\u8bb0\uff1aTransformer \u5e76\u4e0d\u662f\u7f16\u7801\u4e00\u5c42\u3001\u89e3\u7801\u4e00\u5c42\u4ea4\u66ff\u8fdb\u884c\u7684\u3002\u5b83\u7684\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b\u662f**\u4e24\u5ea7\u72ec\u7acb\u5806\u53e0\u7684"\u5854\u697c"**\u524d\u540e\u63a5\u529b\u5b8c\u6210\u7684\uff1a'}),"\n",(0,t.jsx)(e.h3,{id:"1-\u7f16\u7801\u5668\u5854-encoder-tower--\u5168\u77e5\u7684\u8bfb\u4e66\u4eba",children:'1. \u7f16\u7801\u5668\u5854 (Encoder Tower) \u2014\u2014 "\u5168\u77e5\u7684\u8bfb\u4e66\u4eba"'}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u4efb\u52a1"}),'\uff1a\u8d1f\u8d23"\u8bfb"\u3002\u5e76\u884c\u5730\u63a5\u6536\u6574\u4e2a\u6e90\u53e5\u5b50\uff0c\u6df1\u523b\u7406\u89e3\u5176\u542b\u4e49']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u6d41\u7a0b"}),"\uff1a\u6570\u636e\u4ece\u5e95\u5c42\u7684 EncoderLayer \u4e00\u8def\u5411\u4e0a\u8dd1\u5b8c N \u5c42"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u4ea7\u51fa"}),'\uff1a\u6700\u7ec8\u751f\u6210\u4e00\u4efd\u5305\u542b\u9ad8\u5ea6\u6d53\u7f29\u8bed\u4e49\u4fe1\u606f\u7684**"\u6700\u7ec8\u7b14\u8bb0" (Memory \u77e9\u9635)**\u3002\u8fd9\u4efd\u7b14\u8bb0\u5728\u63a5\u4e0b\u6765\u7684\u89e3\u7801\u9636\u6bb5\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f9b\u89e3\u7801\u5668\u53cd\u590d\u67e5\u9605']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u6838\u5fc3\u7ec4\u4ef6"}),"\uff1a\u591a\u5934\u81ea\u6ce8\u610f\u529b (Multi-Head Self-Attention)\uff0c\u7528\u4e8e\u5efa\u7acb\u6e90\u53e5\u5b50\u5185\u90e8\u8bcd\u4e0e\u8bcd\u7684\u8054\u7cfb"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"2-\u89e3\u7801\u5668\u5854-decoder-tower--\u6234\u7740\u9563\u94d0\u7684\u521b\u4f5c\u8005",children:'2. \u89e3\u7801\u5668\u5854 (Decoder Tower) \u2014\u2014 "\u6234\u7740\u9563\u94d0\u7684\u521b\u4f5c\u8005"'}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u4efb\u52a1"}),'\uff1a\u8d1f\u8d23"\u5199"\u3002\u50cf\u6253\u5b57\u673a\u4e00\u6837\uff0c\u81ea\u56de\u5f52\u5730\u4e00\u4e2a\u5b57\u4e00\u4e2a\u5b57\u751f\u6210\u76ee\u6807\u53e5\u5b50']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u6d41\u7a0b"}),"\uff1a\u5728\u751f\u6210\u6bcf\u4e00\u4e2a\u5b57\u65f6\uff0c\u6570\u636e\u90fd\u8981\u8dd1\u5b8c N \u5c42\u89e3\u7801\u5668"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u4e24\u5927\u5173\u952e\u673a\u5236"}),"\uff1a","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:'"\u5185\u7701" (Masked Self-Attention)'}),'\uff1a\u67e5\u770b\u81ea\u5df1\u5df2\u7ecf\u5199\u51fa\u6765\u7684\u5185\u5bb9\u3002\u8fd9\u91cc\u5fc5\u987b\u4f7f\u7528\u56e0\u679c\u63a9\u7801 (Causal Mask)\uff0c\u9632\u6b62\u6a21\u578b\u5728\u8bad\u7ec3\u65f6"\u5077\u770b\u672a\u6765"\u7684\u7b54\u6848']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:'"\u67e5\u9605" (Cross-Attention)'}),'\uff1a\u8fd9\u662f\u8fde\u63a5\u4e24\u5ea7\u5927\u697c\u7684\u6865\u6881\u3002\u89e3\u7801\u5668\u7684\u6bcf\u4e00\u5c42\u90fd\u4f1a\u62ff\u7740\u81ea\u5df1\u7684\u67e5\u8be2 (Q)\uff0c\u53bb\u67e5\u9605\u7f16\u7801\u5668\u63d0\u4f9b\u7684\u90a3\u4efd"\u6700\u7ec8\u7b14\u8bb0" (K \u548c V)\uff0c\u4ece\u800c\u786e\u4fdd\u751f\u6210\u7684\u5185\u5bb9\u5fe0\u5b9e\u4e8e\u539f\u6587']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"\u5b8c\u6574\u4ee3\u7801\u5b9e\u73b0",children:"\u5b8c\u6574\u4ee3\u7801\u5b9e\u73b0"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass PositionalEncoding(nn.Module):\n    """\n    \u3010\u6a21\u5757 1\uff1a\u4f4d\u7f6e\u7f16\u7801 (Positional Encoding)\u3011\n    \u4f5c\u7528\uff1a\u56e0\u4e3a Transformer \u662f\u5e76\u884c\u5904\u7406\u7684\uff0c\u5b83\u672c\u8eab\u4e0d\u77e5\u9053\u8bcd\u7684\u987a\u5e8f\uff08"\u6211\u7231\u4f60"\u548c"\u4f60\u7231\u6211"\u5bf9\u5b83\u6765\u8bf4\u6ca1\u533a\u522b\uff09\u3002\n    \u6211\u4eec\u9700\u8981\u4eba\u4e3a\u5730\u7ed9\u6bcf\u4e2a\u8bcd\u7684\u5411\u91cf\u91cc\u6ce8\u5165\u4f4d\u7f6e\u4fe1\u606f\u3002\n    """\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # \u521b\u5efa\u4e00\u4e2a\u8db3\u591f\u957f\u7684\u77e9\u9635 [max_len, d_model] \u6765\u5b58\u4f4d\u7f6e\u7f16\u7801\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        # \u8bba\u6587\u4e2d\u7684\u6b63\u5f26/\u4f59\u5f26\u516c\u5f0f\u91cc\u7684\u5206\u6bcd\u90e8\u5206\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        # \u5076\u6570\u7ef4\u5ea6\u7528 sin\uff0c\u5947\u6570\u7ef4\u5ea6\u7528 cos\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # \u589e\u52a0\u4e00\u4e2a batch \u7ef4\u5ea6: [1, max_len, d_model]\n        pe = pe.unsqueeze(0)\n        # register_buffer \u8868\u793a\u8fd9\u4e2a\u53c2\u6570\u4e0d\u662f\u6a21\u578b\u9700\u8981\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u4f46\u9700\u8981\u968f\u6a21\u578b\u4fdd\u5b58\n        self.register_buffer(\'pe\', pe)\n\n    def forward(self, x):\n        """\n        x: [batch_size, seq_len, d_model] (\u7ecf\u8fc7 Embedding \u540e\u7684\u8f93\u5165)\n        """\n        # \u3010\u5173\u952e\u64cd\u4f5c\u3011\u76f4\u63a5\u76f8\u52a0 (Add)\uff0c\u800c\u4e0d\u662f\u62fc\u63a5\u3002\n        # \u622a\u53d6\u548c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e00\u6837\u957f\u7684\u4f4d\u7f6e\u7f16\u7801\u52a0\u5230\u8f93\u5165\u4e0a\u3002\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\n\nclass MultiHeadAttention(nn.Module):\n    """\n    \u3010\u6a21\u5757 2\uff1a\u591a\u5934\u6ce8\u610f\u529b (Multi-Head Attention) - \u6838\u5fc3\u7075\u9b42\u3011\n    \u4f5c\u7528\uff1a\u8ba9\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u90fd\u80fd\u5173\u6ce8\u5230\u5176\u4ed6\u8bcd\uff0c\u8ba1\u7b97\u76f8\u5173\u6027 (QKV\u673a\u5236)\u3002\n    "\u591a\u5934"\u610f\u5473\u7740\u7528\u591a\u4e2a\u4e0d\u540c\u7684\u89d2\u5ea6\u53bb\u89c2\u5bdf\uff08\u6bd4\u5982\u4e00\u4e2a\u5934\u770b\u8bed\u6cd5\uff0c\u4e00\u4e2a\u5934\u770b\u8bed\u4e49\uff09\u3002\n    """\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, "d_model \u5fc5\u987b\u80fd\u88ab num_heads \u6574\u9664"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads  # \u6bcf\u4e2a\u5934\u5904\u7406\u7684\u7ef4\u5ea6\u5927\u5c0f (\u4f8b\u5982 512/8 = 64)\n\n        # \u5b9a\u4e49 Q, K, V \u7684\u7ebf\u6027\u6295\u5f71\u5c42 (\u5373\u8bad\u7ec3\u65f6\u8981\u5b66\u7684\u6743\u91cd\u77e9\u9635 Wq, Wk, Wv)\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n\n        # \u6700\u540e\u628a\u6240\u6709\u5934\u62fc\u63a5\u8d77\u6765\u540e\u7684\u7ebf\u6027\u5c42\n        self.w_o = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        """\n        \u5b9e\u73b0\u6838\u5fc3\u516c\u5f0f: Attention(Q, K, V) = softmax( (Q @ K^T) / sqrt(d_k) ) @ V\n        """\n        # 1. \u8ba1\u7b97\u5206\u6570 (Scores)\n        # Q @ K\u8f6c\u7f6e\u3002\u7ed3\u679c shape: [batch, num_heads, seq_len(q), seq_len(k)]\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n\n        # 2. \u7f29\u653e (Scaling) - \u9632\u6b62\u6570\u503c\u8fc7\u5927\u5bfc\u81f4 Softmax \u68af\u5ea6\u6d88\u5931\n        scores = scores / math.sqrt(self.d_k)\n\n        # 3. \u63a9\u7801 (Masking) - \u5982\u679c\u6709 mask\uff0c\u628a\u9700\u8981\u5ffd\u7565\u7684\u4f4d\u7f6e\u586b\u6210\u8d1f\u65e0\u7a77\n        # \u8fd9\u6837 Softmax \u4e4b\u540e\u8fd9\u4e9b\u4f4d\u7f6e\u7684\u6982\u7387\u5c31\u662f 0\n        if mask is not None:\n            # mask \u901a\u5e38\u662f [batch, 1, seq_len, seq_len]\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # 4. \u5f52\u4e00\u5316 (Softmax) - \u5f97\u5230\u6ce8\u610f\u529b\u6743\u91cd\u6982\u7387\uff0c\u5f62\u72b6\u7c7b\u4f3c\u4e8e"\u96e8\u9732\u5747\u6cbe"\u7684\u5206\u5e03\n        attn_probs = F.softmax(scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n\n        # 5. \u52a0\u6743\u6c42\u548c (Weighted Sum) - \u5f97\u5230\u6700\u7ec8\u7684\u4fe1\u606f\u805a\u5408\n        output = torch.matmul(attn_probs, V)\n        return output\n\n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0)\n\n        # 1. \u7ebf\u6027\u6295\u5f71\uff1a\u8f93\u5165 x \u4e58\u4ee5 Wq, Wk, Wv\n        Q = self.w_q(q)\n        K = self.w_k(k)\n        V = self.w_v(v)\n\n        # 2. \u5206\u5934 (Split Heads)\n        # \u7ef4\u5ea6\u53d8\u6362: [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n        # \u7136\u540e\u8f6c\u7f6e\u4ee5\u4fbf\u540e\u7eed\u8ba1\u7b97: -> [batch, num_heads, seq_len, d_k]\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        # 3. \u8ba1\u7b97\u6ce8\u610f\u529b\n        output = self.scaled_dot_product_attention(Q, K, V, mask)\n\n        # 4. \u62fc\u63a5\u5934 (Concat Heads)\n        # \u628a num_heads \u548c d_k \u7ef4\u5ea6\u5e76\u56de\u53bb -> [batch, seq_len, d_model]\n        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n\n        # 5. \u6700\u540e\u7684\u7ebf\u6027\u53d8\u6362\n        output = self.w_o(output)\n        return output\n\n\nclass FeedForward(nn.Module):\n    """\n    \u3010\u6a21\u5757 3\uff1a\u524d\u9988\u795e\u7ecf\u7f51\u7edc (FFN)\u3011\n    \u4f5c\u7528\uff1a\u5728\u6bcf\u4e2a\u6ce8\u610f\u529b\u5c42\u4e4b\u540e\uff0c\u589e\u52a0\u975e\u7ebf\u6027\u80fd\u529b\uff0c\u6574\u5408\u4fe1\u606f\u3002\n    \u7ed3\u6784\u662f\u4e00\u4e2a\u74f6\u9888\u7ed3\u6784\uff1a\u5bbd -> \u7a84 -> \u5bbd (\u4f8b\u5982 512 -> 2048 -> 512)\n    """\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(FeedForward, self).__init__()\n        # \u7b2c\u4e00\u5c42\u7ebf\u6027\u5c42\uff0c\u628a\u7ef4\u5ea6\u6269\u5927 (\u6bd4\u5982 4\u500d)\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        # \u7b2c\u4e8c\u5c42\u7ebf\u6027\u5c42\uff0c\u628a\u7ef4\u5ea6\u7f29\u56de\u539f\u6837\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        # x -> Linear1 -> ReLU -> Dropout -> Linear2\n        x = self.dropout(F.relu(self.linear1(x)))\n        x = self.linear2(x)\n        return x\n\n\nclass EncoderLayer(nn.Module):\n    """\n    \u3010\u6a21\u5757 4\uff1a\u7f16\u7801\u5668\u5757 (Encoder Layer)\u3011\n    Transformer \u5927\u697c\u91cc\u7684\u6807\u51c6\u7816\u5757\u3002\n    \u5305\u542b\uff1aSelf-Attention + FFN + \u6b8b\u5dee\u8fde\u63a5(Add) + \u5c42\u5f52\u4e00\u5316(Norm)\n    \u6b64\u5904\u5b9e\u73b0\u7684\u662f\u539f\u59cb\u8bba\u6587\u7684 Post-LN \u7ed3\u6784 (\u5148\u6b8b\u5dee\uff0c\u540e\u5f52\u4e00\u5316)\u3002\n    """\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # --- \u5b50\u5c42 1: \u591a\u5934\u81ea\u6ce8\u610f\u529b ---\n        # Q, K, V \u90fd\u662f x \u81ea\u5df1\uff0c\u6240\u4ee5\u53eb Self-Attention\n        attn_output = self.self_attn(q=x, k=x, v=x, mask=mask)\n        # Add & Norm\n        x = self.norm1(x + self.dropout1(attn_output))\n\n        # --- \u5b50\u5c42 2:\u524d\u9988\u7f51\u7edc ---\n        ffn_output = self.ffn(x)\n        # Add & Norm\n        x = self.norm2(x + self.dropout2(ffn_output))\n        return x\n\n\nclass DecoderLayer(nn.Module):\n    """\n    \u3010\u6a21\u5757 5\uff1a\u89e3\u7801\u5668\u5757 (Decoder Layer)\u3011\n    \u6bd4\u7f16\u7801\u5668\u66f4\u590d\u6742\uff0c\u6709\u4e09\u4e2a\u5b50\u5c42\u3002\n    \u5173\u952e\u533a\u522b\u5728\u4e8e\u591a\u4e86 Mask \u548c Cross-Attention\u3002\n    """\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        # 1. Masked Self-Attention (\u5e26\u63a9\u7801\u7684\u81ea\u6ce8\u610f\u529b\uff0c\u9632\u6b62\u5077\u770b\u672a\u6765)\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        # 2. Cross-Attention (\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u770b\u7f16\u7801\u5668\u7684\u8f93\u51fa)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        # 3. FFN\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n    def forward(self, x, memory, tgt_mask=None, src_mask=None):\n        """\n        memory: \u7f16\u7801\u5668\u5854\u6700\u7ec8\u8f93\u51fa\u7684"\u7b14\u8bb0"\n        tgt_mask: \u56e0\u679c\u63a9\u7801(Causal Mask)\uff0c\u906e\u4f4f\u672a\u6765\n        src_mask: \u6e90\u6570\u636e\u7684 Padding Mask\uff0c\u906e\u4f4f\u65e0\u7528\u7684\u586b\u5145\u7b26\n        """\n        # --- \u5b50\u5c42 1: Masked Self-Attention (\u5185\u7701) ---\n        # \u91cd\u70b9\uff1a\u4f20\u5165 tgt_mask\uff0c\u786e\u4fdd\u751f\u6210\u7b2c i \u4e2a\u8bcd\u65f6\u53ea\u80fd\u770b\u5230 1 \u5230 i-1 \u7684\u8bcd\n        attn_output = self.self_attn(q=x, k=x, v=x, mask=tgt_mask)\n        x = self.norm1(x + self.dropout1(attn_output))\n\n        # --- \u5b50\u5c42 2: Cross-Attention (\u67e5\u7b14\u8bb0) ---\n        # \u91cd\u70b9\uff1aQ \u6765\u81ea\u89e3\u7801\u5668\u81ea\u5df1 (x)\uff0cK \u548c V \u6765\u81ea\u7f16\u7801\u5668\u7684\u7b14\u8bb0 (memory)\n        # \u8fd9\u91cc\u4f7f\u7528\u7684\u662f src_mask\uff0c\u9632\u6b62\u5173\u6ce8\u5230\u7f16\u7801\u5668\u8f93\u5165\u91cc\u7684 padding\n        attn_output = self.cross_attn(q=x, k=memory, v=memory, mask=src_mask)\n        x = self.norm2(x + self.dropout2(attn_output))\n\n        # --- \u5b50\u5c42 3: FFN (\u63a8\u7406) ---\n        ffn_output = self.ffn(x)\n        x = self.norm3(x + self.dropout3(ffn_output))\n        return x\n\n\nclass Transformer(nn.Module):\n    """\n    \u3010\u6a21\u5757 6\uff1a\u5b8c\u6574\u7684 Transformer \u67b6\u6784\u3011\n    \u5c06\u4e0a\u9762\u7684\u7ec4\u4ef6\u7ec4\u88c5\u6210\u4e24\u5ea7\u5927\u697c\uff1aEncoder Tower \u548c Decoder Tower\u3002\n    """\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, max_len=5000, dropout=0.1):\n        super(Transformer, self).__init__()\n\n        self.d_model = d_model\n\n        # --- 1. Embedding \u5c42 (\u5730\u57fa) ---\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, max_len, dropout)\n\n        # --- 2. Encoder \u5854\u5806\u53e0 ---\n        # \u4f7f\u7528 nn.ModuleList \u6765\u5b58\u50a8 N \u4e2a\u7f16\u7801\u5668\u5c42\n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)\n        ])\n        # \u7f16\u7801\u5668\u6700\u540e\u7684\u5f52\u4e00\u5316\u5c42 (\u6709\u4e9b\u5b9e\u73b0\u4f1a\u52a0)\n        self.encoder_norm = nn.LayerNorm(d_model)\n\n        # --- 3. Decoder \u5854\u5806\u53e0 ---\n        # \u4f7f\u7528 nn.ModuleList \u6765\u5b58\u50a8 N \u4e2a\u89e3\u7801\u5668\u5c42\n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_decoder_layers)\n        ])\n        self.decoder_norm = nn.LayerNorm(d_model)\n\n        # --- 4. \u8f93\u51fa\u5934 (\u5929\u53f0) ---\n        # \u5c06\u5411\u91cf\u6620\u5c04\u56de\u76ee\u6807\u8bed\u8a00\u8bcd\u8868\u5927\u5c0f\uff0c\u7528\u4e8e\u8ba1\u7b97\u6982\u7387\n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n\n        # \u521d\u59cb\u5316\u53c2\u6570 (Trick: \u6709\u52a9\u4e8e\u6a21\u578b\u6536\u655b)\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def encode(self, src, src_mask):\n        """\u7f16\u7801\u5668\u5854\u7684\u524d\u5411\u4f20\u64ad\u6d41\u7a0b"""\n        # 1. Embedding + \u7f29\u653e + \u4f4d\u7f6e\u7f16\u7801\n        # \u4e58\u4ee5 sqrt(d_model) \u662f\u4e3a\u4e86\u5e73\u8861 embedding \u548c\u4f4d\u7f6e\u7f16\u7801\u7684\u6570\u503c\u91cf\u7ea7\n        src = self.src_embedding(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n\n        # 2. \u4e00\u5c42\u4e00\u5c42\u5411\u4e0a\u8dd1\n        for layer in self.encoder_layers:\n            src = layer(src, src_mask)\n\n        # 3. \u6700\u7ec8\u5f52\u4e00\u5316\uff0c\u5f97\u5230"\u6700\u7ec8\u7b14\u8bb0" Memory\n        return self.encoder_norm(src)\n\n    def decode(self, tgt, memory, tgt_mask, src_mask):\n        """\u89e3\u7801\u5668\u5854\u7684\u524d\u5411\u4f20\u64ad\u6d41\u7a0b"""\n        # 1. Embedding + \u7f29\u653e + \u4f4d\u7f6e\u7f16\u7801\n        tgt = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n        tgt = self.pos_encoder(tgt)\n\n        # 2. \u4e00\u5c42\u4e00\u5c42\u5411\u4e0a\u8dd1\n        for layer in self.decoder_layers:\n            # \u6ce8\u610f\uff1a\u6bcf\u4e00\u5c42\u90fd\u63a5\u6536\u540c\u6837\u7684 memory \u548c mask\n            tgt = layer(tgt, memory, tgt_mask, src_mask)\n\n        return self.decoder_norm(tgt)\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        """\n        \u6574\u4e2a\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\u3002\n        \u6ce8\u610f\u6d41\u7a0b\uff1a\u5148\u5b8c\u6574\u8fd0\u884c\u7f16\u7801\u5668\uff0c\u518d\u8fd0\u884c\u89e3\u7801\u5668\u3002\n        """\n        # \u9636\u6bb5 1\uff1a\u5168\u4f53\u7f16\u7801\u5668\u96c6\u5408\uff01\u4ea7\u51fa\u7b14\u8bb0\n        memory = self.encode(src, src_mask)\n\n        # \u9636\u6bb5 2\uff1a\u5168\u4f53\u89e3\u7801\u5668\u96c6\u5408\uff01\u53c2\u8003\u7b14\u8bb0\u8fdb\u884c\u751f\u6210\n        decoder_output = self.decode(tgt, memory, tgt_mask, src_mask)\n\n        # \u9636\u6bb5 3\uff1a\u8f93\u51fa\u5c42\u6620\u5c04\u5f97\u5230 Logits\n        output = self.fc_out(decoder_output)\n        return output\n\n\n# =============================================================================\n# \u3010\u5de5\u5177\u51fd\u6570\uff1a\u751f\u6210\u63a9\u7801 (Masks)\u3011\n# =============================================================================\n\ndef generate_square_subsequent_mask(sz):\n    """\n    \u751f\u6210\u56e0\u679c\u63a9\u7801 (Causal Mask / Look-ahead Mask)\u3002\n    \u7528\u4e8e\u89e3\u7801\u5668\u7684 Self-Attention\uff0c\u9632\u6b62\u770b\u5230\u672a\u6765\u7684\u8bcd\u3002\n    \u751f\u6210\u4e00\u4e2a\u4e0a\u4e09\u89d2\u77e9\u9635 (\u4e0d\u542b\u5bf9\u89d2\u7ebf)\uff0c\u586b\u5145\u4e3a\u8d1f\u65e0\u7a77\u3002\n    """\n    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n    mask = mask.masked_fill(mask == 1, float(\'-inf\'))\n    return mask\n\ndef create_padding_mask(seq, pad_idx):\n    """\n    \u751f\u6210 Padding Mask\u3002\n    \u7528\u4e8e\u544a\u8bc9 Attention \u673a\u5236\u5ffd\u7565\u6389\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u586b\u5145\u7b26 (Pad Token)\u3002\n    \u8fd4\u56de\u5f62\u72b6: [batch_size, 1, 1, seq_len] \u4ee5\u4fbf\u5e7f\u64ad\u5230\u591a\u5934\u6ce8\u610f\u529b\u4e2d\n    """\n    # seq shape: [batch_size, seq_len]\n    # mask shape: [batch_size, seq_len] -> True \u8868\u793a\u662f padding\n    mask = (seq == pad_idx)\n    # \u6269\u5c55\u7ef4\u5ea6\u4ee5\u4fbf\u540e\u7eed\u8ba1\u7b97\n    return mask.unsqueeze(1).unsqueeze(2)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"\u8fd0\u884c\u793a\u4f8b",children:"\u8fd0\u884c\u793a\u4f8b"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# 1. \u5b9a\u4e49\u8d85\u53c2\u6570\nsrc_vocab_size = 1000  # \u6e90\u8bed\u8a00\u8bcd\u8868\u5927\u5c0f\ntgt_vocab_size = 2000  # \u76ee\u6807\u8bed\u8a00\u8bcd\u8868\u5927\u5c0f\nd_model = 512          # \u8bcd\u5411\u91cf\u7ef4\u5ea6\nnum_heads = 8          # \u6ce8\u610f\u529b\u5934\u6570\nnum_layers = 3         # \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5c42\u6570 (\u4e3a\u4e86\u6f14\u793a\u75283\u5c42\uff0c\u539f\u6765\u662f6)\nd_ff = 2048            # FFN \u4e2d\u95f4\u5c42\u7ef4\u5ea6\nmax_len = 100          # \u6700\u5927\u53e5\u5b50\u957f\u5ea6\npad_idx = 0            # \u586b\u5145\u7b26\u7684\u7d22\u5f15 ID\n\n# 2. \u5b9e\u4f8b\u5316\u6a21\u578b\nmodel = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads,\n                    num_layers, num_layers, d_ff, max_len)\n\nprint("\u6a21\u578b\u7ed3\u6784\u5df2\u521b\u5efa\u6210\u529f\uff01")\n\n# 3. \u521b\u5efa\u6a21\u62df\u8f93\u5165\u6570\u636e (Batch size = 2)\n# \u6e90\u53e5\u5b50 (\u6bd4\u5982\u82f1\u6587): \u957f\u5ea6\u4e0d\u4e00\u6837\uff0c\u77ed\u7684\u7528 0 (pad_idx) \u586b\u5145\nsrc_seq = torch.LongTensor([\n    [10, 20, 30, 40, 0, 0], # \u53e5\u5b50 1\u957f\u5ea6 4\n    [15, 25, 35, 45, 55, 0] # \u53e5\u5b50 2\u957f\u5ea6 5\n])\n\n# \u76ee\u6807\u53e5\u5b50\u8f93\u5165 (\u6bd4\u5982\u4e2d\u6587): \u89e3\u7801\u5668\u7684\u8f93\u5165\uff0c\u901a\u5e38\u4ee5 <SOS> \u5f00\u5934\uff0c\u4e0d\u5305\u542b <EOS>\n# \u5047\u8bbe 1 \u662f <SOS>\ntgt_seq_input = torch.LongTensor([\n    [1, 100, 200, 300, 0],\n    [1, 150, 250, 350, 450]\n])\n\n# 4. \u521b\u5efa\u5fc5\u8981\u7684\u63a9\u7801\n# (A) \u6e90\u6570\u636e Padding Mask: \u544a\u8bc9\u7f16\u7801\u5668\u4e0d\u8981\u5173\u6ce8 0\nsrc_mask = create_padding_mask(src_seq, pad_idx)\n\n# (B) \u76ee\u6807\u6570\u636e Padding Mask: \u544a\u8bc9\u89e3\u7801\u5668\u4e0d\u8981\u5173\u6ce8 0\ntgt_pad_mask = create_padding_mask(tgt_seq_input, pad_idx)\n\n# (C) \u76ee\u6807\u6570\u636e\u56e0\u679c\u63a9\u7801: \u544a\u8bc9\u89e3\u7801\u5668\u4e0d\u8981\u5077\u770b\u672a\u6765\ntgt_len = tgt_seq_input.size(1)\ntgt_causal_mask = generate_square_subsequent_mask(tgt_len)\n\n# \u5408\u5e76\u89e3\u7801\u5668\u7684\u4e24\u4e2a\u63a9\u7801\ntgt_mask = tgt_causal_mask\n\nprint(f"\\n\u6e90\u8f93\u5165\u5f62\u72b6: {src_seq.shape}")\nprint(f"\u76ee\u6807\u8f93\u5165\u5f62\u72b6: {tgt_seq_input.shape}")\nprint(f"\u56e0\u679c\u63a9\u7801\u5f62\u72b6: {tgt_mask.shape}")\n\n# 5. \u6a21\u578b\u524d\u5411\u4f20\u64ad\noutput = model(src_seq, tgt_seq_input, src_mask=src_mask, tgt_mask=tgt_mask)\n\n# 6. \u67e5\u770b\u8f93\u51fa\nprint(f"\\n\u6a21\u578b\u8f93\u51fa\u5f62\u72b6 (Logits): {output.shape}")\nprint("\u9884\u671f\u5f62\u72b6\u89e3\u91ca: [Batch Size, Target Seq Len, Target Vocab Size]")\n# \u8f93\u51fa\u5e94\u8be5\u662f [2, 5, 2000]\uff0c\u8868\u793a\u5bf9 batch \u4e2d 2 \u4e2a\u53e5\u5b50\uff0c\u6bcf\u4e2a\u53e5\u5b50\u7684 5 \u4e2a\u4f4d\u7f6e\uff0c\u9884\u6d4b 2000 \u4e2a\u8bcd\u7684\u6982\u7387\u5206\u6570\u3002\n\nprint("\\n\u6f14\u793a\u7ed3\u675f\u3002\u8fd9\u6bb5\u4ee3\u7801\u5b8c\u6574\u5c55\u793a\u4e86 Transformer \u7684\u5185\u90e8\u8ba1\u7b97\u6d41\u7a0b\u3002")\n'})}),"\n",(0,t.jsx)(e.h2,{id:"\u6838\u5fc3\u516c\u5f0f",children:"\u6838\u5fc3\u516c\u5f0f"}),"\n",(0,t.jsx)(e.p,{children:"Transformer \u4e2d\u7684\u6838\u5fc3\u6ce8\u610f\u529b\u673a\u5236\u516c\u5f0f\u4e3a\uff1a"}),"\n",(0,t.jsx)(e.span,{className:"katex-display",children:(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mtext,{children:"Attention"}),(0,t.jsx)(e.mo,{stretchy:"false",children:"("}),(0,t.jsx)(e.mi,{children:"Q"}),(0,t.jsx)(e.mo,{separator:"true",children:","}),(0,t.jsx)(e.mi,{children:"K"}),(0,t.jsx)(e.mo,{separator:"true",children:","}),(0,t.jsx)(e.mi,{children:"V"}),(0,t.jsx)(e.mo,{stretchy:"false",children:")"}),(0,t.jsx)(e.mo,{children:"="}),(0,t.jsx)(e.mtext,{children:"softmax"}),(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mo,{fence:"true",children:"("}),(0,t.jsxs)(e.mfrac,{children:[(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mi,{children:"Q"}),(0,t.jsxs)(e.msup,{children:[(0,t.jsx)(e.mi,{children:"K"}),(0,t.jsx)(e.mi,{children:"T"})]})]}),(0,t.jsx)(e.msqrt,{children:(0,t.jsxs)(e.msub,{children:[(0,t.jsx)(e.mi,{children:"d"}),(0,t.jsx)(e.mi,{children:"k"})]})})]}),(0,t.jsx)(e.mo,{fence:"true",children:")"})]}),(0,t.jsx)(e.mi,{children:"V"})]}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V"})]})})}),(0,t.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,t.jsx)(e.span,{className:"mord text",children:(0,t.jsx)(e.span,{className:"mord",children:"Attention"})}),(0,t.jsx)(e.span,{className:"mopen",children:"("}),(0,t.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,t.jsx)(e.span,{className:"mpunct",children:","}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"K"}),(0,t.jsx)(e.span,{className:"mpunct",children:","}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"V"}),(0,t.jsx)(e.span,{className:"mclose",children:")"}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(e.span,{className:"mrel",children:"="}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"2.4684em",verticalAlign:"-0.95em"}}),(0,t.jsx)(e.span,{className:"mord text",children:(0,t.jsx)(e.span,{className:"mord",children:"softmax"})}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsxs)(e.span,{className:"minner",children:[(0,t.jsx)(e.span,{className:"mopen delimcenter",style:{top:"0em"},children:(0,t.jsx)(e.span,{className:"delimsizing size3",children:"("})}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mopen nulldelimiter"}),(0,t.jsx)(e.span,{className:"mfrac",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsxs)(e.span,{className:"vlist",style:{height:"1.5183em"},children:[(0,t.jsxs)(e.span,{style:{top:"-2.2528em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"mord",children:(0,t.jsx)(e.span,{className:"mord sqrt",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsxs)(e.span,{className:"vlist",style:{height:"0.8572em"},children:[(0,t.jsxs)(e.span,{className:"svg-align",style:{top:"-3em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"mord",style:{paddingLeft:"0.833em"},children:(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mord mathnormal",children:"d"}),(0,t.jsx)(e.span,{className:"msupsub",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.3361em"},children:(0,t.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.03148em"},children:"k"})})]})}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,t.jsx)(e.span,{})})})]})})]})})]}),(0,t.jsxs)(e.span,{style:{top:"-2.8172em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"hide-tail",style:{minWidth:"0.853em",height:"1.08em"},children:(0,t.jsx)(e.svg,{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.08em",viewBox:"0 0 400000 1080",preserveAspectRatio:"xMinYMin slice",children:(0,t.jsx)(e.path,{d:"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z"})})})]})]}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.1828em"},children:(0,t.jsx)(e.span,{})})})]})})})]}),(0,t.jsxs)(e.span,{style:{top:"-3.23em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,t.jsxs)(e.span,{style:{top:"-3.677em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"K"}),(0,t.jsx)(e.span,{className:"msupsub",children:(0,t.jsx)(e.span,{className:"vlist-t",children:(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.8413em"},children:(0,t.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.13889em"},children:"T"})})]})})})})})]})]})]})]}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.93em"},children:(0,t.jsx)(e.span,{})})})]})}),(0,t.jsx)(e.span,{className:"mclose nulldelimiter"})]}),(0,t.jsx)(e.span,{className:"mclose delimcenter",style:{top:"0em"},children:(0,t.jsx)(e.span,{className:"delimsizing size3",children:")"})})]}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"V"})]})]})]})}),"\n",(0,t.jsx)(e.p,{children:"\u5176\u4e2d\uff1a"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsx)(e.mrow,{children:(0,t.jsx)(e.mi,{children:"Q"})}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"Q"})]})})}),(0,t.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"0.8778em",verticalAlign:"-0.1944em"}}),(0,t.jsx)(e.span,{className:"mord mathnormal",children:"Q"})]})})]})," (Query): \u67e5\u8be2\u77e9\u9635"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsx)(e.mrow,{children:(0,t.jsx)(e.mi,{children:"K"})}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"K"})]})})}),(0,t.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"K"})]})})]})," (Key): \u952e\u77e9\u9635"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsx)(e.mrow,{children:(0,t.jsx)(e.mi,{children:"V"})}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"V"})]})})}),(0,t.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"V"})]})})]})," (Value): \u503c\u77e9\u9635"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsx)(e.mrow,{children:(0,t.jsxs)(e.msub,{children:[(0,t.jsx)(e.mi,{children:"d"}),(0,t.jsx)(e.mi,{children:"k"})]})}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"d_k"})]})})}),(0,t.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"0.8444em",verticalAlign:"-0.15em"}}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mord mathnormal",children:"d"}),(0,t.jsx)(e.span,{className:"msupsub",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.3361em"},children:(0,t.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.03148em"},children:"k"})})]})}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,t.jsx)(e.span,{})})})]})})]})]})})]}),": \u952e\u5411\u91cf\u7684\u7ef4\u5ea6"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsx)(e.mrow,{children:(0,t.jsx)(e.msqrt,{children:(0,t.jsxs)(e.msub,{children:[(0,t.jsx)(e.mi,{children:"d"}),(0,t.jsx)(e.mi,{children:"k"})]})})}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"\\sqrt{d_k}"})]})})}),(0,t.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"1.04em",verticalAlign:"-0.1828em"}}),(0,t.jsx)(e.span,{className:"mord sqrt",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsxs)(e.span,{className:"vlist",style:{height:"0.8572em"},children:[(0,t.jsxs)(e.span,{className:"svg-align",style:{top:"-3em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"mord",style:{paddingLeft:"0.833em"},children:(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mord mathnormal",children:"d"}),(0,t.jsx)(e.span,{className:"msupsub",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.3361em"},children:(0,t.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.03148em"},children:"k"})})]})}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,t.jsx)(e.span,{})})})]})})]})})]}),(0,t.jsxs)(e.span,{style:{top:"-2.8172em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"hide-tail",style:{minWidth:"0.853em",height:"1.08em"},children:(0,t.jsx)(e.svg,{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.08em",viewBox:"0 0 400000 1080",preserveAspectRatio:"xMinYMin slice",children:(0,t.jsx)(e.path,{d:"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z"})})})]})]}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.1828em"},children:(0,t.jsx)(e.span,{})})})]})})]})})]}),": \u7f29\u653e\u56e0\u5b50\uff0c\u9632\u6b62\u5185\u79ef\u8fc7\u5927\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"\u4ee3\u7801\u9605\u8bfb\u6307\u5357",children:"\u4ee3\u7801\u9605\u8bfb\u6307\u5357"}),"\n",(0,t.jsx)(e.p,{children:"\u8fd9\u4efd\u7b14\u8bb0\u7684\u4ee3\u7801\u5b9e\u73b0\u9075\u5faa\u4e86\u81ea\u5e95\u5411\u4e0a\u7684\u6784\u5efa\u65b9\u5f0f\uff1a"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u5148\u5b9e\u73b0\u6700\u57fa\u7840\u7684\u96f6\u4ef6"}),"\uff1a",(0,t.jsx)(e.code,{children:"PositionalEncoding"})," (\u4f4d\u7f6e\u7f16\u7801) \u548c ",(0,t.jsx)(e.code,{children:"MultiHeadAttention"})," (\u591a\u5934\u6ce8\u610f\u529b)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u518d\u7ec4\u88c5\u6210\u6807\u51c6\u7684\u697c\u5c42"}),"\uff1a",(0,t.jsx)(e.code,{children:"EncoderLayer"})," \u548c ",(0,t.jsx)(e.code,{children:"DecoderLayer"})]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsxs)(e.strong,{children:["\u6700\u540e\u5728\u4e3b\u7c7b ",(0,t.jsx)(e.code,{children:"Transformer"})," \u4e2d"]}),"\uff1a\u5c06\u8fd9\u4e9b\u697c\u5c42\u5806\u53e0\u6210\u4e24\u5ea7\u5927\u697c\uff0c\u5e76\u5b9a\u4e49\u4e86\u5b8c\u6574\u7684\u524d\u5411\u4f20\u64ad\u903b\u8f91"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"\u5173\u952e\u8981\u70b9\u603b\u7ed3",children:"\u5173\u952e\u8981\u70b9\u603b\u7ed3"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u4f4d\u7f6e\u7f16\u7801"}),"\uff1a\u4f7f\u7528\u6b63\u5f26/\u4f59\u5f26\u51fd\u6570\u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u751f\u6210\u552f\u4e00\u7684\u7f16\u7801\uff0c\u901a\u8fc7\u76f8\u52a0\u7684\u65b9\u5f0f\u6ce8\u5165\u5230\u8bcd\u5411\u91cf\u4e2d"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u591a\u5934\u6ce8\u610f\u529b"}),"\uff1a\u5c06\u6ce8\u610f\u529b\u673a\u5236\u5206\u6210\u591a\u4e2a\u5934\uff0c\u6bcf\u4e2a\u5934\u5173\u6ce8\u4e0d\u540c\u7684\u8bed\u4e49\u5b50\u7a7a\u95f4"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u79bb"}),"\uff1a\u7f16\u7801\u5668\u5b8c\u6574\u8fd0\u884c\u4ea7\u51fa Memory\uff0c\u89e3\u7801\u5668\u5229\u7528 Memory \u9010\u6b65\u751f\u6210\u8f93\u51fa"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u63a9\u7801\u673a\u5236"}),"\uff1a","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Padding Mask\uff1a\u5ffd\u7565\u586b\u5145\u7b26\u53f7"}),"\n",(0,t.jsx)(e.li,{children:"Causal Mask\uff1a\u9632\u6b62\u89e3\u7801\u5668\u770b\u5230\u672a\u6765\u4fe1\u606f"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"\u6b8b\u5dee\u8fde\u63a5\u4e0e\u5c42\u5f52\u4e00\u5316"}),"\uff1a\u5e2e\u52a9\u6df1\u5c42\u7f51\u7edc\u8bad\u7ec3\uff0c\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"\u53c2\u8003\u8d44\u6599",children:"\u53c2\u8003\u8d44\u6599"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["\u539f\u59cb\u8bba\u6587\uff1a",(0,t.jsx)(e.a,{href:"https://arxiv.org/abs/1706.03762",children:"Attention Is All You Need"})," (Vaswani et al., 2017)"]}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"http://jalammar.github.io/illustrated-transformer/",children:"The Illustrated Transformer"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://pytorch.org/tutorials/beginner/transformer_tutorial.html",children:"PyTorch \u5b98\u65b9 Transformer \u6559\u7a0b"})}),"\n"]})]})}function h(s={}){const{wrapper:e}={...(0,r.R)(),...s.components};return e?(0,t.jsx)(e,{...s,children:(0,t.jsx)(c,{...s})}):c(s)}},28453:(s,e,n)=>{n.d(e,{R:()=>l,x:()=>i});var a=n(96540);const t={},r=a.createContext(t);function l(s){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof s?s(e):{...e,...s}},[e,s])}function i(s){let e;return e=s.disableParentContext?"function"==typeof s.components?s.components(t):s.components||t:l(s.components),a.createElement(r.Provider,{value:e},s.children)}}}]);