"use strict";(globalThis.webpackChunkyiwen_blog_website=globalThis.webpackChunkyiwen_blog_website||[]).push([[54399],{2452:e=>{e.exports=JSON.parse('{"tag":{"label":"inference","permalink":"/yiwen-blog-website/docs/tags/inference","allTagsPath":"/yiwen-blog-website/docs/tags","count":3,"items":[{"id":"ai/architectures/transformer/kv-cache","title":"LLM \u63a8\u7406\u6280\u672f\u8be6\u89e3\uff1aKV Cache \u6807\u51c6\u673a\u5236","description":"\u7248\u672c\uff1a 2.0 (\u4fee\u8ba2\u7248)","permalink":"/yiwen-blog-website/docs/ai/architectures/transformer/kv-cache"},{"id":"ai/architectures/transformer/paged-attention","title":"PagedAttention\uff1avLLM \u7684\u663e\u5b58\u7ba1\u7406\u9769\u547d","description":"PagedAttention \u662f\u52a0\u5dde\u5927\u5b66\u4f2f\u514b\u5229\u5206\u6821\uff08UC Berkeley\uff09\u56e2\u961f\u5728 vLLM \u9879\u76ee\u4e2d\u63d0\u51fa\u7684\u6838\u5fc3\u6280\u672f\uff0c\u5b83\u5f7b\u5e95\u6539\u53d8\u4e86 LLM \u63a8\u7406\u7684\u663e\u5b58\u7ba1\u7406\u65b9\u5f0f\u3002","permalink":"/yiwen-blog-website/docs/ai/architectures/transformer/paged-attention"},{"id":"ai/deployment/training-vs-inference-parallelism","title":"Transformer\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u5e76\u884c\u6027\u5dee\u5f02","description":"\u8fd9\u662f Transformer \u67b6\u6784\u4e2d\u6700\u53cd\u76f4\u89c9\u3001\u4e5f\u6700\u6838\u5fc3\u7684\u95ee\u9898\uff1a\u65e2\u7136 Transformer \u662f\u4e00\u4e2a token \u4e00\u4e2a token \u9884\u6d4b\u7684\uff0c\u600e\u4e48\u80fd\u8bf4\u5b83\u662f\u5e76\u884c\u7684\uff1f","permalink":"/yiwen-blog-website/docs/ai/deployment/training-vs-inference-parallelism"}],"unlisted":false}}')}}]);